# Backgrond {#background}

## Forecasting and Modelling

A forecast, in most general terms, is a stated belief about the future \citep{gneitingProbabilisticForecastsCalibration2007} as it will occur. Such a belief can be stated in qualitative or quantitative or quantitative terms. This thesis will almost exclusively focus on quantitative forecasts. 

Quantitative forecasts can either be probabilistic, or they can be point forecasts. A probabilistic forecast \citep{heldProbabilisticForecastingInfectious2017} is a full predictive probability distribution over multiple possible outcomes. A point forecast, on the other hand is a single number that represents a single number for the expected outcome. A probabilistic forecast therefore incorporates uncertainty about different outcomes in a way that a point forecast cannot. Probabilistic forecasts are therefore generally more useful for decision making \citep{heldProbabilisticForecastingInfectious2017} and will be the focus of this thesis. Some authors \citep{farrowHumanJudgmentApproach2017} make a distinction between 'forecast', meaning a probabilistic forecast and 'prediction', meaning a point forecast. We will use the two terms interchangeably. 

The term 'model' in the context of this thesis is describes a simplified representation of the world that allows someone to make statements about the future based on certain inputs. A model can be the specific understanding of the world that a human forecaster has in its mind to make sense of past and current events and that allows her to make forecasts about the future. The term model can also denote a mathematical or computer model, a set of encoded rules that describe and represent the processes that govern events in reality. Mathematical models usually use computers to map observed inputs to a model output. If not otherwise stated, we use the term 'model' to describe a mathematical model which produces a forecast (rather than the mental representation of the world in a person's head). Similarly, 'model-based' predictions mean predictions generated by a compuational model. Furthermore, we use the term 'modeller' to denote a person who develops, codes, or adapts a mathematical model. 
<!-- Alternatively put the model part here -->

We describe anyone or anything that issues a forecast as 'a forecaster'. This can either be a person voicing their judgement, or it can be a computer model or algorithm that issues a forecast based on given inputs, or a combination of both. When more clarity is required, we use the terms 'human forecaster' and either 'mathematical model' or 'computational model'. 

The output of an epidemiological model is not necessarily a forecast. It could also, for example, be a nowcast, 
<!-- (CITATION Sam?),  -->
or a scenario or projection. A nowcast is a description of the world as it is in the present in absence of definitive data. A scenario is the representation of the future as it could look like under certain scenario assumptions, whereas a projection describes the future as it could unfold if conditions stayed the same as they were in the past \citep{funkShorttermForecastsInform2020}. This is in contrast to a forecast which aims to predict the future as it will occur. 

Forecasts (and nowcasts) can be judged eventually by comparing them against observed data. This is much harder for scenarios or projections, as they usually make statements about a world that was not observed. While scenarios are harder to evaluate, they are arguably more useful for decision making. Scenarios are able to show what could occur under different assumptions and different courses of actions. It is more difficult to use forecasts directly as a basis for decision-making, as it unclear how results would change under a possible course of action. By definition, a forecast has to estimate and already incorporate possible courses of action in order to make an accurate statement about the future as it will occur. 

One possibility to to increase predictive accuracy of forecasts is to combine individual forecasts into an ensemble, which usually performs better than any individual forecaster \citep{gneitingWeatherForecastingEnsemble2005, yamanaSuperensembleForecastsDengue2016}. To denote an ensemble of forecasts made by a group of human forecasters we will sometimes use the term 'crowd ensemble' or 'crowd forecast'. Ensembles can be either equally weighted or trained, by assigning ensemble weights based on past performance of a forecaster. Past research suggests that it is very difficult to form ensembles which outperform an equally weighted ensemble \citep{claeskensForecastCombinationPuzzle2016}, but not impossible \citep{brooksNonmechanisticForecastsSeasonal2018}. During the COVID-19 pandemic, forecasts for different target have been systematically collected, aggregated and evaluated by three COVID-19 Forecast Hubs in the US \citep{cramerEvaluationIndividualEnsemble2021}, Germany and Poland \citep{bracherPreregisteredShorttermForecasting2021} and Europe \citep{sherrattPredictivePerformanceMultimodel2022a}. 


## Forecast evaluation

Forecast evaluation in a narrow sense is the process of assessing how well a forecasters predictions align with the actual observations. In a broader sense of the term, forecast evaluation would also include elements such as an assessment of the usefulness of the forecasts to the forecast consumer, or analyses that would help understand underlying characteristics of the model or forecaster better. Forecast evaluation helps modellers and forecasters to improve future predictions, and helps decision makers decide which forecasts to take into account when making decisions. 

For any given forecasting task, the primary aim of any forecaster should be to issue a predictive distribution $F$ that equals the (usually unknown) true data-generating distribution $G$ \citep{gneitingProbabilisticForecastsCalibration2007}. For an ideal forecast we therefore have
$$F = G,$$
where $F$ and $G$ are cumulative distribution functions. 

The similarity between forecasts and observations is usually evaluated using so called proper scoring rules. A scoring rule $S(F, y)$ is a function of the forecast $F$ and the observation $y$ that returns a single numeric value. A scoring rule is said to be proper, if an ideal forecaster receives the best score in expectation. Additional metrics can be used to investigate certain characteristics of a forecaster in more detail, like e.g. a systematic bias, or a tendency to issue overconfident predictions. Chapter \ref{sec:scoringutils} will go into more detail regarding different scoring metrics and approaches. 
  
Scoring rules and evaluation metrics describe a mathematical relationship between forecasts and observations and capture different aspects of how a forecast deviates from observed values. This makes them objective and comparable. However, scoring rules and evaluation measures are not able to directly measure the usefulness of forecasts to forecast consumers. For example, underprediction of a target like hospitalisations could lead to more severe consequences than overprediction for a decision maker relying on a forecast in a way that's not necessarily captured by a given metric. The selection of an appropriate metric therefore should ideally reflect the qualities a forecast consumer values in a forecast. Chapter \ref{sec:scoringutils} reviews different existing metrics and discusses when their application is appropriate. Chapter \ref{sec:LogTransformation} suggests an adaptation of widely-used scoring rules that is particularly suited to evaluating forecasts in an epidemiological context.

Proper scoring rules \citep{brierVerificationForecastsExpressed1950, goodRationalDecisions1952, gneitingStrictlyProperScoring2007} assign a numerical score to a pair of a single predictive distribution and a single observation. Comparing multiple forecasts over time, and across locations and forecast targets comes with additional issues [@cramerEvaluationIndividualEnsemble2021]. One common issue is that scores usually scale with the variance of a forecast target and therefore cannot easily be compared across targets that span several orders of magnitude. We propose a novel solution for this problem in Chapter \ref{sec:LogTransformation}. A second issue arises in the presence of missing forecasts, where forecasters differ in what targets they have issued a forecast for. @cramerEvaluationIndividualEnsemble2021 have proposed a pairwise tournament between models based on the set of overlapping forecasts. A third issue is the complexity of obtaining and presenting results across many dimensions, such as e.g. time, location and forecast targets. Chapter \@ref(sec:scoringutils) presents `scoringutils`, an `R` package that facilitates forecast evaluation, visual presentation of results, and also implements the pairwise comparison approach suggested in @cramerEvaluationIndividualEnsemble2021. 


## Human Judgement forecasting

Human judgement forecasting has a long history and efforts have been made in very different contexts and fields, both inside and outside of academia. Human judgement elicitation processes also differ greatly in their methodology and selection of forecasters. Efforts in the past range from surveys of experts or laypeople, to structured discussions between experts aimed to produce a consensus forecast to large crowd forecasting efforts with thousands of forecasters predicting on a given question. One of the first structured methods proposed to support decision making through forecasting is the Delphi method developed by the RAND Corporation in the 1950s in the context of the Cold War [@dalkeyExperimentalApplicationDELPHI1963; @bernicebrownDELPHIPROCESSMETHODOLOGY1968; @pagePrescribingAustraliansLiving2015]. Forecasts were elicited in a structured process where experts would provide a first estimate, then discuss their estimates and potential disagreements, and then provide a final round of estimates. 

<!-- Outside of academia, human predictions and estimates are routinely used to help decision making in private companies, think tanks and governments. In addition, public predictions on various topics of interest have been collected on several online prediction markets or prediction platforms. Prediction markets such as PredictIt, Manifold Markets, Polymarket [CITATIONS] and traditional betting platforms such as betfair allow users to place bets on an outcome by spending either real money or token money to buy "yes" or "no"-shares in the binary outcome of a given market. Prediction platforms such as Metaculus, INFER or Good Judgement Open elicit direct forecasts from users and offer either points or sometimes monetary rewards for the most accurate users. These markets and platforms have been used in predicting a broad spectrum of events such as elections (CITATION US), wars, or the spread of infectious diseases. Academic authors working on human judgement forecasting have in the past either collaborated with existing prediction platforms (CITATIONS McAndrew, Tetlock), used more traditional means of eliciting predictions such as surveys \citep{recchiaHowWellDid2021} or the Delphi method \citep{dalkeyExperimentalApplicationDELPHI1963, bernicebrownDELPHIPROCESSMETHODOLOGY1968, pagePrescribingAustraliansLiving2015}, or developed their own methods \citep{farrowHumanJudgmentApproach2017}.  -->

<!-- Several reviews have been published on human judgement forecasting, beginning with Armstrong et al. (1986).  -->
<!-- (Bunn and Wright, 1991; Goodwin and Wright, 1993; Lawrence et al., 2006; Webby and O'Connor, 1996) (focusing on comparing performance of humans and models). -->

<!-- @zellnerSurveyHumanJudgement2021 (https://royalsocietypublishing.org/doi/10.1098/rsos.201187) provides a comprehensive literature review of human judgement forecasting approaches. @arvanIntegratingHumanJudgement2019 gives a review of academic papers that combine human judgement and mathematical modelling, focusing on product forecasting. @mcandrewAggregatingPredictionsExperts2021 review methods to aggregate predictions from experts.  -->

Human judgement forecasting is not always feasible, especially at scale, due to the time and effort required of human forecasters. However, Human judgement forecasting has various attractive qualities compared to mathematical models. In particular in the early stages of an outbreak, humans may be able to provide rapid forecasts based on very sparse observational data and can make use of contextual information that is difficult to incorporate into mathematical modelling. They may also help provide guidance on questions that computational models cannot answer, such as whether a state or an organisation like the World Health Organisation will provide assistance to help with an outbreak.  

While human judgement forecasting has seen many applications in academic fields such as such as geo- politics \cite{atanasovDistillingWisdomCrowds2016, tetlockForecastingTournamentsTools2014}, product forecasting \citep{arvanIntegratingHumanJudgement2019}, or meta-science \citep{dreberUsingPredictionMarkets2015, gordonAreReplicationRates2020}, its application to infectious diseases has only recently attracted more widespread attention. Notable examples of human judgement forecasting of infectious diseases include \cite{farrowHumanJudgmentApproach2017, recchiaHowWellDid2021, mcandrewExpertJudgmentModel2022, mcandrewAggregatingHumanJudgment2022, mcandrewChimericForecastingCombining2022, mcandrewEarlyHumanJudgment2022, daviesHumanJudgementForecasting2022}. 

Before the beginning of this PhD only one paper, namely \citet{farrowHumanJudgmentApproach2017} for influenza and chikungunya, had directly compared the performance of human judgement forecasts of infectious diseases against predictions made by computational models. Since then, three more papers have examined the performance of human judgement forecasts of COVID-19 in comparison to computational models, two of them form part of this PhD \citep{farrowHumanJudgmentApproach2017, bosseComparingHumanModelbased2022, mcandrewChimericForecastingCombining2022, BOSSEUKCROWD}. Results overall suggest that a crowd of human forecasters can achieve performance comparable to that of an ensemble of computational models. 


<!-- - Open things:  -->
<!--   - direct 1:1 comparison -->
<!--   - analysis of what human judgment adds to a computer model -->
<!--   - no open source tools to  -->

<!-- ### Crowdforecastr -->

<!-- - Crowdforecastr tool -->
<!--   - R shiny -->
<!--   - probabilistic forecasting -->


<!-- ## Mathematical modelling of infectious diseases -->

<!-- - uses computers -->
<!-- - necessarily some combination of human judgement and algorithmic assumptions -->
<!-- - varying degrees of human intervention.  -->


<!-- From paper:  -->
<!-- Individual computational models usually rely to varying degrees on mechanistic assump- tions about infectious disease dynamics (such as SIR-type compartmental models that aim to represent how individuals move from being susceptible to infected and then recovered or dead). Some are more statistical in nature (such as time series models that detect statistical pat- terns without explicitly modelling disease dynamics). How exactly such a mathematical or computational model is constructed and which assumptions are made depends on subjective opinion and judgement of the researcher who develops and refines the model. Models are commonly adjusted and improved based on whether the model output looks plausible to the researchers involved. -->

<!-- ### Mechanistic models -->

<!-- - mechanistic assumptions about disease progress -->
<!-- - learning about paramters  -->
<!-- - SIR models -->

<!-- ### Statistical models -->

<!-- - uses statistical features, without making any assumptions about the disease progress -->

<!-- ### Semi-mechanistic models -->

<!-- Mixture of the two -->

<!-- ## Data sources -->

<!-- ### German and Polish Forecast Hub -->

<!-- - cite papers -->
<!-- - one of several Forecast Hubs.  -->

<!-- ### Human forecasts of COVID-19 in Germany and Poland -->

<!-- - Collected using crowdforecastr -->
<!-- - Ethics approval: NUMBER -->

<!-- ### European Forecast Hub  -->

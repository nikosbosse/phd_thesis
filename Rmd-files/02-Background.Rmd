# Backgrond {#background}

## Forecasting and Modelling

A forecast, in most general terms, is a stated belief about the future (CITATION Gneiting and Raftery 2007) as it will occur. Such a belief can be stated in qualitative or quantitative or quantitative terms. This thesis will almost exclusively focus on quantitative forecasts. 

Quantitative forecasts can either be probabilistic, or they can be point forecasts. A probabilistic forecast (CITATION Held) is a full predictive probability distribution over multiple possible outcomes. A point forecast, on the other hand is a single number that represents a single number for the expected outcome. A probabilistic forecast therefore incorporates uncertainty about different outcomes in a way that a point forecast cannot. Probabilistic forecasts are therefore generally more useful for decision making (CITATION) and will be the focus of this thesis. Some authors (CITATION) make a distinction between 'forecast', meaning a probabilistic forecast and 'prediction', meaning a point forecast. We will use the two terms interchangeably. 

We understand anyone or anything that issues a forecast as a forecaster. This can either be a person voicing their judgement, or it can be a computer model or algorithm that issues a forecast based on given inputs, or a combination of both. When more clarity is required, we use the terms "human forecaster" and "mathematical model". 

A model in the context of this thesis is understood as a simplified representation of the world that allows someone to make statements about the future based on certain inputs. A model can be the specific understanding of the world that a human forecaster has in its mind to make sense of past and current events and that allows her to make forecasts about the future. The term model can also denote a mathematical or computer model, a set of encoded rules that describe and represent the processes that govern events in reality. Mathematical models usually use computers to map observed inputs to a model output. If not otherwise stated, we use the term 'model' to describe a mathematical model which produces a forecast (rather than the mental representation of the world in a person's head). Furthermore, we use the term 'modeller' to denote a person who develops, codes, or adapts a mathematical model. 
<!-- Alternatively put the model part here -->

The output of an epidemiological model is not necessarily a forecast. It could also, for example, be a nowcast (CITATION Sam?), or a scenario or projection (CITATION?). A nowcast is a description of the world as it is in the present in absence of definitive data. A scenario is the representation of the future as it could look like under certain scenario assumptions, whereas a projection describes the future as it could unfold if conditions stayed the same as they were in the past. [CITATION]. This is in contrast to a forecast which aims to predict the future as it will occur. 

Forecasts (and nowcasts) can be judged eventually by comparing them against observed data. This is much harder for scenarios or projections, as they usually make statements about a world that was not observed. While scenarios are harder to evaluate, they are arguably more useful for decision making. Scenarios are able to show what could occur under different assumptions and different courses of actions. It is more difficult to use forecasts directly as a basis for decision-making. By definition, a forecast has to estimate and incorporate the most likely course of action in order to make a statement about the future as it will occur. 

In order to increase predictive accuracy, individual forecasts can be combined into an ensemble, which usually performs better than any individual forecaster (CITATION Yamana, Kandula, and Shaman 2016; Gneiting and Raftery 2005). Ensembles can be either equally weighted or trained, by assigning ensemble weights based on past performance of a forecaster. Past research suggests that it is very difficult to form ensembles which outperform an equally weighted ensemble (CITATION Claeskens et al. 2016), but not impossible (CITATION Logan C. Brooks et al. n.d., n.d.). During the COVID-19 pandemic, forecasts for different target have been systematically collected, aggregated and evaluated by three COVID-19 Forecast Hubs in the US (CITATION), Germany and Poland (CITATION) and Europe (CITATION). 

Define "Crowd"


## Forecast evaluation

Forecast evaluation in a narrow sense is the process of assessing how well a forecasters predictions align with the actual observations. In a broader sense of the term, forecast evaluation would also include elements such as an assessment of the usefulness of the forecasts to the forecast consumer, or analyses that would help understand underlying characteristics of the model or forecaster better. Forecast evaluation helps modellers and forecasters to improve future predictions, and helps decision makers decide which forecasts to take into account when making decisions. 

For any given forecasting task, the primary aim of any forecaster should be to issue a predictive distribution $F$ that equals the (usually unknown) true data-generating distribution $G$ (Gneiting et al. 2007). For an ideal forecast we therefore have
$$F = G$$,
where $F$ and $G$ are cumulative distribution functions. 

The similarity between forecasts and observations is usually evaluated using so called proper scoring rules. A scoring rule $S(F, y)$ is a function of the forecast $F$ and the observation $y$ that returns a single numeric value. A scoring rule is said to be proper, if an ideal forecaster receives the best score in expectation. Additional metrics can be used to investigate certain characteristics of a forecaster in more detail, like e.g. a systematic bias, or a tendency to issue overconfident predictions. Chapter \@{scoringutils} will go into more detail regarding different scoring metrics and approaches. 

<!-- - History of forecast evaluation  -->
<!--   - started with the log score, then Brier score, etc...  -->
<!--   - proper scoring rule -->
<!--   - possible decomposition -->
  
Scoring rules and evaluation metrics describe a mathematical relationship between forecasts and observations and capture different aspects of how a forecast deviates from observed values. This makes them objective and comparable. However, scoring rules and evaluation measures are not able to directly measure the usefulness of forecasts to forecast consumers. For example, underprediction of a target like hospitalisations could lead to more severe consequences than overprediction for a decision maker relying on a forecast in a way that's not necessarily captured by a given metric. The selection of an appropriate metric therefore should ideally reflect the qualities a forecast consumer values in a forecast. Chapter \@(scoringutils) reviews different existing metrics and discusses when their application is appropriate. Chapter \@(LogTransformation) suggests an adaptation of widely-used scoring rules that is particularly suited to evaluating forecasts in an epidemiological context.

Proper scoring rules (CITAITONS Gneiting, Box, Brier) assign a numerical score to a pair of a single predictive distribution and a single observation. Comparing multiple forecasts over time, and across locations and forecast targets comes with additional issues [@cramerEvaluationIndividualEnsemble2021]. One common issue is that scores usually scale with the variance of a forecast target and therefore cannot easily be compared across targets that span several orders of magnitude. We propose a novel solution for this problem in Chapter \@(LogTransformation). A second issue arises in the presence of missing forecasts, where forecasters differ in what targets they have issued a forecast for. @cramerEvaluationIndividualEnsemble2021 have proposed a pairwise tournament between models based on the set of overlapping forecasts. A third issue is the complexity of obtaining and presenting results across many dimensions, such as e.g. time, location and forecast targets. Chapter \@ref(scoringutils) presents `scoringutils`, an `R` package that facilitates forecast evaluation, visual presentation of results, and also implements the pairwise comparison approach suggest in @cramerEvaluationIndividualEnsemble2021. 


## Human Judgement forecasting

- Literature review

Human judgement forecasting has a long history and efforts have been made in very different contexts and fields, both inside and outside of academia. Human judgement elicitation processes also differ greatly in their methodology and selection of forecasters. Efforts in the past range from surveys of experts or laypeople, to structured discussions between experts aimed to produce a consensus forecast to large crowd forecasting efforts with thousands of forecasters predicting on a given question. One of the first structured methods proposed to support decision making through forecasting is the Delphi method developed by the RAND Corporation in the 1950s in the context of the Cold War [@dalkeyExperimentalApplicationDELPHI1963; @PrescribingAustraliansLiving]. Forecasts were elicited in a structured process where experts would provide a first estimate, then discuss their estimates and potential disagreements, and then provide a final round of estimates. 

Outside of academia, human predictions and estimates are routinely used to help decision making in private companies, think tanks and governments. In addition, public predictions on various topics of interest have been collected on several online prediction markets or prediction platforms. Prediction markets such as PredictIt, Manifold Markets, Polymarket [CITATIONS] and traditional betting platforms such as betfair allow users to place bets on an outcome by spending either real money or token money to buy "yes" or "no"-shares in the binary outcome of a given market. Prediction platforms such as Metaculus, INFER or Good Judgement Open elicit direct forecasts from users and offer either points or sometimes monetary rewards for the most accurate users. These markets and platforms have been used in predicting a broad spectrum of events such as elections (CITATION US), wars, or the spread of infectious diseases. Academic authors working on human judgement forecasting have in the past either collaborated with existing prediction platforms (CITATIONS McAndrew, Tetlock), used more traditional means of eliciting predictions such as surveys (CITATION Recchia) or the Delphi method (CITATION), or developed their own methods (CITATION Farrow). 

Several reviews have been published on human judgement forecasting, beginning with Armstrong et al. (1986). 
(Bunn and Wright, 1991; Goodwin and Wright, 1993; Lawrence et al., 2006; Webby and O'Connor, 1996) (focusing on comparing performance of humans and models).

@zellnerSurveyHumanJudgement2021 (https://royalsocietypublishing.org/doi/10.1098/rsos.201187) provides a comprehensive literature review of human judgement forecasting approaches. @arvanIntegratingHumanJudgement2019 gives a review of academic papers that combine human judgement and mathematical modelling, focusing on product forecasting. @mcandrewAggregatingPredictionsExperts2021 review methods to aggregate predictions from experts. 

While human judgement forecasting has a long tradition in many other academic fields such as such as geo- politics [19, 20], demand forecasting, meta-science [21, 22], sports [23], its application to infectious diseases has only recently attracted more widespread attention. Notable examples of human judgement forecasting of infectious diseases include 
- @farrowHumanJudgmentApproach2017
- @mcandrewExpertJudgmentModel2022
- @mcandrewAggregatingHumanJudgment2022
- @mcandrewChimericForecastingCombining2022
- @mcandrewEarlyHumanJudgment2022
- @recchiaHowWellDid2021
- @daviesHumanJudgementForecasting2022. 

What has been done with what success / performance. 

However, not that much work done to compare human forecasts against model-based predictions in epi (has been done in other fields).

From paper: 
However, with the notable exception of [11], these forecasts were not designed to be evaluated alongside model-based forecasts and usually follow their own (often binary) prediction formats. 
Direct human forecasts may be able to take into account insights and relationships between variables which are hard to specify using epidemiological models. However, it is not entirely clear in which situations human forecasts perform well or badly. For example, @farrowHumanJudgmentApproach2017 found that humans could outperform computer models at predicting the 2014/15 and 2015/16 flu season in the US, a setting where the disease was well known and information about previous seasons was available. However, humans tended to do slightly worse at predicting the 2014/15 outbreak of chikungunya in the Americas, a disease previously largely unobserved and unknown in these regions at the time.



- Comparisons between forecasts and models

- Open things: 
  - direct 1:1 comparison
  - analysis of what human judgment adds to a computer model
  - no open source tools to 

### Crowdforecastr

- Crowdforecastr tool
  - R shiny
  - probabilistic forecasting




## Mathematical modelling of infectious diseases

- uses computers
- necessarily some combination of human judgement and algorithmic assumptions
- varying degrees of human intervention. 


From paper: 
Individual computational models usually rely to varying degrees on mechanistic assump- tions about infectious disease dynamics (such as SIR-type compartmental models that aim to represent how individuals move from being susceptible to infected and then recovered or dead). Some are more statistical in nature (such as time series models that detect statistical pat- terns without explicitly modelling disease dynamics). How exactly such a mathematical or computational model is constructed and which assumptions are made depends on subjective opinion and judgement of the researcher who develops and refines the model. Models are commonly adjusted and improved based on whether the model output looks plausible to the researchers involved.

### Mechanistic models

- mechanistic assumptions about disease progress
- learning about paramters 
- SIR models

### Statistical models

- uses statistical features, without making any assumptions about the disease progress

### Semi-mechanistic models

Mixture of the two



## Data sources

### German and Polish Forecast Hub

- cite papers
- one of several Forecast Hubs. 

### Human forecasts of COVID-19 in Germany and Poland

- Collected using crowdforecastr
- Ethics approval: NUMBER

### European Forecast Hub 


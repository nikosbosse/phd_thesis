---
output:
  pdf_document: default
  html_document: default
---
# Introduction {#intro}

## Motivation

Much of the work presented in this PhD was motivated by issues and questions that arose from the rapid real-time response to the COVID-19 pandemic. At the time, researchers in the UK and all across the world strived to provide accurate and timely forecasts of relevant COVID-19 metrics to decision makers in a setting characterised high uncertainty. There was uncertainty about relevant disease parameters such as the generation interval \citep{wallingaHowGenerationIntervals2006} or transmission routes, but also uncertainty about what kinds of models were suitable to generate accurate forecasts, or how those forecasts should best be evaluated. Before COVID-19, modelling and forecasting of infectious diseases has helped inform decision making in many different settings and for various diseases such as dengue fever \citep{johanssonOpenChallengeAdvance2019, yamanaSuperensembleForecastsDengue2016}, influenza \citep{biggerstaffResultsCentersDisease2016, reichCollaborativeMultiyearMultimodel2019, mcgowanCollaborativeEffortsForecast2019}. During the COVID-19 epidemic however, the attention given to infectious disease modelling and forecasting by both decision makers and the general public increased dramatically \citep{funkShorttermForecastsInform2020, cramerEvaluationIndividualEnsemble2021, bracherPreregisteredShorttermForecasting2021, sherrattPredictivePerformanceMultimodel2022}. 

The usefulness of a model or forecast, of course, depends on how accurately it is able to capture existing and future disease dynamics. Accuracy needs to be measured in order to be able to improve on existing models and forecasts or in order to make a decision about the degree to which they should influence decision making. When evaluating and comparing multiple models or forecasters, we can select from a large variety of scores and metrics that assess predictive performance by comparing forecasts against observed data. Different metrics reward or penalise certain behaviours of forecasts differently \citep{gneitingStrictlyProperScoring2007} and the choice of the metric hence influences the result of the evaluation. It is therefore important to identify and use metrics that capture what forecast consumers actually care about \citep{bracherEvaluatingEpidemicForecasts2021, bosseTransformationForecastsEvaluating2023}. Past literature gives little guidance on what kinds of metric are especially appropriate in an epidemiological context and how to best present results. The desire for better evaluation tools and the uncertainty about how forecasts can best be evaluated in an epidemiological context is the motivation for a major part of this thesis.

One aspect makes interpreting model performance and decision making in the beginning of a new disease outbreak especially challenging: data on the past accuracy of a forecaster or model is usually sparse as there are only few data points available. It is therefore important to establish an understanding of the underlying characteristics of different types of modelling and forecasting approaches that could help estimate a priori how trustworthy different predictions may be. A similar decision that needs to be made in the context of an early disease outbreak is to allocate resources to different types of modelling and forecasting approaches. For example, instead of spending resources on developing mathematical models, one could instead (or in addition) survey experts directly. Infectious disease modelling usually requires significant amount of resources in terms of time and effort. Additionally, the resulting models normally represents a mixture of mathematical model assumptions and human judgement required to develop and tune the model. It is therefore useful to ask how well human judgement and mathematical modelling perform in comparison and what mathematical modelling is able to add above human judgement alone. This question inspired the second major part of the work presented in this thesis. 

## Aims and objectives

The aim of this thesis is to help improve the usefulness of infectious disease forecasting for public health decision making in future outbreaks such as the COVID-19 pandemic by obtaining a deeper understanding of 

- What a 'good' forecast is in an epidemiological context, and how we can evaluate the quality of a forecast. 
- How human judgement and mathematical modelling compare in terms of predictive performance and how they can best be used to obtain useful forecasts. 

It strives to accomplish this by fulfilling the following objectives: 

- Establish appropriate tools to evaluate predictions in `R` and review best practices in forecast evaluation (Paper 1, Chapter \@ref(sec:scoringutils)).
- Develop tools to elicit human forecasts of infectious diseases, specifically COVID-19 (Paper 2)
- Analyse the role of human judgement in forecasting COVID-19 in Germany and Poland. Compare human judgement forecasts against model-based predictions and analyse the added benefit of human input over mathematical models (Paper 2, Chapter \@ref(sec:GermanyPoland)).
- Improve current evaluation methods so that they are betters suited for evaluating forecasts in an epidemiological context (Paper 3, Chapter \@ref(sec:LogTransformation)).
- Examine the potential to use public crowd forecasting tournaments to predict COVID-19 in the UK and explore possibilities to combine human judgement and epidemiological modelling (Paper 4, \@ref(sec:UKforecastingChallenge)).
- Examine whether results obtained in Paper 2 can replicate in a different setting with a different crowd of forecasters (Paper 4). 

<!-- 2. Collect predictions of COVID-19 from humans in Germany, Poland. Compare these human -->
<!-- predictions against model-based forecasts to discern relative strengths and weaknesses of human -->
<!-- forecasters vs. model-based approaches -->
<!-- 3. Collect human forecasts of reported cases and deaths from COVID-19 in the UK as well as -->
<!-- human predictions of the effective reproduction number Rt to explore ways in which human -->
<!-- insight and epidemiological modelling can be combined -->

## Thesis outline

Chapter \@ref(background) provides some background for the following Chapters. It defines important terminology related to forecasting and modelling of infectious diseases used throughout this thesis. It introduces the concept of forecast evaluation and proper scoring rules and provides an overview of past human judgement efforts in infectious disease forecasting. 

Chapter \@ref(sec:scoringutils) (Paper 1) lays the foundation for this thesis by establishing an understanding of forecast evaluation and providing the tools necessary to conduct comparative forecast evaluations. It goes into detail on different existing approaches to evaluate forecasts and when they are appropriate. It introduces `scoringutils`, a software package in \textsf{R} that implements a selection of proper scoring rules and other evaluation metrics and offers users a coherent framework for forecast evaluation in \textsf{R}. The `scoringutils` package, as well as the theoretical aspects discussed in Chapter \@ref(sec:scoringutils) provide the basis for all following chapters. 

Chapter \@ref(sec:GermanyPoland) (Paper 2) applies the evaluation methods described in Chapter \ref{sec:scoringutils} to investigate what human judgement can contribute to the task of forecasting COVID-19. It presents a study conducted in Germany and Poland where human judgement forecasts were submitted to the German and Polish COVID-19 Forecast Hub alongside mathematical models with minimal tuning. The study was motivated both by a desire to obtain a better understanding of how to create good forecasts of COVID-19, as well as the actual need to produce timely and accurate forecasts to the German and Polish Forecast Hub. The study compares human judgement forecasts elicited using a novel open source online application against predictions from two minimally-tuned mathematical models, as well as with an ensemble of model-based predictions. 

Chapter \@ref(sec:LogTransformation) (Paper 3) investigates in greater detail how forecasts should be evaluated specifically in an epidemiological context. The scoring methods discussed in Chapter \ref{sec:scoringutils} were not explicitly developed for application in infectious disease forecasting, but rather describe general mathematical relationships that are detached from the actual context. Evaluating the forecasts we submitted to the German and Polish Forecast Hub, as described in Chapter \ref{sec:GermanyPoland}, surfaced issues with the way that forecasts are currently commonly evaluated in epidemiology. Determining predictive performance based on the absolute distance between forecast and observation, as is common practice, neglects the exponential nature of infectious disease processes. It also leads to scores that are dominated by outlier forecasts, especially during periods of high incidence, and makes it hard to compare forecasts across time, locations or forecast targets. To address these issues, Chapter \@ref(sec:LogTransformation) introduces the idea of transforming forecasts and observations before applying a score in order to obtain an evaluation that is more adequate in an epidemiological context. 

Chapter \@ref(sec:UKforecastingChallenge) (Paper 4) presents a follow-up study to the one presented in Chapter \ref{sec:GermanyPoland}. It analyses the results of a public forecasting challenge in the UK, 
applying insights from Chapter \@ref(sec:LogTransformation) on how to evaluate forecasts in an epidemiological context. Forecasts were submitted to the European COVID-19 Forecast Hub and again compared to the ensemble of all forecasts submitted to the Forecast Hub, this time also taking transformations of forecasts into account for the evaluation. In addition, Chapter \@ref(sec:UKforecastingChallenge) explores a novel way to combine human judgement and mathematical modelling by asking forecasters to predict the effective reproduction number $R_t$ which then gets mapped to cases and deaths using an epidemiological model. 

Chapter \@ref(sec:discussion) discusses the results and implications of the work presented in this thesis. 

<!-- ## Code -->

<!-- - this thesis -->
<!-- - scoringutils -->
<!-- - crowdforecastr -->
<!-- - Germany and Poland -->
<!-- - Log or not -->
<!-- - UK forecasting challenge -->

<!-- ## Additional achievements -->
<!-- - talks -->


<!-- Some text.  -->

<!-- Policy makers have recently started to rely on forecasts to make decisions. Accurate knowledge of the future is immensely valuable in all sorts of areas from farming to economics to public health. With the rise of the novel coronavirus SARS-CoV-2, statistical forecasting has gathered renewed attention. As the virus has spread over the globe, more and more research teams began forecasting the trajectory of the pandemic to help inform public policy. Several countries like the United States, Germany and the United Kingdom have therefore started to aggregate forecasts from different teams. Among these efforts, the US Forecast Hub [@umass-amherstinfluenzaforecastingcenterofexcellenceCovid19forecasthubOrg2020] is the largest and most visible. Its goal is to collect forecasts, to aggregate them, and to make them available to policy makers and the general public in the best possible way. Two questions have been at the centre of these efforts: The first is "how can we best evaluate the performance of a model?" The second is "how can we combine and aggregate different models to get the best possible prediction?". These two questions will be our guiding questions as well throughout this work.  -->












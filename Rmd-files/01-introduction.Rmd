---
output:
  pdf_document: default
  html_document: default
---
# Introduction {#intro}

## Motivation

Much of the work presented in this PhD was motivated by issues and questions that arose from the rapid real-time response to the COVID-19 pandemic. At the time, researchers in the UK and all across the world strived to provide accurate and timely forecasts of relevant COVID-19 metrics to decision makers in a setting characterised by high uncertainty. There was uncertainty about relevant disease parameters such as the generation interval \citep{wallingaHowGenerationIntervals2006} or transmission routes, but also uncertainty about what kinds of models were suitable to generate accurate forecasts, or how those forecasts should best be evaluated. Before COVID-19, modelling and forecasting of infectious diseases has helped inform decision making in many different settings and for various diseases such as dengue fever \citep{johanssonOpenChallengeAdvance2019, yamanaSuperensembleForecastsDengue2016}, influenza \citep{biggerstaffResultsCentersDisease2016, reichCollaborativeMultiyearMultimodel2019, mcgowanCollaborativeEffortsForecast2019}. During the COVID-19 epidemic however, the attention given to infectious disease modelling and forecasting by both decision makers and the general public increased dramatically \citep{funkShorttermForecastsInform2020, cramerEvaluationIndividualEnsemble2022, bracherPreregisteredShorttermForecasting2021, sherrattPredictivePerformanceMultimodel2022}. 

The usefulness of a model or forecast, of course, depends on how accurately it can capture existing and future disease dynamics. Accuracy needs to be measured to be able to improve on existing models and forecasts or to make a decision about the degree to which they should influence decision making. When evaluating and comparing multiple models or forecasters, we can select from a large variety of scores and metrics that assess predictive performance by comparing forecasts against observed data. Different metrics reward or penalise certain behaviours of forecasts differently \citep{gneitingStrictlyProperScoring2007} and the choice of the metric hence influences the result of the evaluation. It is therefore important to identify and use metrics that capture what forecast consumers actually care about \citep{bracherEvaluatingEpidemicForecasts2021, bosseTransformationForecastsEvaluating2023}. There exists an extensive body of literature on scoring rules and ways to evaluate forecasts in a variety of settings. Past evaluations of epidemiological forecasts drew from this literature, but little work went into how to score forecasts in an epidemiological context specifically, and how to best present results. The desire for better evaluation tools and the open questions surrounding the evaluation of forecasts in an epidemiological context were the motivation for a major part of this thesis.

One aspect makes interpreting model performance and decision making in the beginning of a new disease outbreak especially challenging: data on the past accuracy of a forecaster or model is usually sparse as there are only few data points available. It is therefore important to establish a general understanding of the underlying characteristics of different types of modelling and forecasting approaches that could help estimate a priori how trustworthy different predictions may be. This might provide information on which kinds of forecasting approaches may be useful in future infectious disease outbreaks. In the context of an early disease outbreak, resources need to be allocated to different types of modelling and forecasting approaches. For example, instead of spending resources on developing mathematical models, one could instead (or in addition) survey experts directly. Infectious disease modelling usually requires a significant amount of resources in terms of time and effort. The resulting models represent a mixture of mathematical model assumptions and human judgement required to develop and tune the model. It is therefore useful to ask how well human judgement and mathematical modelling perform in comparison and what mathematical modelling is able to add above human judgement alone. This question inspired the second major part of the work presented in this thesis. 

## Aims and objectives

This thesis aims to help improve the usefulness of infectious disease forecasting for public health decision making in future outbreaks such as the COVID-19 pandemic by obtaining a deeper understanding of the following. 

- What is a 'good' forecast in an epidemiological context, and how we can evaluate the quality of a forecast?
- How do human judgement and mathematical modelling compare in terms of predictive performance? How do human judgment and mathematical modelling interplay and how can they best be used and combined to obtain useful forecasts?

It strives to accomplish this by fulfilling the following objectives: 

- Establish appropriate tools to evaluate predictions in `R` following best practices in forecast evaluation (Paper 1, see Chapter \@ref(sec:scoringutils)).
- Develop tools to elicit human forecasts of infectious diseases, specifically COVID-19 (Paper 2, see Chapter \@ref(sec:GermanyPoland))
- Analyse the role of human judgement in forecasting COVID-19 in Germany and Poland. Compare human judgement forecasts against model-based predictions and analyse the added benefit of human input over mathematical modelling (Paper 2, see Chapter \@ref(sec:GermanyPoland)).
- Improve current evaluation methods so that they are better suited for evaluating forecasts in an epidemiological context (Paper 3, see Chapter \@ref(sec:LogTransformation)).
- Examine the potential to use public crowd forecasting tournaments to predict COVID-19 in the UK and explore possibilities to combine human judgement and epidemiological modelling (Paper 4, see Chapter \@ref(sec:UKforecastingChallenge)).
- Examine how well results from one human judgement forecasting effort replicate in a different setting with a different crowd of forecasters (Paper 4, see Chapter \@ref(sec:UKforecastingChallenge)). 

## Thesis outline

Chapter \@ref(background) provides some background for the following Chapters. It defines important terminology related to forecasting and modelling of infectious diseases used throughout this thesis. It reviews core concepts related to forecast evaluation and proper scoring rules and provides an overview of past human judgement efforts in infectious disease forecasting. 

Chapter \@ref(sec:scoringutils) (Paper 1) introduces `scoringutils`, a software package in \textsf{R} that implements a selection of proper scoring rules and other evaluation metrics and offers users a coherent framework for forecast evaluation in \textsf{R}. The `scoringutils` package provides the basis for all forecast evaluations conducted as part of this thesis. 

Chapter \@ref(sec:GermanyPoland) (Paper 2) applies the evaluation methods and tools described in Chapters \@ref(background) and \ref{sec:scoringutils} to investigate what human judgement can contribute to the task of forecasting COVID-19. It presents a study conducted in Germany and Poland where human judgement forecasts were submitted to the German and Polish COVID-19 Forecast Hub alongside mathematical models with minimal tuning. The study was motivated both by a desire to obtain a better understanding of how to create good forecasts of COVID-19, as well as the actual need to produce timely and accurate forecasts for the German and Polish Forecast Hub. The study compares human judgement forecasts elicited using a novel open source online application against predictions from two minimally-tuned mathematical models, as well as with an ensemble of model-based predictions. 

Chapter \@ref(sec:LogTransformation) (Paper 3) investigates in greater detail how forecasts should be evaluated specifically in an epidemiological context. The scoring methods discussed in Chapters \@ref(background) and \ref{sec:scoringutils} were not explicitly developed for application in infectious disease forecasting, but rather describe general mathematical relationships that are detached from the actual context. Evaluating the forecasts we submitted to the German and Polish Forecast Hub, as described in Chapter \ref{sec:GermanyPoland}, surfaced issues with the way that forecasts are currently commonly evaluated in epidemiology. Determining predictive performance based on the absolute distance between forecast and observation, as is common practice, neglects the exponential nature of infectious disease processes. It also leads to scores that are dominated by outlier forecasts, especially during periods of high incidence, and makes it hard to compare forecasts across time, locations or forecast targets. To address these issues, Chapter \@ref(sec:LogTransformation) introduces the idea of transforming forecasts and observations before applying a score in order to obtain an evaluation that is more adequate in an epidemiological context. 

Chapter \@ref(sec:UKforecastingChallenge) (Paper 4) presents a follow-up study to the one presented in Chapter \ref{sec:GermanyPoland}. It analyses the results of a public forecasting challenge in the UK, 
applying insights from Chapter \@ref(sec:LogTransformation) on how to evaluate forecasts in an epidemiological context. Forecasts were submitted to the European COVID-19 Forecast Hub and again compared to the ensemble of all forecasts submitted to the Forecast Hub, this time also taking transformations of forecasts into account for the evaluation. In addition, Chapter \@ref(sec:UKforecastingChallenge) explores a novel way to combine human judgement and mathematical modelling by asking forecasters to predict the effective reproduction number $R_t$ which then gets mapped to cases and deaths using an epidemiological model. 

Chapter \@ref(sec:discussion) discusses the results and implications of the work presented in this thesis. 

## Code

The code for this thesis is publicly available on GitHub\footnote{\url{https://github.com/nikosbosse/phd_thesis}}, as is the code for the scoringutils package and the accompanying Paper (Paper 1)\footnote{\url{https://github.com/epiforecasts/scoringutils}}, the code for the Paper on crowd forecasts in Germany and Poland (Paper 2)\footnote{\url{https://github.com/epiforecasts/covid.german.forecasts}}, the code for the Paper on transforming forecasts before scoring them (Paper 3)\footnote{\url{https://github.com/epiforecasts/transformation-forecast-evaluation}}, and the code for the Paper on the UK Crowd Forecasting Challenge (Paper 4)\footnote{\url{https://github.com/epiforecasts/uk-crowd-forecasting-challenge}}. 

<!-- ## Additional achievements -->
<!-- - talks -->


<!-- Some text.  -->

<!-- Policy makers have recently started to rely on forecasts to make decisions. Accurate knowledge of the future is immensely valuable in all sorts of areas from farming to economics to public health. With the rise of the novel coronavirus SARS-CoV-2, statistical forecasting has gathered renewed attention. As the virus has spread over the globe, more and more research teams began forecasting the trajectory of the pandemic to help inform public policy. Several countries like the United States, Germany and the United Kingdom have therefore started to aggregate forecasts from different teams. Among these efforts, the US Forecast Hub [@umass-amherstinfluenzaforecastingcenterofexcellenceCovid19forecasthubOrg2020] is the largest and most visible. Its goal is to collect forecasts, to aggregate them, and to make them available to policy makers and the general public in the best possible way. Two questions have been at the centre of these efforts: The first is "how can we best evaluate the performance of a model?" The second is "how can we combine and aggregate different models to get the best possible prediction?". These two questions will be our guiding questions as well throughout this work.  -->

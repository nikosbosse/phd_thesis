# Discussion 
\label{sec:discussion}

<!-- Summary of key results -->

<!-- Strengths and limitations -->

<!-- Contributions of this research relative to previous knowledge -->

<!-- Implications and future research opportunities -->


## Summary and contributions to existing work

The work presented in this thesis has made several contributions related to forecast evaluation and human judgement forecasting in epidemiology. 

### Forecast evaluation {-}

The first major contribution is towards improving the evaluation of infectious disease forecasts, both from a practical and a theoretical perspective. 

#### scoringutils {-}
The `scoringutils` package helps make forecast evaluation and the necessary tools more accessible to researchers and decision makers. The package implements a suite of proper scoring rules and other scoring metrics in a flexible framework and offers extensive documentation to guide users through the process. The paper reviews the theoretical foundation of forecast evaluation and provides justification for the choice of a particular score over another. Most of the scores implemented in `scoringutils` had been implemented previously. Notable existing packages are `scoringRules` \citep{scoringRules}, `topmodels` \citep{R-topmodels}, `tscount` \citep{R-tscount} and `Metrics` \citep{R-Metrics}, among others. `scoringutils` improves usability over existing software and is especially geared towards comparing forecasts from different models, regardless of how those forecasts were generated. All forecasts can be scored and summarised in a convenient `data.frame` format and the package offers functionality to compare performance visually across different dimensions and account for missing forecasts. `scoringutils` has since been used in published and unpublished work supporting major public health organisations such the US Centers for Disease Control and Prevention, the European Centre for Disease Prevention and Control, the UK Health Security Agency, and Médecins Sans Frontièrs. 

#### Scoring epidemiological forecasts on transformed scales {-}
In addition to improving the tools available for forecast evaluations, this thesis made a theoretical contribution to our understanding of forecast evaluations in the context of epidemiology. We argued that transforming forecasts and observations prior to applying the continuous ranked probability score (CRPS) or the weighted interval score (WIS) makes it possible to obtain more meaningful results than with the untransformed CRPS and WIS. Both the WIS and the CRPS measure the absolute distance between the forecast and the observation. Scores therefore tend to increase with the order of magnitude of the target and do not take the exponential nature of infectious disease processes into account. This can be mitigated by transforming the forecasts and observations before scoring. The natural logarithm is a particularly attractive transformation in an epidemiological context, as the resulting score represents a measure of how well the forecasts predicted the exponential growth rate. The CRPS applied to log-transformed forecasts can also be understood as an approximate probabilistic version of the symmetric relative error. Furthermore, the natural log transformation can serve as a variance-stabilising transformation, helping to make scores more comparable across time, locations and forecast targets. We compared scores on the natural scale and on the log scale for a set of forecasts submitted to the European COVID-19 Forecast Hub and found that rankings between models changed. Scores were more evenly distributed across time, locations and forecast targets on the log scale. Forecasters were less severely penalised for missing the peak on the log scale, and received higher penalties for missing an upswing of incidences. 

### Human judgement {-}

The second major contribution of this thesis is in improving our understanding of the role of human judgement in infectious disease forecasting and the potential and limitations of human judgement forecasts. 

In order to be able to compare probabilistic forecasts from human forecasters against computational models, we developed a new open source platform and R package, `crowdforecastr`, which allows the elicitation of probabilistic time series forecasts from individual forecasters. When we submitted our forecasts to the German and Polish Forecast Hub in 2020 and 2021, our study was the first one in an epidemiological context to compare full predictive distributions from individual users against model-based predictions. A previous study by \cite{farrowHumanJudgmentApproach2017} had elicited point forecasts from individual participants and combined them to a probabilistic forecasts based on the mean and variation in participants' predictions. 

In the first study presented in Chapter \ref{sec:GermanyPoland}, we compared a small crowd of human forecasters against two minimally-tuned epidemiological models and an ensemble of model-based predictions submitted to the German and Polish Forecast Hub. These submissions contributed to a shared effort to inform the public and public health decision makers in Germany and Poland during the earlier phases of COVID-19. We found predictions for cases from human forecasters to be slightly better in terms of the weighted interval score than forecasts of the Hub ensemble, but worse when predicting deaths. Our minimally-tuned model forecasts performed comparable to our crowd forecasts for short horizons, but noticeably worse for longer horizons. This suggests that human judgement is beneficial in guiding model-based predictions when conditions change over time. We also found that even forecasts that are worse than an pre-existing ensemble can help to improve ensemble forecasts when including it in that ensemble. 

In the second study in the UK presented in Chapter \ref{sec:UKforecastingChallenge}, we repeated the basic set-up of the first study with a larger number of participants in a different country. Forecasts were elicited as part of a public forecasting challenge over the course of 13 weeks and submitted to the European COVID-19 Forecast Hub. In addition, we tested a novel forecasting approach in which users submitted a forecast of the effective reproduction number $R_t$, which would then get mapped to reported cases and deaths using the R package `EpiNow2` \citep{epinow2}. Following the conclusions presented in Chapter \ref{sec:LogTransformation} we evaluated forecasts using both untransformed and log-transformed predictions and observations. We found performance of human and model-based forecasts to be overall comparable. The Hub ensemble performed slightly better than human forecasts on the natural scale, and slightly worse when evaluated on the log scale. Our novel $R_t$ approach performed comparable to other forecasts when predicting cases (which forecasters could observe directly), and noticeably worse for death predictions (which forecasters couldn't see), suggesting that the underlying model did not accurately capture the relation between cases and deaths. 


## Limitations

The work presented in this thesis is subject to various important limitations. This section gives an overview and summary of those limitations, while further details can be found in the corresponding chapters. 

### Forecast evaluation {-}

In terms of the tools developed in this thesis, the `scoringutils` package is still under active development, with new issues opening regularly on Github. One particular area of improvement is usability and robustness. Currently the package lacks robust error handling and input checking, resulting at times in rather cryptic error messages. While tests are in place for most functions, new bugs may also be discovered in the future. 
 
In terms of the theoretical advancements regarding transformations of forecasts before scoring, more work needs to be done. Theoretical considerations suggest that log-transforming forecasts before applying the CRPS or WIS yields a score that better reflects the exponential nature of infectious disease processes. However, whether or not scores on the log scale appear indeed more meaningful to researchers and policy makers in practice remains to be seen. So far we have only conducted a small case study in order to illustrate the effects of transforming forecasts before evaluation. More work is needed to obtain a more complete understanding of the behaviour of scores on the log scale across different applications. 

Several practical issues arise when transforming forecasts. The natural logarithm, for example, does not allow any negative or zero values in the forecasts or observations. In Chapter \ref{sec:LogTransformation} we suggested to add a small value to all observations and forecasts in order to deal with zero values. This does not break propriety, but introduces more degrees of freedom and changes the scores in subtle ways. How to best deal with negative values remains unclear. These may not be a problem in an epidemiological context, where observations are usually counts, but may hinder application in other contexts. Issues may arise even when no zeros or negative values are present, especially when forecasts or observations are small. A small absolute difference between forecast and observation can translate to a large relative difference, causing scores to blow up. This issue can arise in particular if forecasts and observations are restricted to assume integer values. Users may find that forecasts made for small targets can dominate overall scores in an undesirable way, depending on the relationship between mean and variance of the forecast target. \cite{bracherEvaluatingEpidemicForecasts2021} argued before that the fact that CRPS scales with the forecast target conveys meaningful information that gets lost when log-transforming targets. 
Some desirable transformations other than the natural logarithm, such as converting absolute forecasts to forecasts of weekly growth rates by dividing every predicted value by the value in the week before, are restricted to forecasts that are stored in a sample-based format. 


### Human judgement {-}

This thesis has studied the potential of human judgement to forecast infectious diseases such as COVID-19. Both studies, however, suffered from a low number of participants and it is unclear how well results could generalise to other settings. 

The first study in Germany and Poland, presented in Chapter \ref{sec:GermanyPoland}, had a median of six participants per week (the Hub ensemble had a similar amount of ensemble models). Participants were also recruited in a very ad hoc fashion among friends and colleagues, making the sample not representative. In this sense, the study is maybe better understood as a case study of an acute outbreak response effort using human judgement forecasts. The overall evaluation was made difficult by the fact that scores varied a lot from week to week and across locations and forecast horizons. This introduced many researcher degrees of freedom, as results and interpretations could change depending on how forecasts were evaluated. For the study in Germany and Poland, we have only looked at forecasts on the natural scale and therefore lack knowledge of what results would have looked like on the log scale. 

Given the very context-dependent nature of human judgement forecasting, results may not generalise well to other settings. While we attempted to replicate some our findings in Germany and Poland in a second study, there are a few factors that make a direct comparison difficult. In the first study, human forecasts were combined using a mean ensemble, while in the second study we used a quantile-wise median (following the practice adopted by all COVID-19 Forecast Hubs). Changes in the number and composition of forecasters, the shorter time horizon and the different setting (including vaccination and a different COVID-19 variant) also limit comparability. In both studies, human forecasts were compared against an ensemble of model-based predictions. However, those ensembles differed both in terms of their size and their composition. 

All forecasts were only analysed at an aggregate level. We therefore could not obtain an understanding of how individuals contributed to the overall ensemble forecasts. For example, we cannot know how the number of participants influenced overall performance, or whether participants learned over time. Results may have looked different had we successfully retained participants over the course of the studies. In both our studies, most participants only submitted a single forecast, which may have affected overall performance. In addition, we treated forecasters more or less as black boxes, without qualitatively investigating their thought processes in detail. 

We asked forecasters to self-identify as "experts" by whether or not they worked in infectious disease modelling or had professional experience in any related field. However, the value of that information was limited in a few ways. Firstly, we were not able to check participants' statements. Secondly, the question we asked was perhaps too broad to provide a useful measure of the participant's actual expertise for the task at hand. Something like a short quiz may have helped to get more detailed information on the participants' level of expertise. 

One aim of the study in Germany and Poland was to improve our understanding of the relative contributions of human judgement forecasting and epidemiological modelling. However, our approach of comparing direct human forecasts with minimally-tuned epidemiological models was likely flawed and may not have entirely achieved this goal. Firstly, there was a partial, but not complete overlap between the researchers who designed our minimally-tuned models and the participants of the forecasting study. Had these two groups either been disjunct or identical, a fairer comparison would have been possible. This, however, would have come at the expense of an even further reduced number of participants to the point that only one or two forecasters might have been left on a given date. Furthermore, human forecasters were allowed to use any model they liked as an input and we therefore cannot make statements about the extent to which human forecasts were guided by epidemiological modelling. 
The notion of "minimally-tuned" we used in our study is very vague, making it unclear how much our models were actually guided by human judgement rather than being just a mathematical representation of our abstract understanding about infectious disease dynamics. Furthermore, we only used two minimally-tuned models for this study. It is unclear whether our models are able to represent a general class of "minimally-tuned epidemiological models", or whether they are just two specific models with particular strengths and weaknesses that may not generalise to other models. 

For the second crowd forecasting study in the UK, we experimented with a novel forecasting approach that asked human forecasters to predict the effective reproduction number $R_t$ which then got mapped to reported cases and deaths. Forecasters were able to see a preview of the case forecast implied by their $R_t$ forecast, but could not see the corresponding prediction for deaths. Participants therefore had to rely entirely on the underlying model and were not able to adjust parameters like the CFR or the delay between cases and deaths. The specific results we obtained therefore depend very strongly on the specific model used and therefore does not necessarily reflect the overall potential of the proposed approach. 

## Implications and avenues for future work

### Forecast evaluation {-}

Broad interest in the `scoringutils` package suggests that developing and maintaining tools may be an effective way to contribute to the field of infectious disease forecasting. Unfortunately, academia provides few mechanisms to incentivise and reward the development and maintenance of tools in a collaborative and sustainable way. While creating a new tool allows the authors to get some recognition through publication of their work, there are few ways to reward researchers for contributing to existing projects or to put effort into continuously developing software after publication. This promotes an ecosystem with a large number of disconnected tools that solve parts of a larger problem, but don't interface well with each other. 
For `scoringutils`, one important challenge will be to turn it from a PhD project into a community project that is supported by a larger group of users and contributors from different institutions and backgrounds. This would make sure that the package is maintained and developed in a sustainable way in close collaboration with those that actually use it. 
In terms of actual development, a broad range of further improvements and features would be useful. One important area is usability and robustness. For example, it would be helpful to introduce more explicit checking and handling of different inputs and catching errors in order to make error messages more useful for users. Other important potential developments include support for scoring forecasts against distributions (as opposed to only scoring against observed values), adding more metrics, allowing users to pass in custom scoring functions, scoring forecasts in a binned format, and many more.


<!-- One important step forward would be to create an environment in which tools can be developed in ways that are less reliant on the initiatives of individual researchers and where tools can keep being maintained and improved even after those individuals have left.  -->


<!-- Lala The current work has highlighted difficulties and laid out paths for future work and also showed that some progress can be made. It's used a lot and important....  -->

Despite the wide-spread use of proper scoring rules, forecast evaluation is far from a solved problem and much work remains to be done. 

Firstly, we only have rudimentary understanding how the resulting scores translate into "usefulness" of the forecasts. For example, it might be much worse to underpredict the hospitalisations than overpredict them, in a way that is not reflected by ordinary scores. Or it might be that a forecast that is biased, but correctly predicts a trend is more useful than one that shows the wrong trend, but is closer to the actual values and therefore receives a higher score. If we show decision makers different forecasts, which score will choose the model that different decision makers describe as the most useful one?

Transforming forecasts before scoring is one promising way of approaching the issue. Further research into scores on the natural vs. on the log scale is needed. For example, it would be interesting to investigate whether scores on the log scale tend to be more consistent over time than those on the natural scale. Furthermore, developing and exploring new transformations may prove very useful. 
Forecast transformations may be particularly attractive in combination with the possibility to create composite scores as a linear combination of different proper scoring rules. One could for example create a new score that is a weighted combination of scores on the natural and on the log scale. Transformations may be particularly useful when forecasts are represented in the form of predictive samples. This allows, for example, evaluating the shape of the forecast trend line by dividing forecasts for horizon $h$ by the forecasts for horizon $h-1$. 
Another promising approach may be to develop custom proper scoring rules for specific applications using Bayes Acts \citep{brehmerProperizationConstructingProper2020} and general loss functions.

Secondly, we don't have a good understanding of what adequate baselines for comparisons are. For example, predicting whether or not a stone will fall is much easier than predicting whether or not a stock will rise. This is not reflected through scores that only compare the forecast and the outcome. This makes comparisons of forecasts across different settings, locations, times and forecast targets very difficult. Without an appropriate baseline to compare a forecast against, the raw score alone is somewhat meaningless. 

Thirdly, forecast evaluations suffer from a large number of researcher degrees of freedom. Results change a lot depending on choices such as what metric to use, which forecast horizon to focus on, whether forecasts are transformed before scoring etc. This introduces a subjective element into the evaluation and makes results prone to motivated reasoning in a way that is masked by the apparent objectivity of numerical evaluations. 
To mitigate this, researchers should aim to show a broad range of different scores and metrics. It could be good idea to establish shared standards of what should be reported in a forecast evaluation and how. Other ideas include pre-registration of studies or interactive visualisations of scores that forecast consumers can explore on their own. 

Fourthly, we still don't have a very good understanding for how much to trust a forecaster. In the context of the COVID-19 Forecast Hubs, models often showed consistently good performance for a long time and then suddenly broke down when circumstances changed. The Hub ensembles robustly emerged as good (and in most instances the best) choice on average across several Forecast Hubs and many evaluations. But even the ensemble often missed changes in trends and performed poorly. Many key questions still remain open: How much data is needed until we can say confidently that a model / forecaster performs well or badly? Can we identify some kind of reliability measure that would indicate how much we can trust a given forecast at a particular time? Should we use significance tests to compare models / forecasters and if so how much can we trust them? One issue with significance tests in particular is that forecasts are usually correlated across time and location, reducing the effective sample size. Also, as \cite{dieboldComparingPredictiveAccuracy2015} note, there is a difference between comparing forecasts with comparing forecasters, as the observed performance of a single set of forecasts is not necessarily representative of the performance of that forecaster in general. 

Fifthly, the high dimensionality of forecasting data poses a problem for forecast evaluation. In epidemiological setting forecasts are often made by several models at different time points for different forecast horizons, locations and forecast targets. Evaluating and visualising forecasts for different stratification of the data is cumbersome, difficult, and relies heavily on the researcher's judgement. It would be helpful to come up with robust ways to handle the dimensionality of the data better. One obvious candidate is modelling scores using a regression framework. A major obstacle, however, is that scores (at least in epidemiology) tend to be heavily skewed and dominated by outliers. Perhaps a combination of transforming forecasts before scoring and standardising scores may make it possible to use a regression framework, even if p-values for coefficients may not be reliable. 

Sixthly, we don't have established good ways to provide useful feedback for forecasters on how to improve their forecasts. The Forecast Hubs published data on scores and average performance, but struggled to provide actionable feedback. One option would be to create single-model evaluations that explore in detail when and how a model performed well or badly. A more technically sophisticated option might be to provide forecasters with an interface that allows them to manipulate past forecasts and observe how e.g. shifting forecasts or adjusting dispersion would have influenced past scores. 

<!-- Given the importance of forecast evaluations in public discourse it would be important to develop methodology further. Hubs use it and it can influence decision-making.  -->




### Human judgement forecasting {-}

This thesis has attempted to shed some light on the opportunities and limitations associated with human judgement forecasting of infectious diseases such as COVID-19. Our studies provided some evidence that a mixed crowd of human forecasters and an ensemble of model-based predictions can produce forecasts of comparable quality. However, many of the details of what drives good performance, both in models and human forecasters, are still poorly understood. 

Human judgement forecasting and mathematical modelling differ in their advantages and disadvantages. Whether or not one should aim to elicit forecasts from humans or from mathematical models therefore depends mainly on for what purpose and in which circumstances forecasts are to be used. Eliciting both human judgement forecasts and model-based predictions for the same thing is useful in order to obtain an understanding for the relative performance and relative strength of the two different approaches. Going forward, however, obtaining forecasts for the exact same targets may not be the best use of resources. Rather, an important question in the future is how to elicit forecasts in a way that plays to the respective strengths of humans and mathematical models. 
Human judgement may be particularly useful in situations where we're interested in questions that cannot easily captured by modelling or where the relevant information is hard to feed into models. Human judgement forecasting requires a lot of effort though, and past studies often struggled to retain participants over a long time. Both our studies suffered from low participation. For the first study, the low number of participants was in part due to time constraints - the first forecasts were submitted within three weeks of the start of this PhD. However, even with more time and preparation, as was possible for the second study in the UK, we found it hard to recruit and retain large number of participants. 
Generating forecasts over a prolonged period of time may be easier using mathematical models that can be automated and only have to be adapted occasionally when circumstances change. Mathematical modelling may also be advantageous in settings with good data quality and for forecasting tasks that benefit from the ability to do complex calculations. Another advantage of mechanistic models is that they may help us to understand the underlying infectious disease process better and apply those learnings to other settings. 

One interesting question that arose in this thesis was whether humans might have an advantage when predicting cases, whereas computational models might be better at forecasting a lagged quantity such as deaths. This was suggested by the results from our first study in Germany, as well as by \cite{mcandrewChimericForecastingCombining2022}. Our second study in the UK did not confirm this. However, performance when forecasting deaths in our second study may have been affected by changes in the age composition of cases and the rise of the Delta variant in the UK. It could be possible that humans found it easier to adapt to changes in the case fatality ratio than models, possible implying that humans generally tend to be good at incorporating uncertain information and adapting to novel circumstances. Computational models, on the other hand, could be better at estimating the delay distribution between cases and deaths precisely, if circumstances stay constant. This 'story' seems compelling, which is exactly why we should treat it with caution. Given the small number of studies and low sample sizes, we cannot draw strong conclusions and there is a high risk of overinterpreting noise. Ultimately, this highlights that we don't really have a sufficiently good understanding of how and why different forecasting approaches perform well. 

One straightforward way to obtain a better understanding of human judgement forecasting would be to conduct qualitative interviews with forecasters. This could shed light on which factors forecasters take into account and the reasoning they apply. It could also help us understand in how far reasoning of experts and non-experts differs and provide information on how to improve predictive performance in the future. 

Qualitative interviews could be used not only to obtain insights on human judgement forecasting, but also to get a better understanding of the role of human judgement in infectious disease modelling more generally. In our original study in Germany and Poland we described the output of mathematical modelling as a mixture between abstract mathematical assumptions and the subjective human judgement of the researchers developing the models. One aim of the study was to obtain a better understanding of the influence and effect of that human judgement has in the model development process. As discussed in the last section on limitations, it is questionable whether we achieved this to the extent that had intended for. One way to address this question more directly would have been to conduct qualitative interviews with modellers who submitted models to the COVID-19 Forecast Hubs.

Another interesting way of obtaining a better understanding of the influence of human judgement in infectious disease forecasting would be to ask humans to directly manipulate model outputs. This could either be done with the researchers themselves who developed the models, or a different set of forecasters. It would then be possible to analyse how human judgement alone, model-based predictions alone, or a combination of the two would perform. This would address our original research question more directly and could lead to promising ways of improving on the accuracy of both human and model-based predictions. Importantly, general purpose tools that would allow users to adapt arbitrary forecasts are not readily available at present. This is somewhat astonishing, given the widespread use of forecasting and the fact that researchers, at least anecdotally, often meaningfully disagree with aspects or details of their own model-based predictions. 

Combining human judgement and model-based predictions in order to mitigate weaknesses of any one approach is appealing. Related efforts generally follow two aims: improving predictive performance, and making forecasting efforts more scalable by reducing the cognitive load for humans. Several promising ideas have been proposed. One, having human forecasters manually adapt model outputs, was mentioned in the last paragraph. A second one, asking human forecaster to predict a quantity such as $R_t$ or the growth rate of an infectious disease process, was discussed in our paper on the UK Crowd Forecasting Challenge in Chapter \ref{sec:UKforecastingChallenge}. While performance in our study was poor, users quite liked the interface. The study explored an early prototype and there are several improvements that could be made, such as allowing forecasters to see the implied forecasts for deaths and influence it through adjusting the CFR. Another approach is to use human judgement in order to estimate or forecast parameters that are then used as an input to mathematical models \citep[see e.g.][]{venkatramananUtilityHumanJudgment2022a}. This, however, may be not be easy as interpreting and estimating technical parameters such as the serial interval requires expert subject matter knowledge. One could, however, for example ask participants to predict the timing and magnitude of a peak and use that information as a prior to constrain plausible model scenarios. 
Yet another idea would be to use human judgement to guide the formation of forecast ensembles. In the past, it has proven surprisingly difficult to create forecast ensembles that outperform a simple unweighted combination of individual forecasts \citep[this has been called the "forecast combination puzzle", see e.g.][]{claeskensForecastCombinationPuzzle2016}. Weighted ensembles are usually trained based on past performance. However, training algorithms usually lack a deeper understanding of the models involved. Organisers of the various COVID-19 Forecast Hubs, on the other hand, had quite a good understanding of the models involved and their respective strenghts and weaknesses in various situations. Perhaps human judgement that is guided by an understanding of individual models could help in determining model weights that improve on equal weighting. Again, adequate tools would be needed to allow users to see the effect of choosing different model weights on the resulting ensemble. 
Somewhat related, it could be useful to ask forecasters (for example, researchers who submit forecasts to Forecast Hubs) for their personal confidence in their own forecasts. These subjective estimates might be used to improve the ensemble forecast or provide learning opportunities for participants. 



## Conclusion

This thesis has focused on the two areas of forecast evaluation and the comparison of human judgement forecasting and model-based predictions. Throughout, it has touched on a large variety of different questions and topics. What are appropriate tools to evaluate and elicit forecasts? How can we interpret different scores and make them meaningful and useful to modellers and policy makers alike? How can we create scores that better reflect the underlying infectious disease processes we are trying to predict? What are strengths and weaknesses of human judgement forecasts and model-based predictions and how can we best cater to the respective strengths in the future?

This work has provided tangible progress on some of these questions. In particular with regards to forecast evaluation, both `scoringutils` and the paper on forecast transformation (Chapter \ref{sec:LogTransformation}) represent important advances that have found their way into the work of major institutions. With regards to human judgement forecasting, this thesis has contributed to a shared understanding of the relative performance of humans and mathematical models in infectious disease forecasting and has explored several novel approaches of eliciting forecasts and combining human judgement and computational models. 

Overall, likely more questions have been raised than answered. With regards to the human judgement forecasting parts of this thesis, the majority of the work should best be understood as exploratory case studies. On the one hand, our work did contribute to existing knowledge both in terms of our understanding of the performance of human forecasts and the ways that forecast evaluations in general could be conducted. On the other hand, our two studies highlight how difficult it is to obtain conclusive answers, how many researcher degrees of freedom are involved and how many questions remain unsolved. In some sense, the major value of our work on comparing human and model-based forecasts perhaps comes from using a particular case study as a starting point for open exploration and reflection, and from raising interesting questions and suggesting novel areas of inquiry. 
With regards to the applied and the theoretical work on forecast evaluation in this thesis, our efforts highlight how much there is still to be done in terms of interpreting the performance of forecasters. It remains difficult to link scores to the actual usefulness of a forecast. Furthermore, handling the high dimensionality of the different forecast targets across time, location, targets remains a challenge. On the other hand, the work on transforming forecasts before scoring them presented in this thesis opens up a large range of exciting possibilities and avenues for future research. 

The work in this thesis, the tools and theoretical advancements, as well as the ideas and questions it raised hopefully contribute to an infectious disease forecasting ecosystem that allows us to be a little more prepared for the next infectious disease outbreaks than the last. 



<!-- - combining humans and models -->
  <!-- - using humans to predict parameters -->
  <!--   - Metaculus question we authored -->
  <!--   - Monkeypox work that Tom made - things are complicated -->

<!-- - Related: Develop ways to scale human forecasting -->




<!-- Parking lot -->
<!-- The following aims were set out in Chapter \ref{sec:introduction}:  -->
<!-- - Establish appropriate tools to evaluate predictions in `R` and review best practices in forecast evaluation -->
<!-- - Develop tools to elicit human forecasts of infectious diseases, specifically COVID-19 -->
<!-- - Analyse the role of human judgement in forecasting COVID-19 in Germany and Poland. Compare human judgement forecasts against model-based predictions and analyse the added benefit of human input over mathematical models -->
<!-- - Improve current evaluation methods so that they are betters suited for evaluating forecasts in an epidemiological context -->
<!-- - Examine the potential to use public crowd forecasting tournaments to predict COVID-19 in the UK and explore possibilities to combine human judgement and epidemiological modelling -->


<!-- - established tools to evaluate forecasts and reviewed current scoring methods -->
<!-- - improved current evaluation methods in an epidemiological context -->
<!-- - created an open source forecast elicitation platform and used it to inform COVID response -->
<!-- - investigated the role of human judgement forecasting -->
<!-- -  -->

<!-- - Issue with human forecasting:  -->
<!--   - trust in public health decision making -->

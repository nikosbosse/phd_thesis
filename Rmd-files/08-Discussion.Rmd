# Discussion 
\label{sec:discussion}

<!-- Summary of key results -->

<!-- Strengths and limitations -->

<!-- Contributions of this research relative to previous knowledge -->

<!-- Implications and future research opportunities -->


## Summary and contributions to existing work

The work presented in this thesis has made several contributions related to forecast evaluation and human judgement forecasting in epidemiology. 

### Forecast evaluation {-}

The first major contribution is towards improving the evaluation of infectious disease forecasts, both from a practical and a theoretical perspective. 

#### scoringutils {-}
The `scoringutils` package and the review of scoring practices presented in Chapter \ref{sec:scoringutils} help make forecast evaluation and the necessary tools more accessible to researchers and decision makers. The package implements a suite of proper scoring rules and other scoring metrics in a flexible framework and offers extensive documentation to guide users through the process. The paper reviews the theoretical foundation of forecast evaluation and provides justification for the choice of a particular score over another. Most of the scores implemented in `scoringutils` had been implemented previously. Notable existing packages are `scoringRules` \citep{scoringRules}, `topmodels` \citep{topmodels}, `tscount` \citep{tscount} and `Metrics` \citep{Metrics}, among others. `scoringutils` improves usability over existing software and is especially geared towards comparing forecasts from different models, regardless of how those forecasts were generated. All forecasts can be scored and summarised in a convenient `data.frame` format and the package offers functionality to compare performance visually across different dimensions and account for missing forecasts. `scoringutils` has since been used in published and unpublished work supporting major public health organisations such the US Centers for Disease Control and Prevention, the European Centre for Disease Prevention and Control, the UK Health Security Agency, and Médecins Sans Frontièrs. 

#### Evaluating forecasts in an epidemiological context {-}
In addition to improving the tools available for forecast evaluations, this thesis made a theoretical contribution to our understanding of forecast evaluations in the context of epidemiology. We argued that transforming forecasts and observations prior to applying the continuous ranked probability score (CRPS) or the weighted interval score (WIS) makes it possible to obtain more meaningful results than with the untransformed CRPS and WIS. Both the WIS (which is a quantile-based approximation of the CRPS) measure the absolute distance between the forecast and the observation, which scales with the order of magnitude of the target and may not be the most appropriate metric given the exponential nature of infectious disease processes. The natural logarithm is a particularly attractive transformation in an epidemiological context, as the resulting score represents a measure of how well the forecasts predicted the exponential growth rate. The CRPS applied to log-transformed forecasts can also be understood as an approximate probabilistic version of the symmetric relative error. Furthermore, the natural log transformation can serve as a variance-stabilising transformation, helping to make scores more comparable across time, locations and forecast targets. We compared scores on the natural scale and on the log scale for a set of forecasts submitted to the European COVID-19 Forecast Hub and found that rankings between models changed. Scores were more evenly distributed across time, locations and forecast targets on the log scale. Forecasters were less severely penalised for missing the peak on the log scale, and received higher penalties for missing an upswing of incidences. 

### Human judgement {-}

The second major contribution of this thesis is in improving our understanding of the role of human judgement in infectious disease forecasting and the potential and limitations of human judgement forecasts. 

In order to be able to compare probabilistic forecasts from human forecasters against computational models, we developed a new open source platform and R package, `crowdforecastr`, which allows the elicitation of probabilistic time series forecasts from individual forecasters. When we submitted our forecasts to the German and Polish Forecast Hub in 2020 and 2021, our study was the first one in an epidemiological context to compare full predictive distributions from individual users against model-based predictions. A previous study by \cite{farrowHumanJudgmentApproach2017} had elicited point forecasts from individual participants and combined them to a probabilistic forecasts based on the mean and variation in participants' predictions. 

In the first study presented in Chapter \ref{sec:GermanyPoland}, we compared a small crowd of human forecasters against two minimally-tuned epidemiological models and an ensemble of model-based predictions submitted to the German and Polish Forecast Hub. These submissions contributed to a shared effort to inform the public and public health decision makers in Germany and Poland during the earlier phases of COVID-19. We found predictions for cases from human forecasters to be slightly better in terms of the weighted interval score than forecasts of the Hub ensemble, but worse when predicting deaths. Our minimally-tuned model forecasts performed comparable to our crowd forecasts for short horizons, but noticeably worse for longer horizons. This suggests that human judgement is beneficial in guiding model-based predictions when conditions change over time. We also found that even forecasts that are worse than an pre-existing ensemble can help to improve ensemble forecasts when including it in that ensemble. 

In the second study in the UK presented in Chapter \ref{uk-challenge}, we repeated the basic set-up of the first study with a larger number of participants in a different country. Forecasts were elicited as part of a public forecasting challenge over the course of 13 weeks and submitted to the European COVID-19 Forecast Hub. In addition, we tested a novel forecasting approach in which users submitted a forecast of the effective reproduction number $R_t$, which would then get mapped to reported cases and deaths using the R package `EpiNow2` \citep{epinow2}. Following the conclusions presented in Chapter \ref{sec:LogTransformation} we evaluated forecasts using both untransformed and log-transformed predictions and observations. We found performance of human and model-based forecasts to be overall comparable. The Hub ensemble performed slightly better than human forecasts on the natural scale, and slightly worse when evaluated on the log scale. Our novel $R_t$ approach performed comparable to other forecasts when predicting cases (which forecasters could observe directly), and noticeably worse for death predictions (which forecasters couldn't see), suggesting that the underlying model did not accurately capture the relation between cases and deaths. 


## Limitations

The work presented in this thesis is subject to various important limitations. This section gives an overview and summary of those limitations, while further details can be found in the corresponding chapters. 

### Forecast evaluation {-}

In terms of the tools developed in this thesis, the `scoringutils` package is still under active development, with new issues opening periodically on Github and new features added. In particular, the package lacks robust error handling, resulting at times in rather cryptic error messages. While tests are in place for most functions, new bugs may also be discovered in the future. 
 
In terms of the theoretical advancements regarding transformations of forecasts before scoring, only the first steps have been made. Theoretical considerations suggest that log-transforming forecasts before applying the CRPS or WIS yields a score that better reflects the exponential nature of infectious disease processes. However, in our work we have only provided a small example that serves to illustrate the effects of transforming forecasts before evaluation. Whether or not scores on the log scale appear indeed more meaningful to researchers and policy makers remains to be seen. More work is needed to obtain a more complete understanding of the behaviour of scores on the log scale across different applications. 

Severl practical issues arise when transforming forecasts. The natural logarithm, for example, does not allow any negative or zero values in the forecasts or observations. In Chapter \ref{LogTransformation} we suggested to add a small value to all observations and forecasts in order to deal with zero values. This does not break propriety, but introduces more degrees of freedom and changes the scores in subtle ways. How to best deal with negative values remains unclear. These may not be a problem in an epidemiological context, where observations are usually counts, but may hinder application in other contexts. Issues may arise even when no zeros or negative values are present, especially when forecasts or observations are small. A small absolute difference between forecast and observation can translate to a large relative difference, causing scores to blow up. This issue can arise in particular if forecasts and observations are restricted to assume integer values. Users may find that forecasts made for small targets can dominate overall scores in an undesirable way, depending on the relationship between mean and variance of the forecast target. \cite{bracherEvaluatingEpidemicForecasts2021} argued before that the fact that CRPS scales with the forecast target conveys meaningful information that gets lost when log-transforming targets. 
Some desirable transformations other than the natural logarithm, such as converting absolute forecasts to forecasts of weekly growth rates by dividing every predicted value by the value in the week before, are restricted to forecasts that are stored in a sample-based format. 


### Human judgement {-}

This thesis has studied the potential of human judgement to forecast infectious diseases such as COVID-19. Both studies, however, suffered from a low number of participants and it is unclear how well results could generalise to other settings. 

The first study in Germany and Poland, presented in Chapter \ref{GermanyPoland}, had a median of six participants per week (the Hub ensemble had a similar amount of ensemble models). Participants were also recruited in a very ad hoc fashion among friends and colleagues, making the sample not representative. In this sense, the study is maybe better understood as a case study of an acute outbreak response effort using human judgement forecasts. The overall evaluation was made difficult by the fact that scores varied a lot from week to week and across locations and forecast horizons. This introduced many researcher degrees of freedom, as results and interpretations could change depending on how forecasts were evaluated. For the study in Germany and Poland, we have only looked at forecasts on the natural scale and therefore lack knowledge of what results would have looked like on the log scale. 

Given the very context-dependent nature of human judgement forecasting, results may not generalise well to other settings. While we attempted to replicate some our findings in Germany and Poland in a second study, there are a few factors that make a direct comparison difficult. In the first study, human forecasts were combined using a mean ensemble, while in the second study we used a quantile-wise median (following the practice adopted by all COVID-19 Forecast Hubs). Changes in the number and composition of forecasters, the shorter time horizon and the different setting (including vaccination and a different COVID-19 variant) also limit comparability. In both studies, human forecasts were compared against an ensemble of model-based predictions. However, those ensembles differed both in terms of their size and their composition. 

All forecasts were only analysed at an aggregate level. We therefore could not obtain an understanding of how individuals contributed to the overall ensemble forecasts. For example, we cannot know how the number of participants influenced overall performance, or whether participants learned over time. Results may have looked different had we successfully retained participants over the course of the studies. In both our studies, most participants only submitted a single forecast, which may have affected overall performance. In addition, we treated forecasters more or less as black box, without qualitatively investigating their thought processes in detail. 

We asked forecasters to self-identify as "experts" by whether or not they worked in infectious disease modelling or had professional experience in any related field. However, the value of that information was limited in a few ways. Firstly, we were not able to check participants' statements. Secondly, the question we asked was perhaps too broad to provide a useful measure of the participant's actual expertise for the task at hand. Something like a short quiz may have helped to get more detailed information on the participants' level of expertise. 

One aim of the study in Germany and Poland was to improve our understanding of the relative contributions of human judgement forecasting and epidemiological modelling. However, our approach of comparing direct human forecasts with minimally-tuned epidemiological models was likely flawed and may not have entirely achieved this goal. Firstly, there was a partial, but not complete overlap between the researchers who designed our minimally-tuned models and the participants of the forecasting study. Had these two groups either been disjunct or identical, a fairer comparison would have been possible. This, however, would have come at the expense of an even further reduced number of participants to the point that only one or two forecasters might have been left on a given date. Furthermore, human forecasters were allowed to use any model they liked as an input and we therefore cannot make statements about the extent to which human forecasts were guided by epidemiological modelling. 
The notion of "minimally-tuned" we used in our study is very vague, making it unclear how much our models were actually guided by human judgement rather than being just a mathematical representation of our abstract understanding about infectious disease dynamics. Furthermore, we only used two minimally-tuned models for this study. It is unclear whether our models are able to represent a general class of "minimally-tuned epidemiological models", or whether they are just two specific models with particular strengths and weaknesses that may not generalise to other models. 

For the second crowd forecasting study in the UK, we experimented with a novel forecasting approach that asked human forecasters to predict the effective reproduction number $R_t$ which then got mapped to reported cases and deaths. Forecasters were able to see a preview of the case forecast implied by their $R_t$ forecast, but could not see the corresponding prediction for deaths. Participants therefore had to rely entirely on the underlying model and were not able to adjust parameters like the CFR or the delay between cases and deaths. The specific results we obtained therefore depend very strongly on the specific model used and therefore does not necessarily reflect the overall potential of the proposed approach. 

## Implications and avenues for future work

### Forecast evaluation {-}

Broad interest in the `scoringutils` package suggests that developing and maintaining tools may be an effective way to contribute to the field of infectious disease forecasting. Unfortunately, academia provides few mechanisms to incentivise and reward the development and maintenance of tools in a collaborative and sustainable way. While creating a new tool allows the authors to get some recognition through publication of their work, there are few ways to reward researchers for contributing to existing projects or to put effort into continuously developing software after publication. This promotes an ecosystem with a large number of disconnected tools that solve parts of a larger problem, but don't interface well with each other. One important step forward would be to create an environment in which tools can be developed in ways that are less reliant on the initiatives of individual researchers and where tools can keep being maintained and improved even after those individuals have left. 
For the `scoringutlis` package itself, a range of further improvements and features would be useful. One important area is the expansion of current tests and more explicit handling of inputs and errors in order to make error messages more useful for users. In terms of new features, adding more metrics to the package would be desirable, as well as including an option to let users pass in custom scoring functions. Furthermore, it would be helpful if the package were able to score forecasts in a binned format. 


Lala The current work has highlighted difficulties and laid out paths for future work and also showed that some progress can be made. It's used a lot and important.... 

Despite the wide-spread use of proper scoring rules, forecast evaluation is far from a solved problem and much work remains to be done. 

Firstly, we only have rudimentary understanding how the resulting scores translate into "usefulness" of the forecasts. For example, it might be much worse to underpredict the hospitalisations than overpredict them, in a way that is not reflected by ordinary scores. Or it might be that a forecast that is biased, but correctly predicts a trend is more useful than one that shows the wrong trend, but is closer to the actual values and therefore receives a higher score. If we show decision makers different forecasts, which score will choose the model that different decision makers describe as the most useful one?

Transforming forecasts before scoring is one promising way of approaching the issue. Further research into scores on the natural vs. on the log scale is needed. For example, it would be interesting to investigate which whether scores on the log scale tend to be more consistent over time on the natural scale. Furthermore, developing and exploring new transformations may prove very useful. 
Forecast transformations may be particularly attractive in combination with the possibility to create composite scores as a linear combination of different proper scoring rules. One could for example create a new score that is a weighted combination of scores on the natural and on the log scale. It would probably also be possible without breaking propriety to bring the scores on the same scale first by standardising them (in the sense of calculating a Z-value, i.e. subtract the mean and divide by the standard deviation) separately before combining them. Transformations may be particularly useful when forecasts are represented in the form of predictive samples. This allows, for example, evaluating the shape of the forecast trend line by dividing forecasts for horizon $h$ by the forecasts for horizon $h-1$. 
Another promising approach may be to develop custom proper scoring rules for specific applications using Bayes Acts \citep{brehmerProperizationConstructingProper2020} and general loss functions.

Secondly, we don't have a good understanding of what adequate baselines for comparisons are. For example, predicting whether or not a stone will fall is much harder then predicting whether or not a stock will rise. This is not reflected through scores that only compare the forecast and the outcome. This makes comparisons of forecasts across different settings, locations, times and forecast targets very difficult. 

Thirdly, forecast evaluations suffer from a large number of researcher degrees of freedom. Results change a lot depending on choices such as what metric to use, which forecast horizon to focus on, whether forecasts are transformed before scoring etc. This introduces a subjective element into the evaluation and makes results prone to motivated reasoning in a way that is masked by the apparent objectivity of numerical evaluations. 
To mitigate this, researchers should aim to show a broad range of different scores and metrics. It could be good idea to establish shared standards of what should be reported in a forecast evaluation and how. Other ideas include pre-registration of studies or interactive visualisations of scores that forecast consumers can explore on their own. 

Xthly, we still don't have a very good understanding for how much to trust a forecaster. In the context of the COVID-19 Forecast Hubs, models often showed consistent good performance and then suddenly broke down when circumstances changed. The Hub ensembles robustly emerged as good (and in most instances the best) choice on average across several Forecast Hubs and many evaluations. But even the ensemble often missed changes in trends and performed poorly. Many key questions still remain open: How much data is needed until we can say confidently that a model / forecaster performs well or badly? Can we identify some kind of reliability measure that would indicate how much we can trust a given forecast at a particular time? Should we use significance tests to compare models / forecasters and if so how much can we trust them? One issue with significance tests in particular is that forecasts are usually correlated across time and location, reducing the effective sample size. Also, as \cite{dieboldComparingPredictiveAccuracy2015} note, there is a difference between comparing forecasts with comparing forecasters, as the observed performance is not necessarily representative for the forecaster in general. 

Xthly, the high dimensionality of forecasting data poses a problem for forecast evaluation. In epidemiological setting forecasts are often made by several models at different time points for different forecast horizons, locations and forecast targets. Evaluating and visualising forecasts for different stratification of the data is cumbersome, difficult, and relies heavily on the researcher's judgement. It would be helpful to come up with robust ways to handle the dimensionality of the data better. One obvious candidate is modelling scores using a regression framework. A major obstacle, however, is that scores (at least in epidemiology) tend to be heavily skewed and dominated by outliers. Perhaps a combination of transforming forecasts before scoring and standardising scores may make it possible to use a regression framework, even if p-values for coefficients may not be reliable. 

Xthly, we don't have established good ways to provide useful feedback for forecasters on how to improve their forecasts. The Forecast Hubs published data on scores and average performance, but struggled to provide actionable feedback. One option would be to create single-model evaluations that explore in detail when and how a model performed well or badly. A more technically sophisticated option might be to provide forecasters with an interface that allows them to manipulate past forecasts and observe how e.g. shifting forecasts or adjusting dispersion would have influenced past scores. 

<!-- Given the importance of forecast evaluations in public discourse it would be important to develop methodology further. Hubs use it and it can influence decision-making.  -->




### Human judgement forecasting {-}

This thesis has attempted to shed some light on the opportunities and limitations associated with human judgement forecasting of infectious diseases such as COVID-19. Our studies provided some evidence that a mixed crowd of human forecasters and an ensemble of model-based predictions can produce forecasts of comparable quality. However, many of the details of what drives good performance, both in models and human forecasters, are still poorly understood. 

Human judgement forecasting and mathematical modelling differ in their advantages and disadvantages. Whether or not one should aim to elicit forecasts from humans or from mathematical models therefore depends a lot on the application. 

Modelling useful if
- long time frames
- you want to learn something about the underlying disease
- settings that benefit from the ability to do complex calculations
- good data

Human judgement useful if
- high uncertainty
- lots of information that is hard to feed into a model




is more useful in a given 



However, our two studies are maybe better understand as case studies aiming to explore interesting features of human judgement forecast, rather than providing definitive results. Both studies suffered from a small number of participants. 


The low number of participants was in part due to time constraints - the first forecasts were submitted within three weeks of the start of this PhD. However, even with more time and preparation, as was possible for the second study in the UK, we found it hard to recruit a large number of participants. 


Forecast evaluation needs to be improved. 
This reflects a general problem in epidemiological forecasting: we don't really have a good baseline model. 
  
- unclear whether humans are better than at deaths
- humans can in principle keep up with models. 
- human judgement forecasting is hard and it's really tough to recruit enough participants.
- Implications for the Forecast Hub evaluations. Two proper scoring rules commonly used in Epidemiology (among others, by the COVID-19 Forecast Hubs in the US, Europe, and Germany / Poland) are the 

- Established evidence that humans can be good, but still evidence needed regarding the role of human judgement in modelling
  - it would be nice to have a study where you compare a) human forecasts, b) human forecasters who adjust model outputs and c) just the model outputs. 
  - would also need better tools for that. 



## Future work

- doing qualitative interviews with human forecasters might be helpful. 


- using samples

- combining humans and models
  - Rt forecasting
  - using humans to predict parameters
    - Metaculus question we authored
    - Monkeypox work that Tom made - things are complicated
  - Heaving humans come up with model weights
  - having humans adapt models

- Related: Develop ways to scale human forecasting

- how do details of human forecasters


An overall more promising approach might have been to give human forecasters the opportunity to directly manipulate the output from the computational models. These adapted forecasts could then have compared to the original predictions to investigate whether human judgement would have helped to improve directly on the computational models. These forecasts could then have been compared against human predictions made in the complete abscence of any epidemiological modelling. 



<!-- Parking lot -->
<!-- The following aims were set out in Chapter \ref{sec:introduction}:  -->
<!-- - Establish appropriate tools to evaluate predictions in `R` and review best practices in forecast evaluation -->
<!-- - Develop tools to elicit human forecasts of infectious diseases, specifically COVID-19 -->
<!-- - Analyse the role of human judgement in forecasting COVID-19 in Germany and Poland. Compare human judgement forecasts against model-based predictions and analyse the added benefit of human input over mathematical models -->
<!-- - Improve current evaluation methods so that they are betters suited for evaluating forecasts in an epidemiological context -->
<!-- - Examine the potential to use public crowd forecasting tournaments to predict COVID-19 in the UK and explore possibilities to combine human judgement and epidemiological modelling -->


<!-- - established tools to evaluate forecasts and reviewed current scoring methods -->
<!-- - improved current evaluation methods in an epidemiological context -->
<!-- - created an open source forecast elicitation platform and used it to inform COVID response -->
<!-- - investigated the role of human judgement forecasting -->
<!-- -  -->

<!-- - Issue with human forecasting:  -->
<!--   - trust in public health decision making -->
  
-

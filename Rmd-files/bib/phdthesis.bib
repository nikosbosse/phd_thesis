
@article{loveModeratedEstimationFold2014,
	title = {Moderated estimation of fold change and dispersion for {RNA}-seq data with {DESeq2}},
	volume = {15},
	issn = {1465-6906},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4302049/},
	doi = {10.1186/s13059-014-0550-8},
	abstract = {In comparative high-throughput sequencing assays, a fundamental task is the analysis of count data, such as read counts per gene in RNA-seq, for evidence of systematic changes across experimental conditions. Small replicate numbers, discreteness, large dynamic range and the presence of outliers require a suitable statistical approach. We present DESeq2, a method for differential analysis of count data, using shrinkage estimation for dispersions and fold changes to improve stability and interpretability of estimates. This enables a more quantitative analysis focused on the strength rather than the mere presence of differential expression. The DESeq2 package is available at http://www.bioconductor.org/packages/release/bioc/html/DESeq2.html.},
	number = {12},
	urldate = {2018-12-02},
	journal = {Genome Biology},
	author = {Love, Michael I and Huber, Wolfgang and Anders, Simon},
	year = {2014},
	pmid = {25516281},
	pmcid = {PMC4302049},
	file = {PubMed Central Full Text PDF:/mnt/data/Google Drive/Zotero/storage/9ZYZUDWF/Love et al. - 2014 - Moderated estimation of fold change and dispersion.pdf:application/pdf}
}

@article{robinsonSmallsampleEstimationNegative2008,
	title = {Small-sample estimation of negative binomial dispersion, with applications to {SAGE} data},
	volume = {9},
	issn = {1465-4644},
	doi = {10.1093/biostatistics/kxm030},
	abstract = {We derive a quantile-adjusted conditional maximum likelihood estimator for the dispersion parameter of the negative binomial distribution and compare its performance, in terms of bias, to various other methods. Our estimation scheme outperforms all other methods in very small samples, typical of those from serial analysis of gene expression studies, the motivating data for this study. The impact of dispersion estimation on hypothesis testing is studied. We derive an "exact" test that outperforms the standard approximate asymptotic tests.},
	language = {eng},
	number = {2},
	journal = {Biostatistics (Oxford, England)},
	author = {Robinson, Mark D. and Smyth, Gordon K.},
	month = apr,
	year = {2008},
	pmid = {17728317},
	keywords = {Bias, Binomial Distribution, Biometry, Expressed Sequence Tags, Gene Expression Profiling, Gene Library, Humans, Information Storage and Retrieval, Likelihood Functions, Regression Analysis, Research Design, RNA, Messenger, Sample Size, Stochastic Processes, Weights and Measures},
	pages = {321--332},
	file = {Volltext:/mnt/data/Google Drive/Zotero/storage/EFNXKJMY/Robinson und Smyth - 2008 - Small-sample estimation of negative binomial dispe.pdf:application/pdf}
}

@article{robinsonEdgeRBioconductorPackage2010,
	title = {{edgeR}: a {Bioconductor} package for differential expression analysis of digital gene expression data},
	volume = {26},
	issn = {1367-4803},
	shorttitle = {{edgeR}},
	url = {https://academic.oup.com/bioinformatics/article/26/1/139/182458},
	doi = {10.1093/bioinformatics/btp616},
	abstract = {Abstract.  Summary: It is expected that emerging digital gene expression (DGE) technologies will overtake microarray technologies in the near future for many fu},
	language = {en},
	number = {1},
	urldate = {2018-12-01},
	journal = {Bioinformatics},
	author = {Robinson, Mark D. and McCarthy, Davis J. and Smyth, Gordon K.},
	month = jan,
	year = {2010},
	pages = {139--140},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/CKS7MEGJ/Robinson et al. - 2010 - edgeR a Bioconductor package for differential exp.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/8CQEB75X/182458.html:text/html}
}

@article{marioniRNAseqAssessmentTechnical2008,
	title = {{RNA}-seq: {An} assessment of technical reproducibility and comparison with gene expression arrays},
	issn = {1088-9051, 1549-5469},
	shorttitle = {{RNA}-seq},
	url = {http://genome.cshlp.org/content/early/2008/07/30/gr.079558.108},
	doi = {10.1101/gr.079558.108},
	abstract = {Ultra-high-throughput sequencing is emerging as an attractive alternative to microarrays for genotyping, analysis of methylation patterns, and identification of transcription factor binding sites. Here, we describe an application of the Illumina sequencing (formerly Solexa sequencing) platform to study mRNA expression levels. Our goals were to estimate technical variance associated with Illumina sequencing in this context and to compare its ability to identify differentially expressed genes with existing array technologies. To do so, we estimated gene expression differences between liver and kidney RNA samples using multiple sequencing replicates, and compared the sequencing data to results obtained from Affymetrix arrays using the same RNA samples. We find that the Illumina sequencing data are highly replicable, with relatively little technical variation, and thus, for many purposes, it may suffice to sequence each mRNA sample only once (i.e., using one lane). The information in a single lane of Illumina sequencing data appears comparable to that in a single array in enabling identification of differentially expressed genes, while allowing for additional analyses such as detection of low-expressed genes, alternative splice variants, and novel transcripts. Based on our observations, we propose an empirical protocol and a statistical framework for the analysis of gene expression using ultra-high-throughput sequencing technology.},
	language = {en},
	urldate = {2018-12-01},
	journal = {Genome Research},
	author = {Marioni, John C. and Mason, Christopher E. and Mane, Shrikant M. and Stephens, Matthew and Gilad, Yoav},
	month = sep,
	year = {2008},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/J2KNF2GL/Marioni et al. - 2008 - RNA-seq An assessment of technical reproducibilit.pdf:application/pdf;Full Text PDF:/mnt/data/Google Drive/Zotero/storage/UBAV6T4C/Marioni et al. - 2008 - RNA-seq An assessment of technical reproducibilit.pdf:application/pdf;Full Text PDF:/mnt/data/Google Drive/Zotero/storage/8LXW6IH5/Marioni et al. - 2008 - RNA-seq An assessment of technical reproducibilit.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/XMPU6X4T/gr.079558.108.html:text/html;Snapshot:/mnt/data/Google Drive/Zotero/storage/FN28X7D2/gr.079558.108.html:text/html;Snapshot:/mnt/data/Google Drive/Zotero/storage/VKXW3ZBB/gr.079558.108.html:text/html}
}

@article{mccarthyDifferentialExpressionAnalysis2012,
	title = {Differential expression analysis of multifactor {RNA}-{Seq} experiments with respect to biological variation},
	volume = {40},
	issn = {1362-4962, 0305-1048},
	url = {https://academic.oup.com/nar/article/40/10/4288/2411520},
	doi = {10.1093/nar/gks042},
	language = {en},
	number = {10},
	urldate = {2018-12-01},
	journal = {Nucleic Acids Research},
	author = {McCarthy, Davis J. and Chen, Yunshun and Smyth, Gordon K.},
	month = may,
	year = {2012},
	pages = {4288--4297},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/QQM3Y333/McCarthy et al. - 2012 - Differential expression analysis of multifactor RN.pdf:application/pdf;Full Text PDF:/mnt/data/Google Drive/Zotero/storage/W2C628C3/McCarthy et al. - 2012 - Differential expression analysis of multifactor RN.pdf:application/pdf;Full Text PDF:/mnt/data/Google Drive/Zotero/storage/JHGMBRFA/McCarthy et al. - 2012 - Differential expression analysis of multifactor RN.pdf:application/pdf;McCarthy et al. - 2012 - Differential expression analysis of multifactor RN.pdf:/mnt/data/Google Drive/Zotero/storage/8TKBTU43/McCarthy et al. - 2012 - Differential expression analysis of multifactor RN.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/253RSK4M/2411520.html:text/html;Snapshot:/mnt/data/Google Drive/Zotero/storage/LLNKSJAL/2411520.html:text/html;Snapshot:/mnt/data/Google Drive/Zotero/storage/Q6WFXKF6/2411520.html:text/html}
}

@article{andersCountbasedDifferentialExpression2013,
	title = {Count-based differential expression analysis of {RNA} sequencing data using {R} and {Bioconductor}},
	volume = {8},
	issn = {1754-2189, 1750-2799},
	url = {http://www.nature.com/articles/nprot.2013.099},
	doi = {10.1038/nprot.2013.099},
	language = {en},
	number = {9},
	urldate = {2018-12-01},
	journal = {Nature Protocols},
	author = {Anders, Simon and McCarthy, Davis J and Chen, Yunshun and Okoniewski, Michal and Smyth, Gordon K and Huber, Wolfgang and Robinson, Mark D},
	month = sep,
	year = {2013},
	pages = {1765--1786},
	file = {Anders et al. - 2013 - Count-based differential expression analysis of RN.pdf:/mnt/data/Google Drive/Zotero/storage/K3D4K5TK/Anders et al. - 2013 - Count-based differential expression analysis of RN.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/AMN9KUQI/nprot.2013.html:text/html;Submitted Version:/mnt/data/Google Drive/Zotero/storage/2RQPP28C/Anders et al. - 2013 - Count-based differential expression analysis of RN.pdf:application/pdf}
}

@article{dilliesComprehensiveEvaluationNormalization2013,
	title = {A comprehensive evaluation of normalization methods for {Illumina} high-throughput {RNA} sequencing data analysis},
	volume = {14},
	issn = {1467-5463, 1477-4054},
	url = {https://academic.oup.com/bib/article-lookup/doi/10.1093/bib/bbs046},
	doi = {10.1093/bib/bbs046},
	abstract = {During the last 3 years, a number of approaches for the normalization of RNA sequencing data have emerged in the literature, differing both in the type of bias adjustment and in the statistical strategy adopted. However, as data continue to accumulate, there has been no clear consensus on the appropriate normalization method to be used or the impact of a chosen method on the downstream analysis. In this work, we focus on a comprehensive comparison of seven recently proposed normalization methods for the differential analysis of RNA-seq data, with an emphasis on the use of varied real and simulated datasets involving different species and experimental designs to represent data characteristics commonly observed in practice. Based on this comparison study, we propose practical recommendations on the appropriate normalization method to be used and its impact on the differential analysis of RNA-seq data.},
	language = {en},
	number = {6},
	urldate = {2018-12-01},
	journal = {Briefings in Bioinformatics},
	author = {Dillies, M.-A. and Rau, A. and Aubert, J. and Hennequet-Antier, C. and Jeanmougin, M. and Servant, N. and Keime, C. and Marot, G. and Castel, D. and Estelle, J. and Guernec, G. and Jagla, B. and Jouneau, L. and Laloe, D. and Le Gall, C. and Schaeffer, B. and Le Crom, S. and Guedj, M. and Jaffrezic, F. and {on behalf of The French StatOmique Consortium}},
	month = nov,
	year = {2013},
	pages = {671--683},
	file = {Dillies et al. - 2013 - A comprehensive evaluation of normalization method.pdf:/mnt/data/Google Drive/Zotero/storage/XAHDLGIN/Dillies et al. - 2013 - A comprehensive evaluation of normalization method.pdf:application/pdf}
}

@article{lewinBayesianModellingDifferential,
	title = {Bayesian {Modelling} of {Diﬀerential} {Gene} {Expression}},
	language = {en},
	author = {Lewin, Alex and Richardson, Sylvia and Marshall, Clare and Glazier, Anne and Aitman, Tim},
	pages = {28},
	annote = {Modell für Microarrays, nicht Count Data. Außerdem nur zwei Conditions mit diesem Modell möglich. Schön zu lesen, noch nicht fertig bearbeitet.},
	file = {Lewin et al. - Bayesian Modelling of Diﬀerential Gene Expression.pdf:/mnt/data/Google Drive/Zotero/storage/Z8RSZVBM/Lewin et al. - Bayesian Modelling of Diﬀerential Gene Expression.pdf:application/pdf}
}

@article{wangRNASeqRevolutionaryTool2009,
	title = {{RNA}-{Seq}: a revolutionary tool for transcriptomics},
	volume = {10},
	issn = {1471-0056},
	shorttitle = {{RNA}-{Seq}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2949280/},
	doi = {10.1038/nrg2484},
	abstract = {RNA-Seq is a recently developed approach to transcriptome profiling that uses deep-sequencing technologies. Studies using this method have already altered our view of the extent and complexity of eukaryotic transcriptomes. RNA-Seq also provides a far more precise measurement of levels of transcripts and their isoforms than other methods. This article describes the RNA-Seq approach, the challenges associated with its application, and the advances made so far in characterizing several eukaryote transcriptomes.},
	number = {1},
	urldate = {2018-10-04},
	journal = {Nature reviews. Genetics},
	author = {Wang, Zhong and Gerstein, Mark and Snyder, Michael},
	month = jan,
	year = {2009},
	pmid = {19015660},
	pmcid = {PMC2949280},
	pages = {57--63},
	file = {PubMed Central Full Text PDF:/mnt/data/Google Drive/Zotero/storage/HEC6RJVI/Wang et al. - 2009 - RNA-Seq a revolutionary tool for transcriptomics.pdf:application/pdf}
}

@article{jagannathanTranslationalRegulationMitochondrial2015,
	title = {Translational {Regulation} of the {Mitochondrial} {Genome} {Following} {Redistribution} of {Mitochondrial} {MicroRNA} ({MitomiR}) in the {Diabetic} {Heart}},
	volume = {8},
	issn = {1942-325X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4681669/},
	doi = {10.1161/CIRCGENETICS.115.001067},
	abstract = {Background
Cardiomyocytes are rich in mitochondria which are situated in spatially-distinct subcellular regions including those under the plasma membrane, subsarcolemmal mitochondria; and those between the myofibrils, interfibrillar mitochondria. We previously observed subpopulation-specific differences in mitochondrial proteomes following diabetic insult. The objective of this study was to determine whether mitochondrial genome-encoded proteins are regulated by microRNAs inside the mitochondrion and whether subcellular spatial location or diabetes mellitus influences the dynamics.

Methods and Results
Using microarray technology coupled with cross-linking immunoprecipitation and next generation sequencing, we identified a pool of mitochondrial microRNAs, termed mitomiRs that are redistributed in spatially-distinct mitochondrial subpopulations in an inverse manner following diabetic insult. Redistributed mitomiRs displayed distinct interactions with the mitochondrial genome requiring specific stoichiometric associations with RISC constituents argonaute-2 (Ago2) and fragile X mental retardation–related protein 1 (FXR1) for translational regulation. In the presence of Ago2 and FXR1, redistribution of mitomiR-378 to the IFM following diabetic insult led to down regulation of mitochondrially-encoded F0 component ATP6. Next generation sequencing analyses identified specific transcriptome and mitomiR sequences associated with ATP6 regulation. Overexpression of mitomiR-378 in HL-1 cells resulted in its accumulation in the mitochondrion and down-regulation of functional ATP6 protein, while antagomir blockade restored functional ATP6 protein and cardiac pump function.

Conclusions
We propose mitomiRs can translationally regulate mitochondrially-encoded proteins in spatially-distinct mitochondrial subpopulations during diabetes mellitus. The results reveal the requirement of RISC constituents in the mitochondrion for functional mitomiR translational regulation and provide a connecting link between diabetic insult and ATP synthase function.},
	number = {6},
	urldate = {2018-09-12},
	journal = {Circulation. Cardiovascular genetics},
	author = {Jagannathan, Rajaganapathi and Thapa, Dharendra and Nichols, Cody E. and Shepherd, Danielle L. and Stricker, Janelle C. and Croston, Tara L. and Baseler, Walter A. and Lewis, Sara E. and Martinez, Ivan and Hollander, John M.},
	month = dec,
	year = {2015},
	pmid = {26377859},
	pmcid = {PMC4681669},
	pages = {785--802},
	file = {PubMed Central Full Text PDF:/mnt/data/Google Drive/Zotero/storage/PD5SKYZT/Jagannathan et al. - 2015 - Translational Regulation of the Mitochondrial Geno.pdf:application/pdf}
}

@article{matkovichRISCRNASequencing2011,
	title = {{RISC} {RNA} sequencing for context-specific identification of in vivo {miR} targets},
	volume = {108},
	issn = {0009-7330},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3017647/},
	doi = {10.1161/CIRCRESAHA.110.233528},
	abstract = {Rationale
MicroRNAs (miRs) are expanding our understanding of cardiac disease and have the potential to transform cardiovascular therapeutics. One miR can target hundreds of individual mRNAs, but existing methodologies are not sufficient to accurately and comprehensively identify these mRNA targets in vivo.

Objective
To develop methods permitting identification of in vivo miR targets in an unbiased manner, using massively parallel sequencing of mouse cardiac transcriptomes in combination with sequencing of mRNA associated with mouse cardiac RNA-induced silencing complexes (RISCs).

Methods and Results
We optimized techniques for expression profiling small amounts of RNA without introducing amplification bias, and applied this to anti-Argonaute 2 immunoprecipitated RISCs (RISC-Seq) from mouse hearts. By comparing RNA-sequencing results of cardiac RISC and transcriptome from the same individual hearts, we defined 1,645 mRNAs consistently targeted to mouse cardiac RISCs. We employed this approach in hearts overexpressing miRs from Myh6 promoter-driven precursors (programmed RISC-Seq) to identify 209 in vivo targets of miR-133a and 81 in vivo targets of miR-499. Consistent with the fact that miR-133a and miR-499 have widely differing ‘seed’ sequences and belong to different miR families, only 6 targets were common to miR-133a- and miR-499-programmed hearts.

Conclusions
RISC-sequencing is a highly sensitive method for general RISC profiling and individual miR target identification in biological context, and is applicable to any tissue and any disease state.

Summary
MicroRNAs (miRs) are key regulators of mRNA translation in health and disease. While bioinformatic predictions suggest that a single miR may target hundreds of mRNAs, the number of experimentally verified targets of miRs is low. To enable comprehensive, unbiased examination of miR targets, we have performed deep RNA sequencing of cardiac transcriptomes in parallel with cardiac RNA-induced silencing complex (RISC)-associated RNAs (the RISCome), called RISC sequencing. We developed methods that did not require cross-linking of RNAs to RISCs or amplification of mRNA prior to sequencing, making it possible to rapidly perform RISC sequencing from intact tissue while avoiding amplification bias. Comparison of RISCome with transcriptome expression defined the degree of RISC enrichment for each mRNA. The majority of the mRNAs enriched in wild-type cardiac RISComes compared to transcriptomes were bioinformatically predicted to be targets of at least 1 of 139 cardiac-expressed miRs. Programming cardiomyocyte RISCs via transgenic overexpression in adult hearts of miR-133a or miR-499, two miRs that contain entirely different ‘seed’ sequences, elicited differing profiles of RISC-targeted mRNAs. Thus, RISC sequencing represents a highly sensitive method for general RISC profiling and individual miR target identification in biological context.},
	number = {1},
	urldate = {2018-09-12},
	journal = {Circulation research},
	author = {Matkovich, Scot J and Van Booven, Derek J and Eschenbacher, William H and Dorn, Gerald W},
	month = jan,
	year = {2011},
	pmid = {21030712},
	pmcid = {PMC3017647},
	pages = {18--26},
	file = {PubMed Central Full Text PDF:/mnt/data/Google Drive/Zotero/storage/5NF4N6YQ/Matkovich et al. - 2011 - RISC RNA sequencing for context-specific identific.pdf:application/pdf}
}

@article{cuiNovelAlgorithmCalling2016,
	title = {A novel algorithm for calling {mRNA} {m6A} peaks by modeling biological variances in {MeRIP}-seq data},
	volume = {32},
	issn = {1367-4803},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4908365/},
	doi = {10.1093/bioinformatics/btw281},
	abstract = {Motivation:
N6-methyl-adenosine (m6A) is the most prevalent mRNA methylation but precise prediction of its mRNA location is important for understanding its function. A recent sequencing technology, known as Methylated RNA Immunoprecipitation Sequencing technology (MeRIP-seq), has been developed for transcriptome-wide profiling of m6A. We previously developed a peak calling algorithm called exomePeak. However, exomePeak over-simplifies data characteristics and ignores the reads’ variances among replicates or reads dependency across a site region. To further improve the performance, new model is needed to address these important issues of MeRIP-seq data., Results: We propose a novel, graphical model-based peak calling method, MeTPeak, for transcriptome-wide detection of m6A sites from MeRIP-seq data. MeTPeak explicitly models read count of an m6A site and introduces a hierarchical layer of Beta variables to capture the variances and a Hidden Markov model to characterize the reads dependency across a site. In addition, we developed a constrained Newton’s method and designed a log-barrier function to compute analytically intractable, positively constrained Beta parameters. We applied our algorithm to simulated and real biological datasets and demonstrated significant improvement in detection performance and robustness over exomePeak. Prediction results on publicly available MeRIP-seq datasets are also validated and shown to be able to recapitulate the known patterns of m6A, further validating the improved performance of MeTPeak., Availability and implementation: The package ‘MeTPeak’ is implemented in R and C ++, and additional details are available at https://github.com/compgenomics/MeTPeak, Contact:
yufei.huang@utsa.edu or xdchoi@gmail.com, Supplementary information:
Supplementary data are available at Bioinformatics online.},
	number = {12},
	urldate = {2018-09-10},
	journal = {Bioinformatics},
	author = {Cui, Xiaodong and Meng, Jia and Zhang, Shaowu and Chen, Yidong and Huang, Yufei},
	month = jun,
	year = {2016},
	pmid = {27307641},
	pmcid = {PMC4908365},
	pages = {i378--i385},
	file = {PubMed Central Full Text PDF:/mnt/data/Google Drive/Zotero/storage/IYJGCLY9/Cui et al. - 2016 - A novel algorithm for calling mRNA m6A peaks by mo.pdf:application/pdf;PubMed Central Full Text PDF:/mnt/data/Google Drive/Zotero/storage/N727H4U4/Cui et al. - 2016 - A novel algorithm for calling mRNA m6A peaks by mo.pdf:application/pdf}
}

@article{batistaRNAModificationN6methyladenosine2017,
	series = {{RNA} {Epigenetics} ({I})},
	title = {The {RNA} {Modification} {N6}-methyladenosine and {Its} {Implications} in {Human} {Disease}},
	volume = {15},
	issn = {1672-0229},
	url = {http://www.sciencedirect.com/science/article/pii/S1672022917300773},
	doi = {10.1016/j.gpb.2017.03.002},
	abstract = {Impaired gene regulation lies at the heart of many disorders, including developmental diseases and cancer. Furthermore, the molecular pathways that control gene expression are often the target of cellular parasites, such as viruses. Gene expression is controlled through multiple mechanisms that are coordinated to ensure the proper and timely expression of each gene. Many of these mechanisms target the life cycle of the RNA molecule, from transcription to translation. Recently, another layer of regulation at the RNA level involving RNA modifications has gained renewed interest of the scientific community. The discovery that N6-methyladenosine (m6A), a modification present in mRNAs and long noncoding RNAs, can be removed by the activity of RNA demethylases, launched the field of epitranscriptomics; the study of how RNA function is regulated through the addition or removal of post-transcriptional modifications, similar to strategies used to regulate gene expression at the DNA and protein level. The abundance of RNA post-transcriptional modifications is determined by the activity of writer complexes (methylase) and eraser (RNA demethylase) proteins. Subsequently, the effects of RNA modifications materialize as changes in RNA structure and/or modulation of interactions between the modified RNA and RNA binding proteins or regulatory RNAs. Disruption of these pathways impairs gene expression and cellular function. This review focuses on the links between the RNA modification m6A and its implications in human diseases.},
	number = {3},
	urldate = {2018-09-09},
	journal = {Genomics, Proteomics \& Bioinformatics},
	author = {Batista, Pedro J.},
	month = jun,
	year = {2017},
	keywords = {-methyladenosine, Cancer, Epitranscriptomics, Metabolic disease, Viral replication},
	pages = {154--163},
	file = {Batista - 2017 - The RNA Modification N 6 -methyladenosine and Its .pdf:/mnt/data/Google Drive/Zotero/storage/UBCYT2I4/Batista - 2017 - The RNA Modification N 6 -methyladenosine and Its .pdf:application/pdf;ScienceDirect Snapshot:/mnt/data/Google Drive/Zotero/storage/PEDHAD2Q/S1672022917300773.html:text/html}
}

@article{bullardEvaluationStatisticalMethods2010,
	title = {Evaluation of statistical methods for normalization and differential expression in {mRNA}-{Seq} experiments},
	volume = {11},
	issn = {1471-2105},
	url = {https://doi.org/10.1186/1471-2105-11-94},
	doi = {10.1186/1471-2105-11-94},
	abstract = {High-throughput sequencing technologies, such as the Illumina Genome Analyzer, are powerful new tools for investigating a wide range of biological and medical questions. Statistical and computational methods are key for drawing meaningful and accurate conclusions from the massive and complex datasets generated by the sequencers. We provide a detailed evaluation of statistical methods for normalization and differential expression (DE) analysis of Illumina transcriptome sequencing (mRNA-Seq) data.},
	number = {1},
	urldate = {2018-12-02},
	journal = {BMC Bioinformatics},
	author = {Bullard, James H. and Purdom, Elizabeth and Hansen, Kasper D. and Dudoit, Sandrine},
	month = feb,
	year = {2010},
	pages = {94},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/T6NTKDDI/Bullard et al. - 2010 - Evaluation of statistical methods for normalizatio.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/IR9GCWBF/1471-2105-11-94.html:text/html}
}

@article{subramanianGeneSetEnrichment2005,
	title = {Gene set enrichment analysis: {A} knowledge-based approach for interpreting genome-wide expression profiles},
	volume = {102},
	copyright = {Copyright © 2005, The National Academy of Sciences.  Freely available online through the PNAS open access option.},
	issn = {0027-8424, 1091-6490},
	shorttitle = {Gene set enrichment analysis},
	url = {http://www.pnas.org/content/102/43/15545},
	doi = {10.1073/pnas.0506580102},
	abstract = {Although genomewide RNA expression analysis has become a routine tool in biomedical research, extracting biological insight from such information remains a major challenge. Here, we describe a powerful analytical method called Gene Set Enrichment Analysis (GSEA) for interpreting gene expression data. The method derives its power by focusing on gene sets, that is, groups of genes that share common biological function, chromosomal location, or regulation. We demonstrate how GSEA yields insights into several cancer-related data sets, including leukemia and lung cancer. Notably, where single-gene analysis finds little similarity between two independent studies of patient survival in lung cancer, GSEA reveals many biological pathways in common. The GSEA method is embodied in a freely available software package, together with an initial database of 1,325 biologically defined gene sets.},
	language = {en},
	number = {43},
	urldate = {2018-12-02},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Subramanian, Aravind and Tamayo, Pablo and Mootha, Vamsi K. and Mukherjee, Sayan and Ebert, Benjamin L. and Gillette, Michael A. and Paulovich, Amanda and Pomeroy, Scott L. and Golub, Todd R. and Lander, Eric S. and Mesirov, Jill P.},
	month = oct,
	year = {2005},
	pmid = {16199517},
	keywords = {microarray},
	pages = {15545--15550},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/8AG9AWE4/Subramanian et al. - 2005 - Gene set enrichment analysis A knowledge-based ap.pdf:application/pdf;Full Text PDF:/mnt/data/Google Drive/Zotero/storage/PC93AVKK/Subramanian et al. - 2005 - Gene set enrichment analysis A knowledge-based ap.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/U5SU4RBT/15545.html:text/html;Snapshot:/mnt/data/Google Drive/Zotero/storage/TJ2AG9FT/15545.html:text/html}
}

@article{andersDifferentialExpressionAnalysis2010,
	title = {Differential expression analysis for sequence count data},
	volume = {11},
	issn = {1474-760X},
	url = {https://doi.org/10.1186/gb-2010-11-10-r106},
	doi = {10.1186/gb-2010-11-10-r106},
	abstract = {High-throughput sequencing assays such as RNA-Seq, ChIP-Seq or barcode counting provide quantitative readouts in the form of count data. To infer differential signal in such data correctly and with good statistical power, estimation of data variability throughout the dynamic range and a suitable error model are required. We propose a method based on the negative binomial distribution, with variance and mean linked by local regression and present an implementation, DESeq, as an R/Bioconductor package.},
	number = {10},
	urldate = {2018-12-02},
	journal = {Genome Biology},
	author = {Anders, Simon and Huber, Wolfgang},
	month = oct,
	year = {2010},
	pages = {R106},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/IY4SX6PH/Anders and Huber - 2010 - Differential expression analysis for sequence coun.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/W8YK39DN/gb-2010-11-10-r106.html:text/html}
}

@article{wilbanksEvaluationAlgorithmPerformance2010,
	title = {Evaluation of {Algorithm} {Performance} in {ChIP}-{Seq} {Peak} {Detection}},
	volume = {5},
	issn = {1932-6203},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2900203/},
	doi = {10.1371/journal.pone.0011471},
	abstract = {Next-generation DNA sequencing coupled with chromatin immunoprecipitation (ChIP-seq) is revolutionizing our ability to interrogate whole genome protein-DNA interactions. Identification of protein binding sites from ChIP-seq data has required novel computational tools, distinct from those used for the analysis of ChIP-Chip experiments. The growing popularity of ChIP-seq spurred the development of many different analytical programs (at last count, we noted 31 open source methods), each with some purported advantage. Given that the literature is dense and empirical benchmarking challenging, selecting an appropriate method for ChIP-seq analysis has become a daunting task. Herein we compare the performance of eleven different peak calling programs on common empirical, transcription factor datasets and measure their sensitivity, accuracy and usability. Our analysis provides an unbiased critical assessment of available technologies, and should assist researchers in choosing a suitable tool for handling ChIP-seq data.},
	number = {7},
	urldate = {2018-12-03},
	journal = {PLoS ONE},
	author = {Wilbanks, Elizabeth G. and Facciotti, Marc T.},
	month = jul,
	year = {2010},
	pmid = {20628599},
	pmcid = {PMC2900203},
	file = {PubMed Central Full Text PDF:/mnt/data/Google Drive/Zotero/storage/L8H9MCR3/Wilbanks and Facciotti - 2010 - Evaluation of Algorithm Performance in ChIP-Seq Pe.pdf:application/pdf}
}

@article{baileyPracticalGuidelinesComprehensive2013,
	title = {Practical {Guidelines} for the {Comprehensive} {Analysis} of {ChIP}-seq {Data}},
	volume = {9},
	issn = {1553-734X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3828144/},
	doi = {10.1371/journal.pcbi.1003326},
	abstract = {Mapping the chromosomal locations of transcription factors, nucleosomes, histone modifications, chromatin remodeling enzymes, chaperones, and polymerases is one of the key tasks of modern biology, as evidenced by the Encyclopedia of DNA Elements (ENCODE) Project. To this end, chromatin immunoprecipitation followed by high-throughput sequencing (ChIP-seq) is the standard methodology. Mapping such protein-DNA interactions in vivo using ChIP-seq presents multiple challenges not only in sample preparation and sequencing but also for computational analysis. Here, we present step-by-step guidelines for the computational analysis of ChIP-seq data. We address all the major steps in the analysis of ChIP-seq data: sequencing depth selection, quality checking, mapping, data normalization, assessment of reproducibility, peak calling, differential binding analysis, controlling the false discovery rate, peak annotation, visualization, and motif analysis. At each step in our guidelines we discuss some of the software tools most frequently used. We also highlight the challenges and problems associated with each step in ChIP-seq data analysis. We present a concise workflow for the analysis of ChIP-seq data in Figure 1 that complements and expands on the recommendations of the ENCODE and modENCODE projects. Each step in the workflow is described in detail in the following sections.},
	number = {11},
	urldate = {2018-12-03},
	journal = {PLoS Computational Biology},
	author = {Bailey, Timothy and Krajewski, Pawel and Ladunga, Istvan and Lefebvre, Celine and Li, Qunhua and Liu, Tao and Madrigal, Pedro and Taslim, Cenny and Zhang, Jie},
	month = nov,
	year = {2013},
	pmid = {24244136},
	pmcid = {PMC3828144},
	file = {PubMed Central Full Text PDF:/mnt/data/Google Drive/Zotero/storage/UJATMVH8/Bailey et al. - 2013 - Practical Guidelines for the Comprehensive Analysi.pdf:application/pdf}
}

@article{dominissiniTopologyHumanMouse2012,
	title = {Topology of the human and mouse {m6A} {RNA} methylomes revealed by {m6A}-seq},
	volume = {485},
	issn = {1476-4687},
	doi = {10.1038/nature11112},
	abstract = {An extensive repertoire of modifications is known to underlie the versatile coding, structural and catalytic functions of RNA, but it remains largely uncharted territory. Although biochemical studies indicate that N(6)-methyladenosine (m(6)A) is the most prevalent internal modification in messenger RNA, an in-depth study of its distribution and functions has been impeded by a lack of robust analytical methods. Here we present the human and mouse m(6)A modification landscape in a transcriptome-wide manner, using a novel approach, m(6)A-seq, based on antibody-mediated capture and massively parallel sequencing. We identify over 12,000 m(6)A sites characterized by a typical consensus in the transcripts of more than 7,000 human genes. Sites preferentially appear in two distinct landmarks--around stop codons and within long internal exons--and are highly conserved between human and mouse. Although most sites are well preserved across normal and cancerous tissues and in response to various stimuli, a subset of stimulus-dependent, dynamically modulated sites is identified. Silencing the m(6)A methyltransferase significantly affects gene expression and alternative splicing patterns, resulting in modulation of the p53 (also known as TP53) signalling pathway and apoptosis. Our findings therefore suggest that RNA decoration by m(6)A has a fundamental role in regulation of gene expression.},
	language = {eng},
	number = {7397},
	journal = {Nature},
	author = {Dominissini, Dan and Moshitch-Moshkovitz, Sharon and Schwartz, Schraga and Salmon-Divon, Mali and Ungar, Lior and Osenberg, Sivan and Cesarkas, Karen and Jacob-Hirsch, Jasmine and Amariglio, Ninette and Kupiec, Martin and Sorek, Rotem and Rechavi, Gideon},
	month = apr,
	year = {2012},
	pmid = {22575960},
	keywords = {Humans, Adenosine, Alternative Splicing, Animals, Base Sequence, Cell Line, Tumor, Conserved Sequence, Evolution, Molecular, Hep G2 Cells, Metabolome, Methylation, Methyltransferases, Mice, RNA, RNA-Binding Proteins, RNA, Ribosomal, RNA, Transfer, Transcriptome, Methoden, Immunoprecipitation},
	pages = {201--206}
}

@article{meyerComprehensiveAnalysisMRNA2012,
	title = {Comprehensive analysis of {mRNA} methylation reveals enrichment in 3' {UTRs} and near stop codons},
	volume = {149},
	issn = {1097-4172},
	doi = {10.1016/j.cell.2012.05.003},
	abstract = {Methylation of the N(6) position of adenosine (m(6)A) is a posttranscriptional modification of RNA with poorly understood prevalence and physiological relevance. The recent discovery that FTO, an obesity risk gene, encodes an m(6)A demethylase implicates m(6)A as an important regulator of physiological processes. Here, we present a method for transcriptome-wide m(6)A localization, which combines m(6)A-specific methylated RNA immunoprecipitation with next-generation sequencing (MeRIP-Seq). We use this method to identify mRNAs of 7,676 mammalian genes that contain m(6)A, indicating that m(6)A is a common base modification of mRNA. The m(6)A modification exhibits tissue-specific regulation and is markedly increased throughout brain development. We find that m(6)A sites are enriched near stop codons and in 3' UTRs, and we uncover an association between m(6)A residues and microRNA-binding sites within 3' UTRs. These findings provide a resource for identifying transcripts that are substrates for adenosine methylation and reveal insights into the epigenetic regulation of the mammalian transcriptome.},
	language = {eng},
	number = {7},
	journal = {Cell},
	author = {Meyer, Kate D. and Saletore, Yogesh and Zumbo, Paul and Elemento, Olivier and Mason, Christopher E. and Jaffrey, Samie R.},
	month = jun,
	year = {2012},
	pmid = {22608085},
	pmcid = {PMC3383396},
	keywords = {RNA, Messenger, Adenosine, Methylation, Transcriptome, 3' Untranslated Regions, Codon, Terminator, RNA Processing, Post-Transcriptional, RNA, Untranslated},
	pages = {1635--1646},
	file = {Accepted Version:/mnt/data/Google Drive/Zotero/storage/8T6IK7CQ/Meyer et al. - 2012 - Comprehensive analysis of mRNA methylation reveals.pdf:application/pdf}
}

@article{fuGeneExpressionRegulation2014,
	title = {Gene expression regulation mediated through reversible m$^{\textrm{6}}${A} {RNA} methylation},
	volume = {15},
	copyright = {2014 Nature Publishing Group},
	issn = {1471-0064},
	url = {https://www.nature.com/articles/nrg3724},
	doi = {10.1038/nrg3724},
	abstract = {Cellular RNAs carry diverse chemical modifications that used to be regarded as static and having minor roles in 'fine-tuning' structural and functional properties of RNAs. In this Review, we focus on reversible methylation through the most prevalent mammalian mRNA internal modification, N6-methyladenosine (m6A). Recent studies have discovered protein 'writers', 'erasers' and 'readers' of this RNA chemical mark, as well as its dynamic deposition on mRNA and other types of nuclear RNA. These findings strongly indicate dynamic regulatory roles that are analogous to the well-known reversible epigenetic modifications of DNA and histone proteins. This reversible RNA methylation adds a new dimension to the developing picture of post-transcriptional regulation of gene expression.},
	language = {en},
	number = {5},
	urldate = {2018-12-03},
	journal = {Nature Reviews Genetics},
	author = {Fu, Ye and Dominissini, Dan and Rechavi, Gideon and He, Chuan},
	month = may,
	year = {2014},
	pages = {293--306},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/TF47HCJS/Fu et al. - 2014 - Gene expression regulation mediated through revers.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/XMWA2EI9/nrg3724.html:text/html}
}

@article{mcdermaidInterpretationDifferentialGene2018,
	title = {Interpretation of differential gene expression results of {RNA}-seq data: review and integration},
	issn = {1477-4054},
	shorttitle = {Interpretation of differential gene expression results of {RNA}-seq data},
	doi = {10.1093/bib/bby067},
	abstract = {Differential gene expression (DGE) analysis is one of the most common applications of RNA-sequencing (RNA-seq) data. This process allows for the elucidation of differentially expressed genes across two or more conditions and is widely used in many applications of RNA-seq data analysis. Interpretation of the DGE results can be nonintuitive and time consuming due to the variety of formats based on the tool of choice and the numerous pieces of information provided in these results files. Here we reviewed DGE results analysis from a functional point of view for various visualizations. We also provide an R/Bioconductor package, Visualization of Differential Gene Expression Results using R, which generates information-rich visualizations for the interpretation of DGE results from three widely used tools, Cuffdiff, DESeq2 and edgeR. The implemented functions are also tested on five real-world data sets, consisting of one human, one Malus domestica and three Vitis riparia data sets.},
	language = {eng},
	journal = {Briefings in Bioinformatics},
	author = {McDermaid, Adam and Monier, Brandon and Zhao, Jing and Liu, Bingqiang and Ma, Qin},
	month = aug,
	year = {2018},
	pmid = {30099484}
}

@article{yuDynamicM6AModification2018,
	title = {Dynamic {m6A} modification regulates local translation of {mRNA} in axons},
	volume = {46},
	issn = {0305-1048},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5815124/},
	doi = {10.1093/nar/gkx1182},
	abstract = {N
6-methyladenosine (m6A) is a reversible modification in mRNA and has been shown to regulate processing, translation and decay of mRNA. However, the roles of m6A modification in neuronal development are still not known. Here, we found that the m6A eraser FTO is enriched in axons and can be locally translated. Axon-specific inhibition of FTO by rhein, or compartmentalized siRNA knockdown of Fto in axons led to increases of m6A levels. GAP-43 mRNA is modified by m6A and is a substrate of FTO in axons. Loss-of-function of this non-nuclear pool of FTO resulted in increased m6A modification and decreased local translation of axonal GAP-43 mRNA, which eventually repressed axon elongation. Mutation of a predicted m6A site in GAP-43 mRNA eliminated its m6A modification and exempted regulation of its local translation by axonal FTO. This work showed an example of dynamic internal m6A demethylation of non-nuclear localized mRNA by the demethylase FTO. Regulation of m6A modification of axonal mRNA by axonal FTO might be a general mechanism to control their local translation in neuronal development.},
	number = {3},
	urldate = {2018-12-11},
	journal = {Nucleic Acids Research},
	author = {Yu, Jun and Chen, Mengxian and Huang, Haijiao and Zhu, Junda and Song, Huixue and Zhu, Jian and Park, Jaewon and Ji, Sheng-Jian},
	month = feb,
	year = {2018},
	pmid = {29186567},
	pmcid = {PMC5815124},
	pages = {1412--1423},
	file = {PubMed Central Full Text PDF:/mnt/data/Google Drive/Zotero/storage/NCLBUAYT/Yu et al. - 2018 - Dynamic m6A modification regulates local translati.pdf:application/pdf}
}

@article{alarconN6methyladenosineM6AMarks2015,
	title = {N6-methyl-adenosine ({m6A}) marks primary {microRNAs} for processing},
	volume = {519},
	issn = {0028-0836},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4475635/},
	doi = {10.1038/nature14281},
	abstract = {The first step in the biogenesis of microRNAs is the processing of primary microRNAs (pri-miRNAs) by the microprocessor complex, composed of the RNA binding protein DGCR8 and the ribonuclease type III DROSHA–. This initial event requires the recognition of the junction between the stem and the flanking single-stranded RNA of the pri-miRNA hairpin by DGCR8 followed by recruitment of DROSHA, which cleaves the RNA duplex to yield the pre-miRNA product. While the mechanisms underlying pri-miRNA processing have been elucidated, the mechanism by which DGCR8 recognizes and binds pri-miRNAs as opposed to other secondary structures present in transcripts is not understood. We find that methyltransferase like 3 (METTL3) methylates pri-miRNAs, marking them for recognition and processing by DGCR8. Consistent with this, METTL3 depletion reduced the binding of DGCR8 to pri-miRNAs and resulted in the global reduction of mature miRNAs and concomitant accumulation of unprocessed pri-miRNAs. In vitro processing reactions confirmed the sufficiency of the m6A mark in promoting pri-miRNA processing. Finally, gain-of-function experiments revealed that METTL3 is sufficient to enhance miRNA maturation in a global and non-cell-type specific manner. Our findings reveal that the m6A mark acts as a key post-transcriptional modification that promotes the initiation of miRNA biogenesis.},
	number = {7544},
	urldate = {2018-12-30},
	journal = {Nature},
	author = {Alarcón, Claudio R. and Lee, Hyeseung and Goodarzi, Hani and Halberg, Nils and Tavazoie, Sohail F.},
	month = mar,
	year = {2015},
	pmid = {25799998},
	pmcid = {PMC4475635},
	pages = {482--485},
	file = {PubMed Central Full Text PDF:/mnt/data/Google Drive/Zotero/storage/P5WI7ULS/Alarcón et al. - 2015 - N6-methyl-adenosine (m6A) marks primary microRNAs .pdf:application/pdf}
}

@article{hardcastleBaySeqEmpiricalBayesian2010,
	title = {{baySeq}: empirical {Bayesian} methods for identifying differential expression in sequence count data},
	volume = {11},
	issn = {1471-2105},
	shorttitle = {{baySeq}},
	doi = {10.1186/1471-2105-11-422},
	abstract = {BACKGROUND: High throughput sequencing has become an important technology for studying expression levels in many types of genomic, and particularly transcriptomic, data. One key way of analysing such data is to look for elements of the data which display particular patterns of differential expression in order to take these forward for further analysis and validation.
RESULTS: We propose a framework for defining patterns of differential expression and develop a novel algorithm, baySeq, which uses an empirical Bayes approach to detect these patterns of differential expression within a set of sequencing samples. The method assumes a negative binomial distribution for the data and derives an empirically determined prior distribution from the entire dataset. We examine the performance of the method on real and simulated data.
CONCLUSIONS: Our method performs at least as well, and often better, than existing methods for analyses of pairwise differential expression in both real and simulated data. When we compare methods for the analysis of data from experimental designs involving multiple sample groups, our method again shows substantial gains in performance. We believe that this approach thus represents an important step forward for the analysis of count data from sequencing experiments.},
	language = {eng},
	journal = {BMC bioinformatics},
	author = {Hardcastle, Thomas J. and Kelly, Krystyna A.},
	month = aug,
	year = {2010},
	pmid = {20698981},
	pmcid = {PMC2928208},
	keywords = {Gene Expression Profiling, Research Design, Base Sequence, Algorithms, Arabidopsis, Bayes Theorem, RNA, Plant},
	pages = {422},
	file = {Full Text:/mnt/data/Google Drive/Zotero/storage/KCL69KYW/Hardcastle and Kelly - 2010 - baySeq empirical Bayesian methods for identifying.pdf:application/pdf}
}

@article{hardcastleGeneralizedEmpiricalBayesian2016,
	title = {Generalized empirical {Bayesian} methods for discovery of differential data in high-throughput biology},
	volume = {32},
	issn = {1367-4811},
	doi = {10.1093/bioinformatics/btv569},
	abstract = {MOTIVATION: High-throughput data are now commonplace in biological research. Rapidly changing technologies and application mean that novel methods for detecting differential behaviour that account for a 'large P, small n' setting are required at an increasing rate. The development of such methods is, in general, being done on an ad hoc basis, requiring further development cycles and a lack of standardization between analyses.
RESULTS: We present here a generalized method for identifying differential behaviour within high-throughput biological data through empirical Bayesian methods. This approach is based on our baySeq algorithm for identification of differential expression in RNA-seq data based on a negative binomial distribution, and in paired data based on a beta-binomial distribution. Here we show how the same empirical Bayesian approach can be applied to any parametric distribution, removing the need for lengthy development of novel methods for differently distributed data. Comparisons with existing methods developed to address specific problems in high-throughput biological data show that these generic methods can achieve equivalent or better performance. A number of enhancements to the basic algorithm are also presented to increase flexibility and reduce computational costs.
AVAILABILITY AND IMPLEMENTATION: The methods are implemented in the R baySeq (v2) package, available on Bioconductor http://www.bioconductor.org/packages/release/bioc/html/baySeq.html.
CONTACT: tjh48@cam.ac.uk
SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.},
	language = {eng},
	number = {2},
	journal = {Bioinformatics (Oxford, England)},
	author = {Hardcastle, Thomas J.},
	month = jan,
	year = {2016},
	pmid = {26428289},
	keywords = {Binomial Distribution, Gene Expression Profiling, Animals, Algorithms, Bayes Theorem, Rats, Sequence Analysis, RNA, Software},
	pages = {195--202},
	file = {Snapshot:/mnt/data/Google Drive/Zotero/storage/CR3W92VA/1744387.html:text/html}
}

@article{hardcastleEmpiricalBayesianAnalysis2013,
	title = {Empirical {Bayesian} analysis of paired high-throughput sequencing data with a beta-binomial distribution},
	volume = {14},
	issn = {1471-2105},
	doi = {10.1186/1471-2105-14-135},
	abstract = {BACKGROUND: Pairing of samples arises naturally in many genomic experiments; for example, gene expression in tumour and normal tissue from the same patients. Methods for analysing high-throughput sequencing data from such experiments are required to identify differential expression, both within paired samples and between pairs under different experimental conditions.
RESULTS: We develop an empirical Bayesian method based on the beta-binomial distribution to model paired data from high-throughput sequencing experiments. We examine the performance of this method on simulated and real data in a variety of scenarios. Our methods are implemented as part of the RbaySeq package (versions 1.11.6 and greater) available from Bioconductor (http://www.bioconductor.org).
CONCLUSIONS: We compare our approach to alternatives based on generalised linear modelling approaches and show that our method offers significant gains in performance on simulated data. In testing on real data from oral squamous cell carcinoma patients, we discover greater enrichment of previously identified head and neck squamous cell carcinoma associated gene sets than has previously been achieved through a generalised linear modelling approach, suggesting that similar gains in performance may be found in real data. Our methods thus show real and substantial improvements in analyses of high-throughput sequencing data from paired samples.},
	language = {eng},
	journal = {BMC bioinformatics},
	author = {Hardcastle, Thomas J. and Kelly, Krystyna A.},
	month = apr,
	year = {2013},
	pmid = {23617841},
	pmcid = {PMC3658937},
	keywords = {Binomial Distribution, Gene Expression Profiling, Humans, Bayes Theorem, Carcinoma, Squamous Cell, Genomics, Head and Neck Neoplasms, High-Throughput Nucleotide Sequencing, Linear Models, Squamous Cell Carcinoma of Head and Neck},
	pages = {135},
	annote = {könnte interessant sein, weil es um gepaarte Proben geht, die einen Zusammenhang haben. Vllt. relevant für die Methylierung},
	file = {Full Text:/mnt/data/Google Drive/Zotero/storage/HQLEM974/Hardcastle and Kelly - 2013 - Empirical Bayesian analysis of paired high-through.pdf:application/pdf}
}

@article{chenDifferentialExpressionAnalysis2015,
	title = {Differential expression analysis of {RNA} sequencing data by incorporating non-exonic mapped reads},
	volume = {16 Suppl 7},
	issn = {1471-2164},
	doi = {10.1186/1471-2164-16-S7-S14},
	abstract = {BACKGROUND: RNA sequencing (RNA-seq) is a powerful tool for genome-wide expression profiling of biological samples with the advantage of high-throughput and high resolution. There are many existing algorithms nowadays for quantifying expression levels and detecting differential gene expression, but none of them takes the misaligned reads that are mapped to non-exonic regions into account. We developed a novel algorithm, XBSeq, where a statistical model was established based on the assumption that observed signals are the convolution of true expression signals and sequencing noises. The mapped reads in non-exonic regions are considered as sequencing noises, which follows a Poisson distribution. Given measureable observed and noise signals from RNA-seq data, true expression signals, assuming governed by the negative binomial distribution, can be delineated and thus the accurate detection of differential expressed genes.
RESULTS: We implemented our novel XBSeq algorithm and evaluated it by using a set of simulated expression datasets under different conditions, using a combination of negative binomial and Poisson distributions with parameters derived from real RNA-seq data. We compared the performance of our method with other commonly used differential expression analysis algorithms. We also evaluated the changes in true and false positive rates with variations in biological replicates, differential fold changes, and expression levels in non-exonic regions. We also tested the algorithm on a set of real RNA-seq data where the common and different detection results from different algorithms were reported.
CONCLUSIONS: In this paper, we proposed a novel XBSeq, a differential expression analysis algorithm for RNA-seq data that takes non-exonic mapped reads into consideration. When background noise is at baseline level, the performance of XBSeq and DESeq are mostly equivalent. However, our method surpasses DESeq and other algorithms with the increase of non-exonic mapped reads. Only in very low read count condition XBSeq had a slightly higher false discovery rate, which may be improved by adjusting the background noise effect in this situation. Taken together, by considering non-exonic mapped reads, XBSeq can provide accurate expression measurement and thus detect differential expressed genes even in noisy conditions.},
	language = {eng},
	journal = {BMC genomics},
	author = {Chen, Hung-I. and Liu, Yuanhang and Zou, Yi and Lai, Zhao and Sarkar, Devanand and Huang, Yufei and Chen, Yidong},
	year = {2015},
	pmid = {26099631},
	pmcid = {PMC4474535},
	keywords = {Gene Expression Profiling, Animals, Mice, Algorithms, Sequence Analysis, RNA, Gene Expression Regulation, Models, Statistical, Poisson Distribution},
	pages = {S14},
	file = {Full Text:/mnt/data/Google Drive/Zotero/storage/6YNWKPLY/Chen et al. - 2015 - Differential expression analysis of RNA sequencing.pdf:application/pdf}
}

@article{roblesEfficientExperimentalDesign2012,
	title = {Efficient experimental design and analysis strategies for the detection of differential expression using {RNA}-{Sequencing}},
	volume = {13},
	issn = {1471-2164},
	doi = {10.1186/1471-2164-13-484},
	abstract = {BACKGROUND: RNA sequencing (RNA-Seq) has emerged as a powerful approach for the detection of differential gene expression with both high-throughput and high resolution capabilities possible depending upon the experimental design chosen. Multiplex experimental designs are now readily available, these can be utilised to increase the numbers of samples or replicates profiled at the cost of decreased sequencing depth generated per sample. These strategies impact on the power of the approach to accurately identify differential expression. This study presents a detailed analysis of the power to detect differential expression in a range of scenarios including simulated null and differential expression distributions with varying numbers of biological or technical replicates, sequencing depths and analysis methods.
RESULTS: Differential and non-differential expression datasets were simulated using a combination of negative binomial and exponential distributions derived from real RNA-Seq data. These datasets were used to evaluate the performance of three commonly used differential expression analysis algorithms and to quantify the changes in power with respect to true and false positive rates when simulating variations in sequencing depth, biological replication and multiplex experimental design choices.
CONCLUSIONS: This work quantitatively explores comparisons between contemporary analysis tools and experimental design choices for the detection of differential expression using RNA-Seq. We found that the DESeq algorithm performs more conservatively than edgeR and NBPSeq. With regard to testing of various experimental designs, this work strongly suggests that greater power is gained through the use of biological replicates relative to library (technical) replicates and sequencing depth. Strikingly, sequencing depth could be reduced as low as 15\% without substantial impacts on false positive or true positive rates.},
	language = {eng},
	journal = {BMC genomics},
	author = {Robles, José A. and Qureshi, Sumaira E. and Stephen, Stuart J. and Wilson, Susan R. and Burden, Conrad J. and Taylor, Jennifer M.},
	month = sep,
	year = {2012},
	pmid = {22985019},
	pmcid = {PMC3560154},
	keywords = {Gene Expression Profiling, Algorithms, Sequence Analysis, RNA, Statistics as Topic},
	pages = {484},
	file = {Full Text:/mnt/data/Google Drive/Zotero/storage/T9JRMUA9/Robles et al. - 2012 - Efficient experimental design and analysis strateg.pdf:application/pdf}
}

@article{mazaPapyroComparisonTMM2016,
	title = {In {Papyro} {Comparison} of {TMM} ({edgeR}), {RLE} ({DESeq2}), and {MRN} {Normalization} {Methods} for a {Simple} {Two}-{Conditions}-{Without}-{Replicates} {RNA}-{Seq} {Experimental} {Design}},
	volume = {7},
	issn = {1664-8021},
	doi = {10.3389/fgene.2016.00164},
	abstract = {In the past 5 years, RNA-Seq has become a powerful tool in transcriptome analysis even though computational methods dedicated to the analysis of high-throughput sequencing data are yet to be standardized. It is, however, now commonly accepted that the choice of a normalization procedure is an important step in such a process, for example in differential gene expression analysis. The present article highlights the similarities between three normalization methods: TMM from edgeR R package, RLE from DESeq2 R package, and MRN. Both TMM and DESeq2 are widely used for differential gene expression analysis. This paper introduces properties that show when these three methods will give exactly the same results. These properties are proven mathematically and illustrated by performing in silico calculations on a given RNA-Seq data set.},
	language = {eng},
	journal = {Frontiers in Genetics},
	author = {Maza, Elie},
	year = {2016},
	pmid = {27695478},
	pmcid = {PMC5025571},
	keywords = {comparison of methods, DESeq2, edgeR, normalization, RNA-seq data},
	pages = {164},
	file = {Full Text:/mnt/data/Google Drive/Zotero/storage/PG2WQXP2/Maza - 2016 - In Papyro Comparison of TMM (edgeR), RLE (DESeq2),.pdf:application/pdf}
}

@incollection{mackayHyperparametersOptimizeIntegrate1996,
	address = {Dordrecht},
	title = {Hyperparameters: {Optimize}, or {Integrate} {Out}?},
	isbn = {978-90-481-4407-5 978-94-015-8729-7},
	shorttitle = {Hyperparameters},
	url = {http://link.springer.com/10.1007/978-94-015-8729-7_2},
	abstract = {I examine two approximate methods for computational implementation of Bayesian hierarchical models, that is, models which include unknown hyperparameters such as regularization constants. In the `evidence framework' the model parameters are integrated over, and the resulting evidence is maximized over the hyperparameters. The optimized hyperparameters are used to de ne a Gaussian approximation to the posterior distribution. In the alternative `MAP' method, the true posterior probability is found by integrating over the hyperparameters. The true posterior is then maximized over the model parameters, and a Gaussian approximation is made. The similarities of the two approaches, and their relative merits, are discussed, and comparisons are made with the ideal hierarchical Bayesian solution.},
	language = {en},
	urldate = {2019-01-02},
	booktitle = {Maximum {Entropy} and {Bayesian} {Methods}},
	publisher = {Springer Netherlands},
	author = {MacKay, David J. C.},
	editor = {Heidbreder, Glenn R.},
	year = {1996},
	doi = {10.1007/978-94-015-8729-7_2},
	pages = {43--59},
	file = {MacKay - 1996 - Hyperparameters Optimize, or Integrate Out.pdf:/mnt/data/Google Drive/Zotero/storage/LQ8PXJWY/MacKay - 1996 - Hyperparameters Optimize, or Integrate Out.pdf:application/pdf}
}

@article{chenDifferentialMethylationAnalysis2017,
	title = {Differential methylation analysis of reduced representation bisulfite sequencing experiments using {edgeR}},
	volume = {6},
	issn = {2046-1402},
	doi = {10.12688/f1000research.13196.2},
	abstract = {Cytosine methylation is an important DNA epigenetic modification. In vertebrates, methylation occurs at CpG sites, which are dinucleotides where a cytosine is immediately followed by a guanine in the DNA sequence from 5' to 3'. When located in the promoter region of a gene, DNA methylation is often associated with transcriptional silencing of the gene. Aberrant DNA methylation is associated with the development of various diseases such as cancer. Bisulfite sequencing (BS-seq) is the current "gold-standard" technology for high-resolution profiling of DNA methylation. Reduced representation bisulfite sequencing (RRBS) is an efficient form of BS-seq that targets CpG-rich DNA regions in order to save sequencing costs. A typical bioinformatics aim is to identify CpGs that are differentially methylated (DM) between experimental conditions. This workflow demonstrates that differential methylation analysis of RRBS data can be conducted using software and methodology originally developed for RNA-seq data. The RNA-seq pipeline is adapted to methylation by adding extra columns to the design matrix to account for read coverage at each CpG, after which the RRBS and RNA-seq pipelines are almost identical. This approach is statistically natural and gives analysts access to a rich collection of analysis tools including generalized linear models, gene set testing and pathway analysis. The article presents a complete start to finish case study analysis of RRBS profiles of different cell populations from the mouse mammary gland using the Bioconductor package edgeR. We show that lineage-committed cells are typically hyper-methylated compared to progenitor cells and this is true on all the autosomes but not the sex chromosomes. We demonstrate a strong negative correlation between methylation of promoter regions and gene expression as measured by RNA-seq for the same cell types, showing that methylation is a regulatory mechanism involved in epithelial linear commitment.},
	language = {eng},
	journal = {F1000Research},
	author = {Chen, Yunshun and Pal, Bhupinder and Visvader, Jane E. and Smyth, Gordon K.},
	year = {2017},
	pmid = {29333247},
	pmcid = {PMC5747346.2},
	keywords = {Methylation, Bioconductor, BS-seq, differential methylation analysis},
	pages = {2055},
	file = {Differential methylation analysis of reduced representation bisulfite sequencing experiments using edgeR - F1000Research:/mnt/data/Google Drive/Zotero/storage/73NX4L6C/v2.html:text/html;Full Text:/mnt/data/Google Drive/Zotero/storage/TQQIKCWG/Chen et al. - 2017 - Differential methylation analysis of reduced repre.pdf:application/pdf;Full Text:/mnt/data/Google Drive/Zotero/storage/RL66H59B/Chen et al. - 2018 - Differential methylation analysis of reduced repre.pdf:application/pdf}
}

@article{wuNewShrinkageEstimator2013,
	title = {A new shrinkage estimator for dispersion improves differential expression detection in {RNA}-seq data},
	volume = {14},
	issn = {1468-4357},
	doi = {10.1093/biostatistics/kxs033},
	abstract = {Recent developments in RNA-sequencing (RNA-seq) technology have led to a rapid increase in gene expression data in the form of counts. RNA-seq can be used for a variety of applications, however, identifying differential expression (DE) remains a key task in functional genomics. There have been a number of statistical methods for DE detection for RNA-seq data. One common feature of several leading methods is the use of the negative binomial (Gamma-Poisson mixture) model. That is, the unobserved gene expression is modeled by a gamma random variable and, given the expression, the sequencing read counts are modeled as Poisson. The distinct feature in various methods is how the variance, or dispersion, in the Gamma distribution is modeled and estimated. We evaluate several large public RNA-seq datasets and find that the estimated dispersion in existing methods does not adequately capture the heterogeneity of biological variance among samples. We present a new empirical Bayes shrinkage estimate of the dispersion parameters and demonstrate improved DE detection.},
	language = {eng},
	number = {2},
	journal = {Biostatistics (Oxford, England)},
	author = {Wu, Hao and Wang, Chi and Wu, Zhijin},
	month = apr,
	year = {2013},
	pmid = {23001152},
	pmcid = {PMC3590927},
	keywords = {Binomial Distribution, Gene Expression Profiling, Humans, Bayes Theorem, Sequence Analysis, RNA, Models, Statistical, Poisson Distribution, Biostatistics, Databases, Nucleic Acid},
	pages = {232--243},
	file = {Full Text:/mnt/data/Google Drive/Zotero/storage/DG7W28SY/Wu et al. - 2013 - A new shrinkage estimator for dispersion improves .pdf:application/pdf}
}

@article{zhuHeavytailedPriorDistributions,
	title = {Heavy-tailed prior distributions for sequence count data: removing the noise and preserving large differences},
	shorttitle = {Heavy-tailed prior distributions for sequence count data},
	url = {https://academic.oup.com/bioinformatics/advance-article/doi/10.1093/bioinformatics/bty895/5159452},
	doi = {10.1093/bioinformatics/bty895},
	abstract = {AbstractMotivation.  In RNA-seq differential expression analysis, investigators aim to detect those genes with changes in expression level across conditions, de},
	language = {en},
	urldate = {2019-01-04},
	journal = {Bioinformatics},
	author = {Zhu, Anqi and Ibrahim, Joseph G. and Love, Michael I.},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/4GI2Z6NK/Zhu et al. - Heavy-tailed prior distributions for sequence coun.pdf:application/pdf;Full Text PDF:/mnt/data/Google Drive/Zotero/storage/53QRL4SC/Zhu et al. - Heavy-tailed prior distributions for sequence coun.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/46HPAP6J/5159452.html:text/html;Snapshot:/mnt/data/Google Drive/Zotero/storage/QXATEWWE/5159452.html:text/html}
}

@article{pickrellUnderstandingMechanismsUnderlying2010,
	title = {Understanding mechanisms underlying human gene expression variation with {RNA} sequencing},
	volume = {464},
	issn = {1476-4687},
	doi = {10.1038/nature08872},
	abstract = {Understanding the genetic mechanisms underlying natural variation in gene expression is a central goal of both medical and evolutionary genetics, and studies of expression quantitative trait loci (eQTLs) have become an important tool for achieving this goal. Although all eQTL studies so far have assayed messenger RNA levels using expression microarrays, recent advances in RNA sequencing enable the analysis of transcript variation at unprecedented resolution. We sequenced RNA from 69 lymphoblastoid cell lines derived from unrelated Nigerian individuals that have been extensively genotyped by the International HapMap Project. By pooling data from all individuals, we generated a map of the transcriptional landscape of these cells, identifying extensive use of unannotated untranslated regions and more than 100 new putative protein-coding exons. Using the genotypes from the HapMap project, we identified more than a thousand genes at which genetic variation influences overall expression levels or splicing. We demonstrate that eQTLs near genes generally act by a mechanism involving allele-specific expression, and that variation that influences the inclusion of an exon is enriched within and near the consensus splice sites. Our results illustrate the power of high-throughput sequencing for the joint analysis of variation in transcription, splicing and allele-specific expression across individuals.},
	language = {eng},
	number = {7289},
	journal = {Nature},
	author = {Pickrell, Joseph K. and Marioni, John C. and Pai, Athma A. and Degner, Jacob F. and Engelhardt, Barbara E. and Nkadori, Everlyne and Veyrieras, Jean-Baptiste and Stephens, Matthew and Gilad, Yoav and Pritchard, Jonathan K.},
	month = apr,
	year = {2010},
	pmid = {20220758},
	pmcid = {PMC3089435},
	keywords = {Gene Expression Profiling, Humans, RNA, Messenger, Sequence Analysis, RNA, Gene Expression Regulation, African Continental Ancestry Group, Alleles, Consensus Sequence, DNA, Complementary, Exons, Genetic Variation, Nigeria, Polymorphism, Single Nucleotide, Quantitative Trait Loci, RNA Splice Sites, Transcription, Genetic},
	pages = {768--772},
	file = {Accepted Version:/mnt/data/Google Drive/Zotero/storage/GYDKG9XQ/Pickrell et al. - 2010 - Understanding mechanisms underlying human gene exp.pdf:application/pdf}
}

@article{biNPEBseqNonparametricEmpirical2013,
	title = {{NPEBseq}: nonparametric empirical bayesian-based procedure for differential expression analysis of {RNA}-seq data},
	volume = {14},
	issn = {1471-2105},
	shorttitle = {{NPEBseq}},
	doi = {10.1186/1471-2105-14-262},
	abstract = {BACKGROUND: RNA-seq, a massive parallel-sequencing-based transcriptome profiling method, provides digital data in the form of aligned sequence read counts. The comparative analyses of the data require appropriate statistical methods to estimate the differential expression of transcript variants across different cell/tissue types and disease conditions.
RESULTS: We developed a novel nonparametric empirical Bayesian-based approach (NPEBseq) to model the RNA-seq data. The prior distribution of the Bayesian model is empirically estimated from the data without any parametric assumption, and hence the method is "nonparametric" in nature. Based on this model, we proposed a method for detecting differentially expressed genes across different conditions. We also extended this method to detect differential usage of exons from RNA-seq data. The evaluation of NPEBseq on both simulated and publicly available RNA-seq datasets and comparison with three popular methods showed improved results for experiments with or without biological replicates.
CONCLUSIONS: NPEBseq can successfully detect differential expression between different conditions not only at gene level but also at exon level from RNA-seq datasets. In addition, NPEBSeq performs significantly better than current methods and can be applied to genome-wide RNA-seq datasets. Sample datasets and R package are available at http://bioinformatics.wistar.upenn.edu/NPEBseq.},
	language = {eng},
	journal = {BMC bioinformatics},
	author = {Bi, Yingtao and Davuluri, Ramana V.},
	month = aug,
	year = {2013},
	pmid = {23981227},
	pmcid = {PMC3765716},
	keywords = {Gene Expression Profiling, RNA, Bayes Theorem, Sequence Analysis, RNA, Software, High-Throughput Nucleotide Sequencing, Computational Biology, Sequence Alignment, Statistics, Nonparametric},
	pages = {262},
	file = {Full Text:/mnt/data/Google Drive/Zotero/storage/U9YSE5HP/Bi and Davuluri - 2013 - NPEBseq nonparametric empirical bayesian-based pr.pdf:application/pdf}
}

@article{vandewielBayesianAnalysisRNA2013,
	title = {Bayesian analysis of {RNA} sequencing data by estimating multiple shrinkage priors},
	volume = {14},
	issn = {1468-4357},
	doi = {10.1093/biostatistics/kxs031},
	abstract = {Next generation sequencing is quickly replacing microarrays as a technique to probe different molecular levels of the cell, such as DNA or RNA. The technology provides higher resolution, while reducing bias. RNA sequencing results in counts of RNA strands. This type of data imposes new statistical challenges. We present a novel, generic approach to model and analyze such data. Our approach aims at large flexibility of the likelihood (count) model and the regression model alike. Hence, a variety of count models is supported, such as the popular NB model, which accounts for overdispersion. In addition, complex, non-balanced designs and random effects are accommodated. Like some other methods, our method provides shrinkage of dispersion-related parameters. However, we extend it by enabling joint shrinkage of parameters, including those for which inference is desired. We argue that this is essential for Bayesian multiplicity correction. Shrinkage is effectuated by empirically estimating priors. We discuss several parametric (mixture) and non-parametric priors and develop procedures to estimate (parameters of) those. Inference is provided by means of local and Bayesian false discovery rates. We illustrate our method on several simulations and two data sets, also to compare it with other methods. Model- and data-based simulations show substantial improvements in the sensitivity at the given specificity. The data motivate the use of the ZI-NB as a powerful alternative to the NB, which results in higher detection rates for low-count data. Finally, compared with other methods, the results on small sample subsets are more reproducible when validated on their large sample complements, illustrating the importance of the type of shrinkage.},
	language = {eng},
	number = {1},
	journal = {Biostatistics (Oxford, England)},
	author = {Van De Wiel, Mark A. and Leday, Gwenaël G. R. and Pardo, Luba and Rue, Håvard and Van Der Vaart, Aad W. and Van Wieringen, Wessel N.},
	month = jan,
	year = {2013},
	pmid = {22988280},
	keywords = {Base Sequence, RNA, Bayes Theorem, Sequence Analysis, RNA, Models, Statistical, Computer Simulation, Data Interpretation, Statistical, Molecular Sequence Data},
	pages = {113--128},
	file = {Full Text:/mnt/data/Google Drive/Zotero/storage/7N4XVQ6Y/Van De Wiel et al. - 2013 - Bayesian analysis of RNA sequencing data by estima.pdf:application/pdf}
}

@article{alvarez-castroBayesianAnalysisHighdimensional,
	title = {Bayesian analysis of high-dimensional count data},
	language = {en},
	author = {Alvarez-Castro, Ignacio},
	pages = {150},
	file = {Alvarez-Castro - Bayesian analysis of high-dimensional count data.pdf:/mnt/data/Google Drive/Zotero/storage/AG4T7EPT/Alvarez-Castro - Bayesian analysis of high-dimensional count data.pdf:application/pdf}
}

@article{lunItDEliciousRecipe2016,
	title = {It's {DE}-licious: {A} {Recipe} for {Differential} {Expression} {Analyses} of {RNA}-seq {Experiments} {Using} {Quasi}-{Likelihood} {Methods} in {edgeR}},
	volume = {1418},
	issn = {1940-6029},
	shorttitle = {It's {DE}-licious},
	doi = {10.1007/978-1-4939-3578-9_19},
	abstract = {RNA sequencing (RNA-seq) is widely used to profile transcriptional activity in biological systems. Here we present an analysis pipeline for differential expression analysis of RNA-seq experiments using the Rsubread and edgeR software packages. The basic pipeline includes read alignment and counting, filtering and normalization, modelling of biological variability and hypothesis testing. For hypothesis testing, we describe particularly the quasi-likelihood features of edgeR. Some more advanced downstream analysis steps are also covered, including complex comparisons, gene ontology enrichment analyses and gene set testing. The code required to run each step is described, along with an outline of the underlying theory. The chapter includes a case study in which the pipeline is used to study the expression profiles of mammary gland cells in virgin, pregnant and lactating mice.},
	language = {eng},
	journal = {Methods in Molecular Biology (Clifton, N.J.)},
	author = {Lun, Aaron T. L. and Chen, Yunshun and Smyth, Gordon K.},
	year = {2016},
	pmid = {27008025},
	keywords = {Gene Expression Profiling, Humans, Likelihood Functions, Animals, Mice, Sequence Analysis, RNA, Software, High-Throughput Nucleotide Sequencing, Linear Models, Computational Biology, Sequence Alignment, Databases, Genetic, Differential expression, Generalized linear models, Molecular Sequence Annotation, Quasi-likelihood, Read alignment, Read counts, RNA-seq, Variability},
	pages = {391--416}
}

@article{yueRNAN6methyladenosineMethylation2015,
	title = {{RNA} {N6}-methyladenosine methylation in post-transcriptional gene expression regulation},
	volume = {29},
	issn = {0890-9369},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4511210/},
	doi = {10.1101/gad.262766.115},
	abstract = {N6-methyladenosine (m6A) is the most prevalent internal modification that occurs in the messenger RNA (mRNA) of most eukaryotes. In this review, Yue et al. summarize recent progress in the study of the m6A mRNA methylation machineries across eukaryotes and discuss their newly uncovered roles in post-transcriptional gene expression regulation., N6-methyladenosine (m6A) is the most prevalent and internal modification that occurs in the messenger RNAs (mRNA) of most eukaryotes, although its functional relevance remained a mystery for decades. This modification is installed by the m6A methylation “writers” and can be reversed by demethylases that serve as “erasers.” In this review, we mainly summarize recent progress in the study of the m6A mRNA methylation machineries across eukaryotes and discuss their newly uncovered biological functions. The broad roles of m6A in regulating cell fates and embryonic development highlight the existence of another layer of epigenetic regulation at the RNA level, where mRNA is subjected to chemical modifications that affect protein expression.},
	number = {13},
	urldate = {2019-01-07},
	journal = {Genes \& Development},
	author = {Yue, Yanan and Liu, Jianzhao and He, Chuan},
	month = jul,
	year = {2015},
	pmid = {26159994},
	pmcid = {PMC4511210},
	pages = {1343--1355},
	file = {PubMed Central Full Text PDF:/mnt/data/Google Drive/Zotero/storage/BGZXIBZL/Yue et al. - 2015 - RNA N6-methyladenosine methylation in post-transcr.pdf:application/pdf}
}

@article{berulavaN6adenosineMethylationMiRNAs2015,
	title = {N6-adenosine methylation in {MiRNAs}},
	volume = {10},
	issn = {1932-6203},
	doi = {10.1371/journal.pone.0118438},
	abstract = {Methylation of N6-adenosine (m6A) has been observed in many different classes of RNA, but its prevalence in microRNAs (miRNAs) has not yet been studied. Here we show that a knockdown of the m6A demethylase FTO affects the steady-state levels of several miRNAs. Moreover, RNA immunoprecipitation with an anti-m6A-antibody followed by RNA-seq revealed that a significant fraction of miRNAs contains m6A. By motif searches we have discovered consensus sequences discriminating between methylated and unmethylated miRNAs. The epigenetic modification of an epigenetic modifier as described here adds a new layer to the complexity of the posttranscriptional regulation of gene expression.},
	language = {eng},
	number = {2},
	journal = {PloS One},
	author = {Berulava, Tea and Rahmann, Sven and Rademacher, Katrin and Klein-Hitpass, Ludgar and Horsthemke, Bernhard},
	year = {2015},
	pmid = {25723394},
	pmcid = {PMC4344304},
	keywords = {Humans, RNA, Messenger, Adenosine, Methylation, Alpha-Ketoglutarate-Dependent Dioxygenase FTO, Epigenesis, Genetic, Gene Knockdown Techniques, HEK293 Cells, MicroRNAs, Proteins, RNA Stability},
	pages = {e0118438},
	file = {Full Text:/mnt/data/Google Drive/Zotero/storage/HZBWDIG9/Berulava et al. - 2015 - N6-adenosine methylation in MiRNAs.pdf:application/pdf}
}

@article{dundarIntroductionDifferentialGene,
	title = {Introduction to diﬀerential gene expression analysis using {RNA}-seq},
	language = {en},
	author = {Dündar, Written Friederike and Skrabanek, Luce and Zumbo, Paul},
	pages = {89},
	file = {Dündar et al. - Introduction to diﬀerential gene expression analys.pdf:/mnt/data/Google Drive/Zotero/storage/FD253CIE/Dündar et al. - Introduction to diﬀerential gene expression analys.pdf:application/pdf}
}

@article{liuQNBDifferentialRNA2017,
	title = {{QNB}: differential {RNA} methylation analysis for count-based small-sample sequencing data with a quad-negative binomial model},
	volume = {18},
	issn = {1471-2105},
	shorttitle = {{QNB}},
	url = {https://doi.org/10.1186/s12859-017-1808-4},
	doi = {10.1186/s12859-017-1808-4},
	abstract = {As a newly emerged research area, RNA epigenetics has drawn increasing attention recently for the participation of RNA methylation and other modifications in a number of crucial biological processes. Thanks to high throughput sequencing techniques, such as, MeRIP-Seq, transcriptome-wide RNA methylation profile is now available in the form of count-based data, with which it is often of interests to study the dynamics at epitranscriptomic layer. However, the sample size of RNA methylation experiment is usually very small due to its costs; and additionally, there usually exist a large number of genes whose methylation level cannot be accurately estimated due to their low expression level, making differential RNA methylation analysis a difficult task.},
	number = {1},
	urldate = {2019-02-25},
	journal = {BMC Bioinformatics},
	author = {Liu, Lian and Zhang, Shao-Wu and Huang, Yufei and Meng, Jia},
	month = aug,
	year = {2017},
	pages = {387},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/2KXY76NX/Liu et al. - 2017 - QNB differential RNA methylation analysis for cou.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/2BDKEXHI/s12859-017-1808-4.html:text/html}
}

@article{gelmanUnderstandingPredictiveInformation2014,
	title = {Understanding predictive information criteria for {Bayesian} models},
	volume = {24},
	issn = {1573-1375},
	url = {https://doi.org/10.1007/s11222-013-9416-2},
	doi = {10.1007/s11222-013-9416-2},
	abstract = {We review the Akaike, deviance, and Watanabe-Akaike information criteria from a Bayesian perspective, where the goal is to estimate expected out-of-sample-prediction error using a bias-corrected adjustment of within-sample error. We focus on the choices involved in setting up these measures, and we compare them in three simple examples, one theoretical and two applied. The contribution of this paper is to put all these information criteria into a Bayesian predictive context and to better understand, through small examples, how these methods can apply in practice.},
	language = {en},
	number = {6},
	urldate = {2019-02-28},
	journal = {Statistics and Computing},
	author = {Gelman, Andrew and Hwang, Jessica and Vehtari, Aki},
	month = nov,
	year = {2014},
	keywords = {AIC, Bayes, Cross-validation, DIC, Prediction, WAIC},
	pages = {997--1016},
	file = {Submitted Version:/mnt/data/Google Drive/Zotero/storage/B2VVZDAP/Gelman et al. - 2014 - Understanding predictive information criteria for .pdf:application/pdf}
}

@article{mengProtocolRNAMethylation2014,
	title = {A protocol for {RNA} methylation differential analysis with {MeRIP}-{Seq} data and {exomePeak} {R}/{Bioconductor} package},
	volume = {69},
	issn = {1095-9130},
	doi = {10.1016/j.ymeth.2014.06.008},
	abstract = {Despite the prevalent studies of DNA/Chromatin related epigenetics, such as, histone modifications and DNA methylation, RNA epigenetics has not drawn deserved attention until a new affinity-based sequencing approach MeRIP-Seq was developed and applied to survey the global mRNA N6-methyladenosine (m(6)A) in mammalian cells. As a marriage of ChIP-Seq and RNA-Seq, MeRIP-Seq has the potential to study the transcriptome-wide distribution of various post-transcriptional RNA modifications. We have previously developed an R/Bioconductor package 'exomePeak' for detecting RNA methylation sites under a specific experimental condition or the identifying the differential RNA methylation sites in a case control study from MeRIP-Seq data. Compared with other relatively well studied data types such as ChIP-Seq and RNA-Seq, the study of MeRIP-Seq data is still at very early stage, and existing protocols are not optimized for dealing with the intrinsic characteristic of MeRIP-Seq data. We therein provide here a detailed and easy-to-use protocol of using exomePeak R/Bioconductor package along with other software programs for analysis of MeRIP-Seq data, which covers raw reads alignment, RNA methylation site detection, motif discovery, differential RNA methylation analysis, and functional analysis. Particularly, the rationales behind each processing step as well as the specific method used, the best practice, and possible alternative strategies are briefly discussed. The exomePeak R/Bioconductor package is freely available from Bioconductor: http://www.bioconductor.org/packages/release/bioc/html/exomePeak.html.},
	language = {eng},
	number = {3},
	journal = {Methods (San Diego, Calif.)},
	author = {Meng, Jia and Lu, Zhiliang and Liu, Hui and Zhang, Lin and Zhang, Shaowu and Chen, Yidong and Rao, Manjeet K. and Huang, Yufei},
	month = oct,
	year = {2014},
	pmid = {24979058},
	pmcid = {PMC4194139},
	keywords = {Humans, RNA, Messenger, Animals, Base Sequence, RNA, RNA Processing, Post-Transcriptional, Sequence Analysis, RNA, Software, Differential RNA methylation, DNA Methylation, Epigenomics, exomePeak, MeRIP-Seq, N6-methyladenosine (m6A), RNA methylation},
	pages = {274--281},
	file = {Accepted Version:/mnt/data/Google Drive/Zotero/storage/ZZ5B9YM2/Meng et al. - 2014 - A protocol for RNA methylation differential analys.pdf:application/pdf}
}

@misc{alTranscriptomewideMappingMethyladenosine,
	title = {Transcriptome-wide mapping of {N}(6)-methyladenosine by m(6){A}-seq based on immunocapturing and massively parallel sequencing. - {PubMed} - {NCBI}},
	url = {https://www.ncbi.nlm.nih.gov/pubmed/23288318},
	abstract = {Nat Protoc. 2013 Jan;8(1):176-89. doi: 10.1038/nprot.2012.148. Epub 2013 Jan 3. Research Support, Non-U.S. Gov't},
	language = {en},
	urldate = {2019-03-01},
	author = {al, et, Dominissini D.}
}

@article{cuiHEPeakHMMbasedExome2015,
	title = {{HEPeak}: an {HMM}-based exome peak-finding package for {RNA} epigenome sequencing data},
	volume = {16 Suppl 4},
	issn = {1471-2164},
	shorttitle = {{HEPeak}},
	doi = {10.1186/1471-2164-16-S4-S2},
	abstract = {BACKGROUND: Methylated RNA Immunoprecipatation combined with RNA sequencing (MeRIP-seq) is revolutionizing the de novo study of RNA epigenomics at a higher resolution. However, this new technology poses unique bioinformatics problems that call for novel and sophisticated statistical computational solutions, aiming at identifying and characterizing transcriptome-wide methyltranscriptome.
RESULTS: We developed HEP, a Hidden Markov Model (HMM)-based Exome Peak-finding algorithm for predicting transcriptome methylation sites using MeRIP-seq data. In contrast to exomePeak, our previously developed MeRIP-seq peak calling algorithm, HEPeak models the correlation between continuous bins in an m6A peak region and it is a model-based approach, which admits rigorous statistical inference. HEPeak was evaluated on a simulated MeRIP-seq dataset and achieved higher sensitivity and specificity than exomePeak. HEPeak was also applied to real MeRIP-seq datasets from human HEK293T cell line and mouse midbrain cells and was shown to be able to recapitulate known m6A distribution in transcripts and identify novel m6A sites in long non-coding RNAs.
CONCLUSIONS: In this paper, a novel HMM-based peak calling algorithm, HEPeak, was developed for peak calling for MeRIP-seq data. HEPeak is written in R and is publicly available.},
	language = {eng},
	journal = {BMC genomics},
	author = {Cui, Xiaodong and Meng, Jia and Rao, Manjeet K. and Chen, Yidong and Huang, Yufei},
	year = {2015},
	pmid = {25917296},
	pmcid = {PMC4416174},
	keywords = {Humans, RNA, Messenger, Animals, Methylation, Mice, RNA Processing, Post-Transcriptional, Algorithms, Sequence Analysis, RNA, HEK293 Cells, Epigenomics, Cells, Cultured, Exome, Markov Chains, Mesencephalon},
	pages = {S2},
	file = {Full Text:/mnt/data/Google Drive/Zotero/storage/G7BDPQPG/Cui et al. - 2015 - HEPeak an HMM-based exome peak-finding package fo.pdf:application/pdf}
}

@article{mengExomebasedAnalysisRNA2013,
	title = {Exome-based analysis for {RNA} epigenome sequencing data},
	volume = {29},
	issn = {1367-4811},
	doi = {10.1093/bioinformatics/btt171},
	abstract = {MOTIVATION: Fragmented RNA immunoprecipitation combined with RNA sequencing enabled the unbiased study of RNA epigenome at a near single-base resolution; however, unique features of this new type of data call for novel computational techniques.
RESULT: Through examining the connections of RNA epigenome sequencing data with two well-studied data types, ChIP-Seq and RNA-Seq, we unveiled the salient characteristics of this new data type. The computational strategies were discussed accordingly, and a novel data processing pipeline was proposed that combines several existing tools with a newly developed exome-based approach 'exomePeak' for detecting, representing and visualizing the post-transcriptional RNA modification sites on the transcriptome.
AVAILABILITY: The MATLAB package 'exomePeak' and additional details are available at http://compgenomics.utsa.edu/exomePeak/.},
	language = {eng},
	number = {12},
	journal = {Bioinformatics (Oxford, England)},
	author = {Meng, Jia and Cui, Xiaodong and Rao, Manjeet K. and Chen, Yidong and Huang, Yufei},
	month = jun,
	year = {2013},
	pmid = {23589649},
	pmcid = {PMC3673212},
	keywords = {Humans, Transcriptome, RNA Processing, Post-Transcriptional, bearbeitet, Immunoprecipitation, Sequence Analysis, RNA, Software, Epigenesis, Genetic, HEK293 Cells, Exome},
	pages = {1565--1567},
	file = {btt171_Supplementary_Data.zip:/mnt/data/Google Drive/Zotero/storage/XTJSEZDK/btt171_Supplementary_Data.zip:application/zip;Full Text:/mnt/data/Google Drive/Zotero/storage/4G6CQ7NZ/Meng et al. - 2013 - Exome-based analysis for RNA epigenome sequencing .pdf:application/pdf}
}

@article{antanaviciuteM6aViewerSoftwareDetection2017,
	title = {{m6aViewer}: software for the detection, analysis, and visualization of {N6}-methyladenosine peaks from {m6A}-seq/{ME}-{RIP} sequencing data},
	volume = {23},
	issn = {1469-9001},
	shorttitle = {{m6aViewer}},
	doi = {10.1261/rna.058206.116},
	abstract = {Recent methods for transcriptome-wide N6-methyladenosine (m6A) profiling have facilitated investigations into the RNA methylome and established m6A as a dynamic modification that has critical regulatory roles in gene expression and may play a role in human disease. However, bioinformatics resources available for the analysis of m6A sequencing data are still limited. Here, we describe m6aViewer-a cross-platform application for analysis and visualization of m6A peaks from sequencing data. m6aViewer implements a novel m6A peak-calling algorithm that identifies high-confidence methylated residues with more precision than previously described approaches. The application enables data analysis through a graphical user interface, and thus, in contrast to other currently available tools, does not require the user to be skilled in computer programming. m6aViewer and test data can be downloaded here: http://dna2.leeds.ac.uk/m6a.},
	language = {eng},
	number = {10},
	journal = {RNA (New York, N.Y.)},
	author = {Antanaviciute, Agne and Baquero-Perez, Belinda and Watson, Christopher M. and Harrison, Sally M. and Lascelles, Carolina and Crinnion, Laura and Markham, Alexander F. and Bonthron, David T. and Whitehouse, Adrian and Carr, Ian M.},
	year = {2017},
	pmid = {28724534},
	pmcid = {PMC5602108},
	keywords = {Adenosine, Sequence Analysis, RNA, Software, Computational Biology, RNA methylation, m6A, m6A-seq, next-generation sequencing, peak-calling, User-Computer Interface},
	pages = {1493--1501},
	file = {Full Text:/mnt/data/Google Drive/Zotero/storage/XLZRMJQT/Antanaviciute et al. - 2017 - m6aViewer software for the detection, analysis, a.pdf:application/pdf}
}

@article{zhangBayesianHierarchicalModel2018,
	title = {A {Bayesian} hierarchical model for analyzing methylated {RNA} immunoprecipitation sequencing data},
	volume = {6},
	issn = {2095-4697},
	url = {https://doi.org/10.1007/s40484-018-0149-2},
	doi = {10.1007/s40484-018-0149-2},
	abstract = {BackgroundThe recently emerged technology of methylated RNA immunoprecipitation sequencing (MeRIP-seq) sheds light on the study of RNA epigenetics. This new bioinformatics question calls for effective and robust peaking calling algorithms to detect mRNA methylation sites from MeRIP-seq data.MethodsWe propose a Bayesian hierarchical model to detect methylation sites from MeRIP-seq data. Our modeling approach includes several important characteristics. First, it models the zero-inflated and over-dispersed counts by deploying a zero-inflated negative binomial model. Second, it incorporates a hidden Markov model (HMM) to account for the spatial dependency of neighboring read enrichment. Third, our Bayesian inference allows the proposed model to borrow strength in parameter estimation, which greatly improves the model stability when dealing with MeRIP-seq data with a small number of replicates. We use Markov chain Monte Carlo (MCMC) algorithms to simultaneously infer the model parameters in a de novo fashion. The R Shiny demo is available at https://qiwei.shinyapps.io/BaySeqPeak and the R/C ++ code is available at https://github.com/liqiwei2000/BaySeqPeak.ResultsIn simulation studies, the proposed method outperformed the competing methods exomePeak and MeTPeak, especially when an excess of zeros were present in the data. In real MeRIP-seq data analysis, the proposed method identified methylation sites that were more consistent with biological knowledge, and had better spatial resolution compared to the other methods.ConclusionsIn this study, we develop a Bayesian hierarchical model to identify methylation peaks in MeRIP-seq data. The proposed method has a competitive edge over existing methods in terms of accuracy, robustness and spatial resolution.Open image in new window},
	language = {en},
	number = {3},
	urldate = {2019-03-05},
	journal = {Quantitative Biology},
	author = {Zhang, Minzhe and Li, Qiwei and Xie, Yang},
	month = sep,
	year = {2018},
	keywords = {Bayesian inference, hidden Markov model, MeRIP-seq data, RNA epigenomics, zero-inflated negative binomial},
	pages = {275--286},
	file = {Springer Full Text PDF:/mnt/data/Google Drive/Zotero/storage/9BGA345F/Zhang et al. - 2018 - A Bayesian hierarchical model for analyzing methyl.pdf:application/pdf}
}

@article{dominissiniTranscriptomewideMappingMethyladenosine2013,
	title = {Transcriptome-wide mapping of {N}(6)-methyladenosine by m(6){A}-seq based on immunocapturing and massively parallel sequencing},
	volume = {8},
	issn = {1750-2799},
	doi = {10.1038/nprot.2012.148},
	abstract = {N(6)-methyladenosine-sequencing (m(6)A-seq) is an immunocapturing approach for the unbiased transcriptome-wide localization of m(6)A in high resolution. To our knowledge, this is the first protocol to allow a global view of this ubiquitous RNA modification, and it is based on antibody-mediated enrichment of methylated RNA fragments followed by massively parallel sequencing. Building on principles of chromatin immunoprecipitation-sequencing (ChIP-seq) and methylated DNA immunoprecipitation (MeDIP), read densities of immunoprecipitated RNA relative to untreated input control are used to identify methylated sites. A consensus motif is deduced, and its distance to the point of maximal enrichment is assessed; these measures further corroborate the success of the protocol. Identified locations are intersected in turn with gene architecture to draw conclusions regarding the distribution of m(6)A between and within gene transcripts. When applied to human and mouse transcriptomes, m(6)A-seq generated comprehensive methylation profiles revealing, for the first time, tenets governing the nonrandom distribution of m(6)A. The protocol can be completed within {\textasciitilde}9 d for four different sample pairs (each consists of an immunoprecipitation and corresponding input).},
	language = {eng},
	number = {1},
	journal = {Nature Protocols},
	author = {Dominissini, Dan and Moshitch-Moshkovitz, Sharon and Salmon-Divon, Mali and Amariglio, Ninette and Rechavi, Gideon},
	month = jan,
	year = {2013},
	pmid = {23288318},
	keywords = {Gene Expression Profiling, Humans, Adenosine, Animals, Methylation, Mice, RNA Processing, Post-Transcriptional, Immunoprecipitation},
	pages = {176--189}
}

@article{cuiMeTDiffNovelDifferential2018,
	title = {{MeTDiff}: {A} {Novel} {Differential} {RNA} {Methylation} {Analysis} for {MeRIP}-{Seq} {Data}},
	volume = {15},
	issn = {1545-5963},
	shorttitle = {{MeTDiff}},
	doi = {10.1109/TCBB.2015.2403355},
	abstract = {N6-Methyladenosine (m6A) transcriptome methylation is an exciting new research area that just captures the attention of research community. We present in this paper, MeTDiff, a novel computational tool for predicting differential m6A methylation sites from Methylated RNA immunoprecipitation sequencing (MeRIP-Seq) data. Compared with the existing algorithm exomePeak, the advantages of MeTDiff are that it explicitly models the reads variation in data and also devices a more power likelihood ratio test for differential methylation site prediction. Comprehensive evaluation of MeTDiff's performance using both simulated and real datasets showed that MeTDiff is much more robust and achieved much higher sensitivity and specificity over exomePeak.},
	number = {2},
	journal = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},
	author = {Cui, X. and Zhang, L. and Meng, J. and Rao, M. K. and Chen, Y. and Huang, Y.},
	month = mar,
	year = {2018},
	keywords = {RNA, exomePeak, beta-binomial modeling, bioinformatics, Bioinformatics, Computational biology, Computational modeling, data analysis, differential m6A methylation sites, differential methylation site prediction, differential RNA methylation, differential RNA methylation analysis, genetics, IEEE transactions, IP networks, MeRIP-Seq data, MeTDiff, methylated RNA immunoprecipitation sequencing data, molecular biophysics, molecular configurations, N6-Methyladenosine (m6A), N6-methyladenosine transcriptome methylation, power likelihood ratio test, Robustness, Sequential analysis},
	pages = {526--534},
	file = {IEEE Xplore Abstract Record:/mnt/data/Google Drive/Zotero/storage/X7MDSBTD/7052329.html:text/html;IEEE Xplore Full Text PDF:/mnt/data/Google Drive/Zotero/storage/DTRMZZMY/Cui et al. - 2018 - MeTDiff A Novel Differential RNA Methylation Anal.pdf:application/pdf}
}

@article{meyerDynamicEpitranscriptomeN6methyladenosine2014,
	title = {The dynamic epitranscriptome: {N6}-methyladenosine and gene expression control},
	volume = {15},
	issn = {1471-0080},
	shorttitle = {The dynamic epitranscriptome},
	doi = {10.1038/nrm3785},
	abstract = {N(6)-methyladenosine (m(6)A) is a modified base that has long been known to be present in non-coding RNAs, ribosomal RNA, polyadenylated RNA and at least one mammalian mRNA. However, our understanding of the prevalence of this modification has been fundamentally redefined by transcriptome-wide m(6)A mapping studies, which have shown that m(6)A is present in a large subset of the transcriptome in specific regions of mRNA. This suggests that mRNA may undergo post-transcriptional methylation to regulate its fate and function, which is analogous to methyl modifications in DNA. Thus, the pattern of methylation constitutes an mRNA 'epitranscriptome'. The identification of adenosine methyltransferases ('writers'), m(6)A demethylating enzymes ('erasers') and m(6)A-binding proteins ('readers') is helping to define cellular pathways for the post-transcriptional regulation of mRNAs.},
	language = {eng},
	number = {5},
	journal = {Nature Reviews. Molecular Cell Biology},
	author = {Meyer, Kate D. and Jaffrey, Samie R.},
	month = may,
	year = {2014},
	pmid = {24713629},
	pmcid = {PMC4393108},
	keywords = {Humans, RNA, Messenger, Adenosine, Animals, Methylation, Methyltransferases, Transcriptome, Alpha-Ketoglutarate-Dependent Dioxygenase FTO, Epigenesis, Genetic, Proteins, RNA Stability, AlkB Homolog 5, RNA Demethylase, Dioxygenases, Gene Expression, Membrane Proteins},
	pages = {313--326},
	file = {Accepted Version:/mnt/data/Google Drive/Zotero/storage/89BJZS2C/Meyer and Jaffrey - 2014 - The dynamic epitranscriptome N6-methyladenosine a.pdf:application/pdf}
}

@article{robinsonScalingNormalizationMethod2010,
	title = {A scaling normalization method for differential expression analysis of {RNA}-seq data},
	volume = {11},
	issn = {1474-760X},
	url = {https://doi.org/10.1186/gb-2010-11-3-r25},
	doi = {10.1186/gb-2010-11-3-r25},
	abstract = {The fine detail provided by sequencing-based transcriptome surveys suggests that RNA-seq is likely to become the platform of choice for interrogating steady state RNA. In order to discover biologically important changes in expression, we show that normalization continues to be an essential step in the analysis. We outline a simple and effective method for performing normalization and show dramatically improved results for inferring differential expression in simulated and publicly available data sets.},
	number = {3},
	urldate = {2019-03-20},
	journal = {Genome Biology},
	author = {Robinson, Mark D. and Oshlack, Alicia},
	month = mar,
	year = {2010},
	pages = {R25},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/XB33QA35/Robinson and Oshlack - 2010 - A scaling normalization method for differential ex.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/GSACFJHZ/gb-2010-11-3-r25.html:text/html}
}

@article{liuDRMECountbasedDifferential2016,
	title = {{DRME}: {Count}-based differential {RNA} methylation analysis at small sample size scenario},
	volume = {499},
	issn = {1096-0309},
	shorttitle = {{DRME}},
	doi = {10.1016/j.ab.2016.01.014},
	abstract = {Differential methylation, which concerns difference in the degree of epigenetic regulation via methylation between two conditions, has been formulated as a beta or beta-binomial distribution to address the within-group biological variability in sequencing data. However, a beta or beta-binomial model is usually difficult to infer at small sample size scenario with discrete reads count in sequencing data. On the other hand, as an emerging research field, RNA methylation has drawn more and more attention recently, and the differential analysis of RNA methylation is significantly different from that of DNA methylation due to the impact of transcriptional regulation. We developed DRME to better address the differential RNA methylation problem. The proposed model can effectively describe within-group biological variability at small sample size scenario and handles the impact of transcriptional regulation on RNA methylation. We tested the newly developed DRME algorithm on simulated and 4 MeRIP-Seq case-control studies and compared it with Fisher's exact test. It is in principle widely applicable to several other RNA-related data types as well, including RNA Bisulfite sequencing and PAR-CLIP. The code together with an MeRIP-Seq dataset is available online (https://github.com/lzcyzm/DRME) for evaluation and reproduction of the figures shown in this article.},
	language = {eng},
	journal = {Analytical Biochemistry},
	author = {Liu, Lian and Zhang, Shao-Wu and Gao, Fan and Zhang, Yixin and Huang, Yufei and Chen, Runsheng and Meng, Jia},
	month = apr,
	year = {2016},
	pmid = {26851340},
	keywords = {Methylation, RNA, Algorithms, MeRIP-Seq, RNA methylation, Differential methylation, N(6)-Methyladenosine (m(6)A), Negative binomial distribution, Particle Size, R/Bioconductor package},
	pages = {15--23}
}

@article{liBayesianMixtureModel2017,
	title = {A {Bayesian} mixture model for clustering and selection of feature occurrence rates under mean constraints: {LI} et al.},
	volume = {10},
	issn = {19321864},
	shorttitle = {A {Bayesian} mixture model for clustering and selection of feature occurrence rates under mean constraints},
	url = {http://doi.wiley.com/10.1002/sam.11350},
	doi = {10.1002/sam.11350},
	language = {en},
	number = {6},
	urldate = {2019-03-26},
	journal = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
	author = {Li, Qiwei and Guindani, Michele and Reich, Brian J. and Bondell, Howard D. and Vannucci, Marina},
	month = dec,
	year = {2017},
	pages = {393--409},
	file = {A Bayesian mixture model for clustering and selection of feature occurrence rates under mean constraints - Li - 2017 - Statistical Analysis and Data Mining\: The ASA Data Science Journal - Wiley Online Library:/mnt/data/Google Drive/Zotero/storage/SG97HCMU/sam.html:text/html;Li et al. - 2017 - A Bayesian mixture model for clustering and select.pdf:/mnt/data/Google Drive/Zotero/storage/X7JDUHJL/Li et al. - 2017 - A Bayesian mixture model for clustering and select.pdf:application/pdf}
}

@article{leon-noveloBayesianEstimationNegative2015,
	title = {Bayesian {Estimation} of {Negative} {Binomial} {Parameters} with {Applications} to {RNA}-{Seq} {Data}},
	url = {http://arxiv.org/abs/1512.00475},
	abstract = {RNA-Seq data characteristically exhibits large variances, which need to be appropriately accounted for in the model. We first explore the effects of this variability on the maximum likelihood estimator (MLE) of the overdispersion parameter of the negative binomial distribution, and propose instead the use an estimator obtained via maximization of the marginal likelihood in a conjugate Bayesian framework. We show, via simulation studies, that the marginal MLE can better control this variation and produce a more stable and reliable estimator. We then formulate a conjugate Bayesian hierarchical model, in which the estimate of overdispersion is a marginalized estimate and use this estimator to propose a Bayesian test to detect differentially expressed genes with RNA-Seq data. We use numerical studies to show that our much simpler approach is competitive with other negative binomial based procedures, and we use a real data set to illustrate the implementation and flexibility of the procedure.},
	urldate = {2019-03-26},
	journal = {arXiv:1512.00475 [stat]},
	author = {Leon-Novelo, Luis and Fuentes, Claudio and Emerson, Sarah},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.00475},
	keywords = {Statistics - Methodology},
	file = {arXiv\:1512.00475 PDF:/mnt/data/Google Drive/Zotero/storage/TEJH9NPP/Leon-Novelo et al. - 2015 - Bayesian Estimation of Negative Binomial Parameter.pdf:application/pdf;arXiv.org Snapshot:/mnt/data/Google Drive/Zotero/storage/2EUYYKYK/1512.html:text/html}
}

@article{yuShrinkageEstimationDispersion2013,
	title = {Shrinkage estimation of dispersion in {Negative} {Binomial} models for {RNA}-seq experiments with small sample size},
	volume = {29},
	issn = {1367-4803},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3654711/},
	doi = {10.1093/bioinformatics/btt143},
	abstract = {Motivation: RNA-seq experiments produce digital counts of reads that are affected by both biological and technical variation. To distinguish the systematic changes in expression between conditions from noise, the counts are frequently modeled by the Negative Binomial distribution. However, in experiments with small sample size, the per-gene estimates of the dispersion parameter are unreliable., Method: We propose a simple and effective approach for estimating the dispersions. First, we obtain the initial estimates for each gene using the method of moments. Second, the estimates are regularized, i.e. shrunk towards a common value that minimizes the average squared difference between the initial estimates and the shrinkage estimates. The approach does not require extra modeling assumptions, is easy to compute and is compatible with the exact test of differential expression., Results: We evaluated the proposed approach using 10 simulated and experimental datasets and compared its performance with that of currently popular packages edgeR, DESeq, baySeq, BBSeq and SAMseq. For these datasets, sSeq performed favorably for experiments with small sample size in sensitivity, specificity and computational time., Availability:
http://www.stat.purdue.edu/∼ovitek/Software.html and Bioconductor., Contact:
ovitek@purdue.edu, Supplementary information:
Supplementary data are available at Bioinformatics online.},
	number = {10},
	urldate = {2019-03-27},
	journal = {Bioinformatics},
	author = {Yu, Danni and Huber, Wolfgang and Vitek, Olga},
	month = may,
	year = {2013},
	pmid = {23589650},
	pmcid = {PMC3654711},
	pages = {1275--1282},
	file = {PubMed Central Full Text PDF:/mnt/data/Google Drive/Zotero/storage/FB7Z6CIH/Yu et al. - 2013 - Shrinkage estimation of dispersion in Negative Bin.pdf:application/pdf}
}

@book{kruschkeBayesianEstimationHierarchical2015,
	title = {Bayesian {Estimation} in {Hierarchical} {Models}},
	volume = {1},
	url = {http://oxfordhandbooks.com/view/10.1093/oxfordhb/9780199957996.001.0001/oxfordhb-9780199957996-e-13},
	abstract = {Bayesian data analysis involves describing data by meaningful mathematical models, and allocating credibility to parameter values that are consistent with the data and with prior knowledge. The Bayesian approach is ideally suited for constructing hierarchical models, which are useful for data structures with multiple levels, such as data from individuals who are members of groups which in turn are in higher-level organizations. Hierarchical models have parameters that meaningfully describe the data at their multiple levels and connect information within and across levels. Bayesian methods are very ﬂexible and straightforward for estimating parameters of complex hierarchical models (and simpler models too). We provide an introduction to the ideas of hierarchical models and to the Bayesian estimation of their parameters, illustrated with two extended examples. One example considers baseball batting averages of individual players grouped by ﬁelding position. A second example uses a hierarchical extension of a cognitive process model to examine individual differences in attention allocation of people who have eating disorders. We conclude by discussing Bayesian model comparison as a case of hierarchical modeling.},
	language = {en},
	urldate = {2019-04-05},
	publisher = {Oxford University Press},
	author = {Kruschke, John K. and Vanpaemel, Wolf},
	editor = {Busemeyer, Jerome R. and Wang, Zheng and Townsend, James T. and Eidels, Ami},
	month = dec,
	year = {2015},
	doi = {10.1093/oxfordhb/9780199957996.013.13},
	file = {Kruschke and Vanpaemel - 2015 - Bayesian Estimation in Hierarchical Models.pdf:/mnt/data/Google Drive/Zotero/storage/JGCUA954/Kruschke and Vanpaemel - 2015 - Bayesian Estimation in Hierarchical Models.pdf:application/pdf}
}

@article{piegorschMaximumLikelihoodEstimation1990,
	title = {Maximum {Likelihood} {Estimation} for the {Negative} {Binomial} {Dispersion} {Parameter}},
	volume = {46},
	issn = {0006341X},
	url = {https://www.jstor.org/stable/2532104?origin=crossref},
	doi = {10.2307/2532104},
	abstract = {A follow-up investigation to that given by Clark and Perry (1989, Biometrics 45, 309-316) is presented, giving details for maximum likelihood estimation for the dispersion parameter from a negative binomial distribution.},
	language = {en},
	number = {3},
	urldate = {2019-04-05},
	journal = {Biometrics},
	author = {Piegorsch, Walter W.},
	month = sep,
	year = {1990},
	pages = {863},
	file = {Piegorsch - 1990 - Maximum Likelihood Estimation for the Negative Bin.pdf:/mnt/data/Google Drive/Zotero/storage/W6HQ2J8A/Piegorsch - 1990 - Maximum Likelihood Estimation for the Negative Bin.pdf:application/pdf}
}

@article{lloyd-smithMaximumLikelihoodEstimation2007,
	title = {Maximum {Likelihood} {Estimation} of the {Negative} {Binomial} {Dispersion} {Parameter} for {Highly} {Overdispersed} {Data}, with {Applications} to {Infectious} {Diseases}},
	volume = {2},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0000180},
	doi = {10.1371/journal.pone.0000180},
	abstract = {Background The negative binomial distribution is used commonly throughout biology as a model for overdispersed count data, with attention focused on the negative binomial dispersion parameter, k. A substantial literature exists on the estimation of k, but most attention has focused on datasets that are not highly overdispersed (i.e., those with k≥1), and the accuracy of confidence intervals estimated for k is typically not explored. Methodology This article presents a simulation study exploring the bias, precision, and confidence interval coverage of maximum-likelihood estimates of k from highly overdispersed distributions. In addition to exploring small-sample bias on negative binomial estimates, the study addresses estimation from datasets influenced by two types of event under-counting, and from disease transmission data subject to selection bias for successful outbreaks. Conclusions Results show that maximum likelihood estimates of k can be biased upward by small sample size or under-reporting of zero-class events, but are not biased downward by any of the factors considered. Confidence intervals estimated from the asymptotic sampling variance tend to exhibit coverage below the nominal level, with overestimates of k comprising the great majority of coverage errors. Estimation from outbreak datasets does not increase the bias of k estimates, but can add significant upward bias to estimates of the mean. Because k varies inversely with the degree of overdispersion, these findings show that overestimation of the degree of overdispersion is very rare for these datasets.},
	language = {en},
	number = {2},
	urldate = {2019-04-05},
	journal = {PLOS ONE},
	author = {Lloyd-Smith, James O.},
	month = feb,
	year = {2007},
	keywords = {Binomials, Epidemiology, Infectious disease control, Infectious disease epidemiology, Infectious disease surveillance, Probability distribution, Respiratory infections, Simulation and modeling},
	pages = {e180},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/H886U24V/Lloyd-Smith - 2007 - Maximum Likelihood Estimation of the Negative Bino.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/89I3PGCF/article.html:text/html}
}

@article{rissoZINBWaVEGeneralFlexible2017,
	title = {{ZINB}-{WaVE}: {A} general and flexible method for signal extraction from single-cell {RNA}-seq data},
	copyright = {© 2017, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	shorttitle = {{ZINB}-{WaVE}},
	url = {https://www.biorxiv.org/content/10.1101/125112v1},
	doi = {10.1101/125112},
	abstract = {{\textless}h3{\textgreater}Abstract{\textless}/h3{\textgreater} {\textless}p{\textgreater}Single-cell RNA sequencing (scRNA-seq) is a powerful technique that enables researchers to measure gene expression at the resolution of single cells. Because of the low amount of RNA present in a single cell, many genes fail to be detected even though they are expressed; these genes are usually referred to as dropouts. Here, we present a general and flexible zero-inflated negative binomial model (ZINB-WaVE), which leads to low-dimensional representations of the data that account for zero inflation (dropouts), over-dispersion, and the count nature of the data. We demonstrate, with simulations and real data, that the model and its associated estimation procedure are able to give a more stable and accurate low-dimensional representation of the data than principal component analysis (PCA) and zero-inflated factor analysis (ZIFA), without the need for a preliminary normalization step.{\textless}/p{\textgreater}},
	language = {en},
	urldate = {2019-04-09},
	journal = {bioRxiv},
	author = {Risso, Davide and Perraudeau, Fanny and Gribkova, Svetlana and Dudoit, Sandrine and Vert, Jean-Philippe},
	month = apr,
	year = {2017},
	pages = {125112},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/5GXVPPWT/Risso et al. - 2017 - ZINB-WaVE A general and flexible method for signa.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/4Z26YX54/125112v1.html:text/html}
}

@article{vandenbergeObservationWeightsUnlock2018,
	title = {Observation weights unlock bulk {RNA}-seq tools for zero inflation and single-cell applications},
	volume = {19},
	issn = {1474-760X},
	url = {https://doi.org/10.1186/s13059-018-1406-4},
	doi = {10.1186/s13059-018-1406-4},
	abstract = {Dropout events in single-cell RNA sequencing (scRNA-seq) cause many transcripts to go undetected and induce an excess of zero read counts, leading to power issues in differential expression (DE) analysis. This has triggered the development of bespoke scRNA-seq DE methods to cope with zero inflation. Recent evaluations, however, have shown that dedicated scRNA-seq tools provide no advantage compared to traditional bulk RNA-seq tools. We introduce a weighting strategy, based on a zero-inflated negative binomial model, that identifies excess zero counts and generates gene- and cell-specific weights to unlock bulk RNA-seq DE pipelines for zero-inflated data, boosting performance for scRNA-seq.},
	number = {1},
	urldate = {2019-04-11},
	journal = {Genome Biology},
	author = {Van den Berge, Koen and Perraudeau, Fanny and Soneson, Charlotte and Love, Michael I. and Risso, Davide and Vert, Jean-Philippe and Robinson, Mark D. and Dudoit, Sandrine and Clement, Lieven},
	month = feb,
	year = {2018},
	pages = {24},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/SMUBSS9S/Van den Berge et al. - 2018 - Observation weights unlock bulk RNA-seq tools for .pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/NX28TJNP/s13059-018-1406-4.html:text/html}
}

@incollection{DoingBayesianData2015,
	title = {Doing {Bayesian} {Data} {Analysis} - {Kruschke}},
	isbn = {978-0-12-405888-0},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780124058880099992},
	language = {en},
	urldate = {2019-04-15},
	booktitle = {Doing {Bayesian} {Data} {Analysis}},
	publisher = {Elsevier},
	year = {2015},
	doi = {10.1016/B978-0-12-405888-0.09999-2},
	pages = {i--ii},
	file = {2015 - Front Matter.pdf:/mnt/data/Google Drive/Zotero/storage/PCMMKEAK/2015 - Front Matter.pdf:application/pdf}
}

@article{liBayesianMixtureModel2017a,
	title = {A {Bayesian} mixture model for clustering and selection of feature occurrence rates under mean constraints: {LI} et al.},
	volume = {10},
	issn = {19321864},
	shorttitle = {A {Bayesian} mixture model for clustering and selection of feature occurrence rates under mean constraints},
	url = {http://doi.wiley.com/10.1002/sam.11350},
	doi = {10.1002/sam.11350},
	language = {en},
	number = {6},
	urldate = {2019-04-15},
	journal = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
	author = {Li, Qiwei and Guindani, Michele and Reich, Brian J. and Bondell, Howard D. and Vannucci, Marina},
	month = dec,
	year = {2017},
	pages = {393--409},
	file = {Li et al. - 2017 - A Bayesian mixture model for clustering and select.pdf:/mnt/data/Google Drive/Zotero/storage/TZKGSGF9/Li et al. - 2017 - A Bayesian mixture model for clustering and select.pdf:application/pdf}
}

@article{borgwardtBayesianTwosampleTests2009,
	title = {Bayesian two-sample tests},
	url = {http://arxiv.org/abs/0906.4032},
	abstract = {In this paper, we present two classes of Bayesian approaches to the twosample problem. Our ﬁrst class of methods extends the Bayesian t-test to include all parametric models in the exponential family and their conjugate priors. Our second class of methods uses Dirichlet process mixtures (DPM) of such conjugate-exponential distributions as ﬂexible nonparametric priors over the unknown distributions.},
	language = {en},
	urldate = {2019-04-15},
	journal = {arXiv:0906.4032 [cs]},
	author = {Borgwardt, Karsten M. and Ghahramani, Zoubin},
	month = jun,
	year = {2009},
	note = {arXiv: 0906.4032},
	keywords = {Computer Science - Machine Learning},
	file = {Borgwardt and Ghahramani - 2009 - Bayesian two-sample tests.pdf:/mnt/data/Google Drive/Zotero/storage/ZSQ62H2A/Borgwardt and Ghahramani - 2009 - Bayesian two-sample tests.pdf:application/pdf}
}

@article{kruschkeBayesianEstimationSupersedes2013,
	title = {Bayesian estimation supersedes the t test.},
	volume = {142},
	issn = {1939-2222, 0096-3445},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0029146},
	doi = {10.1037/a0029146},
	abstract = {Bayesian estimation for 2 groups provides complete distributions of credible values for the effect size, group means and their difference, standard deviations and their difference, and the normality of the data. The method handles outliers. The decision rule can accept the null value (unlike traditional t tests) when certainty in the estimate is high (unlike Bayesian model comparison using Bayes factors). The method also yields precise estimates of statistical power for various research goals. The software and programs are free and run on Macintosh, Windows, and Linux platforms.},
	language = {en},
	number = {2},
	urldate = {2019-04-15},
	journal = {Journal of Experimental Psychology: General},
	author = {Kruschke, John K.},
	year = {2013},
	pages = {573--603},
	file = {Kruschke - 2013 - Bayesian estimation supersedes the t test..pdf:/mnt/data/Google Drive/Zotero/storage/FRBW2QBE/Kruschke - 2013 - Bayesian estimation supersedes the t test..pdf:application/pdf}
}

@book{mcelreathStatisticalRethinkingBayesian2018,
	edition = {1},
	title = {Statistical {Rethinking}: {A} {Bayesian} {Course} with {Examples} in {R} and {Stan}},
	isbn = {978-1-315-37249-5},
	shorttitle = {Statistical {Rethinking}},
	url = {https://www.taylorfrancis.com/books/9781315362618},
	language = {en},
	urldate = {2019-04-15},
	publisher = {Chapman and Hall/CRC},
	author = {McElreath, Richard},
	month = jan,
	year = {2018},
	doi = {10.1201/9781315372495},
	file = {McElreath - 2018 - Statistical Rethinking A Bayesian Course with Exa.pdf:/mnt/data/Google Drive/Zotero/storage/GXRZJRFH/McElreath - 2018 - Statistical Rethinking A Bayesian Course with Exa.pdf:application/pdf}
}

@article{ignatiadisDatadrivenHypothesisWeighting2016,
	title = {Data-driven hypothesis weighting increases detection power in genome-scale multiple testing},
	volume = {13},
	copyright = {2016 Nature Publishing Group},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/nmeth.3885},
	doi = {10.1038/nmeth.3885},
	abstract = {Hypothesis weighting improves the power of large-scale multiple testing. We describe independent hypothesis weighting (IHW), a method that assigns weights using covariates independent of the P-values under the null hypothesis but informative of each test's power or prior probability of the null hypothesis (http://www.bioconductor.org/packages/IHW). IHW increases power while controlling the false discovery rate and is a practical approach to discovering associations in genomics, high-throughput biology and other large data sets.},
	language = {en},
	number = {7},
	urldate = {2019-04-16},
	journal = {Nature Methods},
	author = {Ignatiadis, Nikolaos and Klaus, Bernd and Zaugg, Judith B. and Huber, Wolfgang},
	month = jul,
	year = {2016},
	pages = {577--580},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/W4R7AQY3/Ignatiadis et al. - 2016 - Data-driven hypothesis weighting increases detecti.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/VWBZVDZT/nmeth.html:text/html}
}

@article{stephensFalseDiscoveryRates2017,
	title = {False discovery rates: a new deal},
	volume = {18},
	issn = {1465-4644},
	shorttitle = {False discovery rates},
	url = {https://academic.oup.com/biostatistics/article/18/2/275/2557030},
	doi = {10.1093/biostatistics/kxw041},
	abstract = {Summary.  We introduce a new Empirical Bayes approach for large-scale hypothesis testing, including estimating false discovery rates (FDRs), and effect sizes. T},
	language = {en},
	number = {2},
	urldate = {2019-04-16},
	journal = {Biostatistics},
	author = {Stephens, Matthew},
	month = apr,
	year = {2017},
	pages = {275--294},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/RJKBMUJZ/Stephens - 2017 - False discovery rates a new deal.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/F7IGBZJL/2557030.html:text/html}
}

@article{leekSvaseqRemovingBatch2014,
	title = {svaseq: removing batch effects and other unwanted noise from sequencing data},
	volume = {42},
	issn = {0305-1048},
	shorttitle = {svaseq},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4245966/},
	doi = {10.1093/nar/gku864},
	abstract = {It is now known that unwanted noise and unmodeled artifacts such as batch effects can dramatically reduce the accuracy of statistical inference in genomic experiments. These sources of noise must be modeled and removed to accurately measure biological variability and to obtain correct statistical inference when performing high-throughput genomic analysis. We introduced surrogate variable analysis (sva) for estimating these artifacts by (i) identifying the part of the genomic data only affected by artifacts and (ii) estimating the artifacts with principal components or singular vectors of the subset of the data matrix. The resulting estimates of artifacts can be used in subsequent analyses as adjustment factors to correct analyses. Here I describe a version of the sva approach specifically created for count data or FPKMs from sequencing experiments based on appropriate data transformation. I also describe the addition of supervised sva (ssva) for using control probes to identify the part of the genomic data only affected by artifacts. I present a comparison between these versions of sva and other methods for batch effect estimation on simulated data, real count-based data and FPKM-based data. These updates are available through the sva Bioconductor package and I have made fully reproducible analysis using these methods available from: https://github.com/jtleek/svaseq.},
	number = {21},
	urldate = {2019-04-16},
	journal = {Nucleic Acids Research},
	author = {Leek, Jeffrey T.},
	month = dec,
	year = {2014},
	pmid = {25294822},
	pmcid = {PMC4245966},
	pages = {e161},
	file = {PubMed Central Full Text PDF:/mnt/data/Google Drive/Zotero/storage/HXAUTNR9/Leek - 2014 - svaseq removing batch effects and other unwanted .pdf:application/pdf}
}

@article{rissoNormalizationRNAseqData2014,
	title = {Normalization of {RNA}-seq data using factor analysis of control genes or samples},
	volume = {32},
	copyright = {2014 Nature Publishing Group},
	issn = {1546-1696},
	url = {https://www.nature.com/articles/nbt.2931},
	doi = {10.1038/nbt.2931},
	abstract = {Normalization of RNA-sequencing (RNA-seq) data has proven essential to ensure accurate inference of expression levels. Here, we show that usual normalization approaches mostly account for sequencing depth and fail to correct for library preparation and other more complex unwanted technical effects. We evaluate the performance of the External RNA Control Consortium (ERCC) spike-in controls and investigate the possibility of using them directly for normalization. We show that the spike-ins are not reliable enough to be used in standard global-scaling or regression-based normalization procedures. We propose a normalization strategy, called remove unwanted variation (RUV), that adjusts for nuisance technical effects by performing factor analysis on suitable sets of control genes (e.g., ERCC spike-ins) or samples (e.g., replicate libraries). Our approach leads to more accurate estimates of expression fold-changes and tests of differential expression compared to state-of-the-art normalization methods. In particular, RUV promises to be valuable for large collaborative projects involving multiple laboratories, technicians, and/or sequencing platforms.},
	language = {en},
	number = {9},
	urldate = {2019-04-16},
	journal = {Nature Biotechnology},
	author = {Risso, Davide and Ngai, John and Speed, Terence P. and Dudoit, Sandrine},
	month = sep,
	year = {2014},
	pages = {896--902},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/DBFSSZWX/Risso et al. - 2014 - Normalization of RNA-seq data using factor analysi.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/YMGZHS9D/nbt.html:text/html}
}

@article{gerardUnifyingGeneralizingMethods2017,
	title = {Unifying and {Generalizing} {Methods} for {Removing} {Unwanted} {Variation} {Based} on {Negative} {Controls}},
	url = {http://arxiv.org/abs/1705.08393},
	abstract = {Unwanted variation, including hidden confounding, is a well-known problem in many fields, particularly large-scale gene expression studies. Recent proposals to use control genes --- genes assumed to be unassociated with the covariates of interest --- have led to new methods to deal with this problem. Going by the moniker Removing Unwanted Variation (RUV), there are many versions --- RUV1, RUV2, RUV4, RUVinv, RUVrinv, RUVfun. In this paper, we introduce a general framework, RUV*, that both unites and generalizes these approaches. This unifying framework helps clarify connections between existing methods. In particular we provide conditions under which RUV2 and RUV4 are equivalent. The RUV* framework also preserves an advantage of RUV approaches --- their modularity --- which facilitates the development of novel methods based on existing matrix imputation algorithms. We illustrate this by implementing RUVB, a version of RUV* based on Bayesian factor analysis. In realistic simulations based on real data we found that RUVB is competitive with existing methods in terms of both power and calibration, although we also highlight the challenges of providing consistently reliable calibration among data sets.},
	urldate = {2019-04-16},
	journal = {arXiv:1705.08393 [math, stat]},
	author = {Gerard, David and Stephens, Matthew},
	month = may,
	year = {2017},
	note = {arXiv: 1705.08393},
	keywords = {Statistics - Methodology, 62J15 (Primary) 62F15, 62H25, 62P10 (Secondary), Mathematics - Statistics Theory},
	annote = {Comment: 34 pages, 6 figures, methods implemented at https://github.com/dcgerard/vicar , results reproducible at https://github.com/dcgerard/ruvb\_sims},
	file = {arXiv\:1705.08393 PDF:/mnt/data/Google Drive/Zotero/storage/LFR8VJ4F/Gerard and Stephens - 2017 - Unifying and Generalizing Methods for Removing Unw.pdf:application/pdf;arXiv.org Snapshot:/mnt/data/Google Drive/Zotero/storage/8PDIFYVI/1705.html:text/html}
}

@article{huntleyReportingToolsAutomatedResult2013,
	title = {{ReportingTools}: an automated result processing and presentation toolkit for high-throughput genomic analyses},
	volume = {29},
	issn = {1367-4803},
	shorttitle = {{ReportingTools}},
	url = {https://academic.oup.com/bioinformatics/article/29/24/3220/194666},
	doi = {10.1093/bioinformatics/btt551},
	abstract = {Abstract.  Summary: It is common for computational analyses to generate large amounts of complex data that are difficult to process and share with collaborators},
	language = {en},
	number = {24},
	urldate = {2019-04-18},
	journal = {Bioinformatics},
	author = {Huntley, Melanie A. and Larson, Jessica L. and Chaivorapol, Christina and Becker, Gabriel and Lawrence, Michael and Hackney, Jason A. and Kaminker, Joshua S.},
	month = dec,
	year = {2013},
	pages = {3220--3221},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/UUQESTFR/Huntley et al. - 2013 - ReportingTools an automated result processing and.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/9A7HNGQV/194666.html:text/html}
}

@article{eraslanSinglecellRNAseqDenoising2019,
	title = {Single-cell {RNA}-seq denoising using a deep count autoencoder},
	volume = {10},
	copyright = {2019 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-018-07931-2},
	doi = {10.1038/s41467-018-07931-2},
	abstract = {Single-cell RNA sequencing is a powerful method to study gene expression, but noise in the data can obstruct analysis. Here the authors develop a denoising method based on a deep count autoencoder network that scales linearly with the number of cells, and therefore is compatible with large data sets.},
	language = {En},
	number = {1},
	urldate = {2019-04-26},
	journal = {Nature Communications},
	author = {Eraslan, Gökcen and Simon, Lukas M. and Mircea, Maria and Mueller, Nikola S. and Theis, Fabian J.},
	month = jan,
	year = {2019},
	pages = {390},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/SFRPXZPB/Eraslan et al. - 2019 - Single-cell RNA-seq denoising using a deep count a.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/7MN7STMU/s41467-018-07931-2.html:text/html}
}

@article{cahillImprovedIdentificationConcordant2018,
	title = {Improved identification of concordant and discordant gene expression signatures using an updated rank-rank hypergeometric overlap approach},
	volume = {8},
	copyright = {2018 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-018-27903-2},
	doi = {10.1038/s41598-018-27903-2},
	abstract = {Recent advances in large-scale gene expression profiling necessitate concurrent development of biostatistical approaches to reveal meaningful biological relationships. Most analyses rely on significance thresholds for identifying differentially expressed genes. We use an approach to compare gene expression datasets using ‘threshold-free’ comparisons. Significance cut-offs to identify genes shared between datasets may be too stringent and may miss concordant patterns of gene expression with potential biological relevance. A threshold-free approach gaining popularity in several research areas, including neuroscience, is Rank–Rank Hypergeometric Overlap (RRHO). Genes are ranked by their p-value and effect size direction, and ranked lists are compared to identify significantly overlapping genes across a continuous significance gradient rather than at a single arbitrary cut-off. We have updated the previous RRHO analysis by accurately detecting overlap of genes changed in the same and opposite directions between two datasets. Here, we use simulated and real data to show the drawbacks of the previous algorithm as well as the utility of our new algorithm. For example, we show the power of detecting discordant transcriptional patterns in the postmortem brain of subjects with psychiatric disorders. The new R package, RRHO2, offers a new, more intuitive visualization of concordant and discordant gene overlap.},
	language = {En},
	number = {1},
	urldate = {2019-04-30},
	journal = {Scientific Reports},
	author = {Cahill, Kelly M. and Huo, Zhiguang and Tseng, George C. and Logan, Ryan W. and Seney, Marianne L.},
	month = jun,
	year = {2018},
	pages = {9588},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/EXQLU2YM/Cahill et al. - 2018 - Improved identification of concordant and discorda.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/2TAF3FYG/s41598-018-27903-2.html:text/html}
}

@article{kharchenkoBayesianApproachSinglecell2014,
	title = {Bayesian approach to single-cell differential expression analysis},
	volume = {11},
	copyright = {2014 Nature Publishing Group},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/nmeth.2967},
	doi = {10.1038/nmeth.2967},
	abstract = {Single-cell data provide a means to dissect the composition of complex tissues and specialized cellular environments. However, the analysis of such measurements is complicated by high levels of technical noise and intrinsic biological variability. We describe a probabilistic model of expression-magnitude distortions typical of single-cell RNA-sequencing measurements, which enables detection of differential expression signatures and identification of subpopulations of cells in a way that is more tolerant of noise.},
	language = {en},
	number = {7},
	urldate = {2019-05-03},
	journal = {Nature Methods},
	author = {Kharchenko, Peter V. and Silberstein, Lev and Scadden, David T.},
	month = jul,
	year = {2014},
	pages = {740--742},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/URQFX6DA/Kharchenko et al. - 2014 - Bayesian approach to single-cell differential expr.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/J6ZBLML6/nmeth.html:text/html}
}

@article{fanCharacterizingTranscriptionalHeterogeneity2016,
	title = {Characterizing transcriptional heterogeneity through pathway and gene set overdispersion analysis},
	volume = {13},
	copyright = {2016 Nature Publishing Group},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/nmeth.3734},
	doi = {10.1038/nmeth.3734},
	abstract = {The transcriptional state of a cell reflects a variety of biological factors, from cell-type-specific features to transient processes such as the cell cycle, all of which may be of interest. However, identifying such aspects from noisy single-cell RNA-seq data remains challenging. We developed pathway and gene set overdispersion analysis (PAGODA) to resolve multiple, potentially overlapping aspects of transcriptional heterogeneity by testing gene sets for coordinated variability among measured cells.},
	language = {en},
	number = {3},
	urldate = {2019-05-03},
	journal = {Nature Methods},
	author = {Fan, Jean and Salathia, Neeraj and Liu, Rui and Kaeser, Gwendolyn E. and Yung, Yun C. and Herman, Joseph L. and Kaper, Fiona and Fan, Jian-Bing and Zhang, Kun and Chun, Jerold and Kharchenko, Peter V.},
	month = mar,
	year = {2016},
	pages = {241--244},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/VA49TQ3K/Fan et al. - 2016 - Characterizing transcriptional heterogeneity throu.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/J79YC3R4/nmeth.html:text/html}
}

@article{bacherSCnormRobustNormalization2017,
	title = {{SCnorm}: robust normalization of single-cell {RNA}-seq data},
	volume = {14},
	copyright = {2017 Nature Publishing Group},
	issn = {1548-7105},
	shorttitle = {{SCnorm}},
	url = {https://www.nature.com/articles/nmeth.4263},
	doi = {10.1038/nmeth.4263},
	abstract = {The normalization of RNA-seq data is essential for accurate downstream inference, but the assumptions upon which most normalization methods are based are not applicable in the single-cell setting. Consequently, applying existing normalization methods to single-cell RNA-seq data introduces artifacts that bias downstream analyses. To address this, we introduce SCnorm for accurate and efficient normalization of single-cell RNA-seq data.},
	language = {en},
	number = {6},
	urldate = {2019-05-05},
	journal = {Nature Methods},
	author = {Bacher, Rhonda and Chu, Li-Fang and Leng, Ning and Gasch, Audrey P. and Thomson, James A. and Stewart, Ron M. and Newton, Michael and Kendziorski, Christina},
	month = jun,
	year = {2017},
	pages = {584--586},
	file = {Accepted Version:/mnt/data/Google Drive/Zotero/storage/HBMJDWG2/Bacher et al. - 2017 - SCnorm robust normalization of single-cell RNA-se.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/AJBKL4QM/nmeth.html:text/html}
}

@article{ruedaDifferentialExpressionAnalysis,
	title = {Differential {Expression} {Analysis} using {edgeR}},
	language = {en},
	author = {Rueda, Oscar and Pereira, Bernard},
	pages = {4},
	file = {Rueda and Pereira - Differential Expression Analysis using edgeR.pdf:/mnt/data/Google Drive/Zotero/storage/ZXP86Y7T/Rueda and Pereira - Differential Expression Analysis using edgeR.pdf:application/pdf}
}

@article{kruschkeBayesianEstimationSupersedes2013a,
	title = {Bayesian estimation supersedes the t test.},
	volume = {142},
	issn = {1939-2222, 0096-3445},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0029146},
	doi = {10.1037/a0029146},
	language = {en},
	number = {2},
	urldate = {2019-05-06},
	journal = {Journal of Experimental Psychology: General},
	author = {Kruschke, John K.},
	year = {2013},
	pages = {573--603}
}

@article{guhaBayesianHiddenMarkov2008,
	title = {Bayesian {Hidden} {Markov} {Modeling} of {Array} {CGH} {Data}},
	volume = {103},
	issn = {0162-1459},
	doi = {10.1198/016214507000000923},
	abstract = {Genomic alterations have been linked to the development and progression of cancer. The technique of comparative genomic hybridization (CGH) yields data consisting of fluorescence intensity ratios of test and reference DNA samples. The intensity ratios provide information about the number of copies in DNA. Practical issues such as the contamination of tumor cells in tissue specimens and normalization errors necessitate the use of statistics for learning about the genomic alterations from array CGH data. As increasing amounts of array CGH data become available, there is a growing need for automated algorithms for characterizing genomic profiles. Specifically, there is a need for algorithms that can identify gains and losses in the number of copies based on statistical considerations, rather than merely detect trends in the data.We adopt a Bayesian approach, relying on the hidden Markov model to account for the inherent dependence in the intensity ratios. Posterior inferences are made about gains and losses in copy number. Localized amplifications (associated with oncogene mutations) and deletions (associated with mutations of tumor suppressors) are identified using posterior probabilities. Global trends such as extended regions of altered copy number are detected. Because the posterior distribution is analytically intractable, we implement a Metropolis-within-Gibbs algorithm for efficient simulation-based inference. Publicly available data on pancreatic adenocarcinoma, glioblastoma multiforme, and breast cancer are analyzed, and comparisons are made with some widely used algorithms to illustrate the reliability and success of the technique.},
	language = {eng},
	number = {482},
	journal = {Journal of the American Statistical Association},
	author = {Guha, Subharup and Li, Yi and Neuberg, Donna},
	month = jun,
	year = {2008},
	pmid = {22375091},
	pmcid = {PMC3286622},
	pages = {485--497},
	file = {Full Text:/mnt/data/Google Drive/Zotero/storage/39BWIRR9/Guha et al. - 2008 - Bayesian Hidden Markov Modeling of Array CGH Data.pdf:application/pdf}
}

@article{wuNewShrinkageEstimator2013a,
	title = {A new shrinkage estimator for dispersion improves differential expression detection in {RNA}-seq data},
	volume = {14},
	issn = {1465-4644},
	url = {https://academic.oup.com/biostatistics/article/14/2/232/376433},
	doi = {10.1093/biostatistics/kxs033},
	abstract = {Abstract.  Recent developments in RNA-sequencing (RNA-seq) technology have led to a rapid increase in gene expression data in the form of counts. RNA-seq can be},
	language = {en},
	number = {2},
	urldate = {2019-05-14},
	journal = {Biostatistics},
	author = {Wu, Hao and Wang, Chi and Wu, Zhijin},
	month = apr,
	year = {2013},
	pages = {232--243},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/HQ3G25GD/Wu et al. - 2013 - A new shrinkage estimator for dispersion improves .pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/J3FQHLX9/376433.html:text/html}
}

@article{cahillImprovedIdentificationConcordant2018a,
	title = {Improved identification of concordant and discordant gene expression signatures using an updated rank-rank hypergeometric overlap approach},
	volume = {8},
	copyright = {2018 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-018-27903-2},
	doi = {10.1038/s41598-018-27903-2},
	abstract = {Recent advances in large-scale gene expression profiling necessitate concurrent development of biostatistical approaches to reveal meaningful biological relationships. Most analyses rely on significance thresholds for identifying differentially expressed genes. We use an approach to compare gene expression datasets using ‘threshold-free’ comparisons. Significance cut-offs to identify genes shared between datasets may be too stringent and may miss concordant patterns of gene expression with potential biological relevance. A threshold-free approach gaining popularity in several research areas, including neuroscience, is Rank–Rank Hypergeometric Overlap (RRHO). Genes are ranked by their p-value and effect size direction, and ranked lists are compared to identify significantly overlapping genes across a continuous significance gradient rather than at a single arbitrary cut-off. We have updated the previous RRHO analysis by accurately detecting overlap of genes changed in the same and opposite directions between two datasets. Here, we use simulated and real data to show the drawbacks of the previous algorithm as well as the utility of our new algorithm. For example, we show the power of detecting discordant transcriptional patterns in the postmortem brain of subjects with psychiatric disorders. The new R package, RRHO2, offers a new, more intuitive visualization of concordant and discordant gene overlap.},
	language = {En},
	number = {1},
	urldate = {2019-05-14},
	journal = {Scientific Reports},
	author = {Cahill, Kelly M. and Huo, Zhiguang and Tseng, George C. and Logan, Ryan W. and Seney, Marianne L.},
	month = jun,
	year = {2018},
	pages = {9588},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/R7GP5J3D/Cahill et al. - 2018 - Improved identification of concordant and discorda.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/UIBAJM9R/s41598-018-27903-2.html:text/html}
}

@article{rissoRUVSeqRemoveUnwanted,
	title = {{RUVSeq}: {Remove} {Unwanted} {Variation} from {RNA}-{Seq} {Data}},
	language = {en},
	author = {Risso, Davide},
	pages = {9},
	file = {Risso - RUVSeq Remove Unwanted Variation from RNA-Seq Dat.pdf:/mnt/data/Google Drive/Zotero/storage/MU7WPLGA/Risso - RUVSeq Remove Unwanted Variation from RNA-Seq Dat.pdf:application/pdf}
}

@misc{gerardVariousIdeasConfounder2019,
	title = {Various {Ideas} for {Confounder} {Adjustment} in {Regression}: dcgerard/vicar},
	copyright = {GPL-3.0},
	shorttitle = {Various {Ideas} for {Confounder} {Adjustment} in {Regression}},
	url = {https://github.com/dcgerard/vicar},
	urldate = {2019-05-14},
	author = {Gerard, David},
	month = apr,
	year = {2019},
	note = {original-date: 2016-06-07T16:40:43Z}
}

@article{lambertZeroInflatedPoissonRegression1992,
	title = {Zero-{Inflated} {Poisson} {Regression}, with an {Application} to {Defects} in {Manufacturing}},
	volume = {34},
	issn = {0040-1706},
	url = {https://www.jstor.org/stable/1269547},
	doi = {10.2307/1269547},
	abstract = {[Zero-inflated Poisson (ZIP) regression is a model for count data with excess zeros. It assumes that with probability p the only possible observation is 0, and with probability 1 - p, a Poisson(λ) random variable is observed. For example, when manufacturing equipment is properly aligned, defects may be nearly impossible. But when it is misaligned, defects may occur according to a Poisson(λ) distribution. Both the probability p of the perfect, zero defect state and the mean number of defects λ in the imperfect state may depend on covariates. Sometimes p and λ are unrelated; other times p is a simple function of λ such as p=1/(1+λ $^{\textrm{τ}}$) for an unknown constant τ. In either case, ZIP regression models are easy to fit. The maximum likelihood estimates (MLE's) are approximately normal in large samples, and confidence intervals can be constructed by inverting likelihood ratio tests or using the approximate normality of the MLE's. Simulations suggest that the confidence intervals based on likelihood ratio tests are better, however. Finally, ZIP regression models are not only easy to interpret, but they can also lead to more refined data analyses. For example, in an experiment concerning soldering defects on printed wiring boards, two sets of conditions gave about the same mean number of defects, but the perfect state was more likely under one set of conditions and the mean number of defects in the imperfect state was smaller under the other set of conditions; that is, ZIP regression can show not only which conditions give lower mean number of defects but also why the means are lower.]},
	number = {1},
	urldate = {2019-05-15},
	journal = {Technometrics},
	author = {Lambert, Diane},
	year = {1992},
	pages = {1--14}
}

@article{lambertZeroInflatedPoissonRegression1992a,
	title = {Zero-{Inflated} {Poisson} {Regression}, {With} {An} {Application} to {Defects} in {Manufacturing}},
	volume = {34},
	doi = {10.1080/00401706.1992.10485228},
	abstract = {Zero-inflated Poisson (ZIP) regression is a model for count data with excess zeros. It assumes that with probability p the only possible observation is 0, and with probability 1 – p, a Poisson(λ) random variable is observed. For example, when manufacturing equipment is properly aligned, defects may be nearly impossible. But when it is misaligned, defects may occur according to a Poisson(λ) distribution. Both the probability p of the perfect, zero defect state and the mean number of defects λ in the imperfect state may depend on covariates. Sometimes p and λ are unrelated; other times p is a simple function of λ such as p = l/(1 + λ) for an unknown constant T. In either case, ZIP regression models are easy to fit. The maximum likelihood estimates (MLE's) are approximately normal in large samples, and confidence intervals can be constructed by inverting likelihood ratio tests or using the approximate normality of the MLE's. Simulations suggest that the confidence intervals based on likelihood ratio tests are better, however. Finally, ZIP regression models are not only easy to interpret, but they can also lead to more refined data analyses. For example, in an experiment concerning soldering defects on printed wiring boards, two sets of conditions gave about the same mean number of defects, but the perfect state was more likely under one set of conditions and the mean number of defects in the imperfect state was smaller under the other set of conditions; that is, ZIP regression can show not only which conditions give lower mean number of defects but also why the means are lower.},
	journal = {Technometrics},
	author = {Lambert, Diane},
	month = feb,
	year = {1992},
	pages = {1--14},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/ZNF6FCZT/Lambert - 1992 - Zero-Inflated Poisson Regression, With An Applicat.pdf:application/pdf}
}

@article{huangBiologicalFunctionsMicroRNAs2011,
	title = {Biological functions of {microRNAs}: a review},
	volume = {67},
	issn = {1877-8755},
	shorttitle = {Biological functions of {microRNAs}},
	url = {https://doi.org/10.1007/s13105-010-0050-6},
	doi = {10.1007/s13105-010-0050-6},
	abstract = {MicroRNAs (miRNAs) are a recently discovered family of endogenous, noncoding RNA molecules approximately 22 nt in length. miRNAs modulate gene expression post-transcriptionally by binding to complementary sequences in the coding or 3′ untranslated region of target messenger RNAs (mRNAs). It is now clear that the biogenesis and function of miRNAs are related to the molecular mechanisms of various clinical diseases, and that they can potentially regulate every aspect of cellular activity, including differentiation and development, metabolism, proliferation, apoptotic cell death, viral infection and tumorgenesis. Here, we review recent advances in miRNA research, and discuss the diverse roles of miRNAs in disease.},
	language = {en},
	number = {1},
	urldate = {2019-05-22},
	journal = {Journal of Physiology and Biochemistry},
	author = {Huang, Yong and Shen, Xing Jia and Zou, Quan and Wang, Sheng Peng and Tang, Shun Ming and Zhang, Guo Zheng},
	month = mar,
	year = {2011},
	keywords = {Biogenesis, Disease, Expression, Function, Keywords, MicroRNA},
	pages = {129--139},
	file = {Springer Full Text PDF:/mnt/data/Google Drive/Zotero/storage/FXH38JHG/Huang et al. - 2011 - Biological functions of microRNAs a review.pdf:application/pdf}
}

@book{m.davisMicroRNAsNotFineTuners2015,
	title = {{MicroRNAs}: {Not} “{Fine}-{Tuners}” but {Key} {Regulators} of {Neuronal} {Development} and {Function}},
	volume = {6},
	shorttitle = {{MicroRNAs}},
	abstract = {MicroRNAs (miRNAs) are a class of short non-coding RNAs that operate as prominent post-transcriptional regulators of eukaryotic gene expression. miRNAs are abundantly expressed in the brain of most animals and exert diverse roles. The anatomical and functional complexity of the brain requires the precise coordination of multilayered gene regulatory networks. The flexibility, speed, and reversibility of miRNA function provide precise temporal and spatial gene regulatory capabilities that are crucial for the correct functioning of the brain. Studies have shown that the underlying molecular mechanisms controlled by miRNAs in the nervous systems of invertebrate and vertebrate models are remarkably conserved in humans. We endeavor to provide insight into the roles of miRNAs in the nervous systems of these model organisms and discuss how such information may be used to inform regarding diseases of the human brain.},
	author = {M. Davis, Gregory and Haas, Matilda and Pocock, Roger},
	month = nov,
	year = {2015},
	doi = {10.3389/fneur.2015.00245},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/HKCBZPB5/M. Davis et al. - 2015 - MicroRNAs Not “Fine-Tuners” but Key Regulators of.pdf:application/pdf}
}

@article{caiBriefReviewMechanisms2009,
	title = {A {Brief} {Review} on the {Mechanisms} of {miRNA} {Regulation}},
	volume = {7},
	issn = {1672-0229},
	url = {http://www.sciencedirect.com/science/article/pii/S1672022908600443},
	doi = {10.1016/S1672-0229(08)60044-3},
	abstract = {MicroRNAs (miRNAs) are a class of short, endogenously-initiated non-coding RNAs that post-transcriptionally control gene expression via either translational repression or mRNA degradation. It is becoming evident that miRNAs are playing significant roles in regulatory mechanisms operating in various organisms, including developmental timing and host-pathogen interactions as well as cell differentiation, proliferation, apoptosis and tumorigenesis. Likewise, as a regulatory element, miRNA itself is coordinatively modulated by multifarious effectors when carrying out basic functions, such as SNP, miRNA editing, methylation and circadian clock. This mini-review summarized the current understanding of interactions between miRNAs and their targets, including recent advancements in deciphering the regulatory mechanisms that control the biogenesis and functionality of miRNAs in various cellular processes.},
	number = {4},
	urldate = {2019-05-22},
	journal = {Genomics, Proteomics \& Bioinformatics},
	author = {Cai, Yimei and Yu, Xiaomin and Hu, Songnian and Yu, Jun},
	month = dec,
	year = {2009},
	keywords = {miRNA, miRNA regulation, non-coding RNA, targets},
	pages = {147--154},
	file = {jaskiewicz2012.pdf:/mnt/data/Google Drive/Zotero/storage/5SXG7AHS/jaskiewicz2012.pdf:application/pdf;ScienceDirect Full Text PDF:/mnt/data/Google Drive/Zotero/storage/7HXU67AZ/Cai et al. - 2009 - A Brief Review on the Mechanisms of miRNA Regulati.pdf:application/pdf;ScienceDirect Snapshot:/mnt/data/Google Drive/Zotero/storage/HP44VFNY/S1672022908600443.html:text/html}
}

@article{wahidMicroRNAsSynthesisMechanism2010,
	title = {{MicroRNAs}: {Synthesis}, mechanism, function, and recent clinical trials},
	volume = {1803},
	issn = {0167-4889},
	shorttitle = {{MicroRNAs}},
	url = {http://www.sciencedirect.com/science/article/pii/S0167488910001837},
	doi = {10.1016/j.bbamcr.2010.06.013},
	abstract = {MicroRNAs (miRNAs) are a class of small, endogenous RNAs of 21–25 nucleotides (nts) in length. They play an important regulatory role in animals and plants by targeting specific mRNAs for degradation or translation repression. Recent scientific advances have revealed the synthesis pathways and the regulatory mechanisms of miRNAs in animals and plants. miRNA-based regulation is implicated in disease etiology and has been studied for treatment. Furthermore, several preclinical and clinical trials have been initiated for miRNA-based therapeutics. In this review, the existing knowledge about miRNAs synthesis, mechanisms for regulation of the genome, and their widespread functions in animals and plants is summarized. The current status of preclinical and clinical trials regarding miRNA therapeutics is also reviewed. The recent findings in miRNA studies, summarized in this review, may add new dimensions to small RNA biology and miRNA therapeutics.},
	number = {11},
	urldate = {2019-05-22},
	journal = {Biochimica et Biophysica Acta (BBA) - Molecular Cell Research},
	author = {Wahid, Fazli and Shehzad, Adeeb and Khan, Taous and Kim, You Young},
	month = nov,
	year = {2010},
	keywords = {miRNA, miRNA based gene regulations, miRNA therapeutics, mRNA degradation, Small RNAs},
	pages = {1231--1243},
	file = {ScienceDirect Full Text PDF:/mnt/data/Google Drive/Zotero/storage/9VVBJ4HA/Wahid et al. - 2010 - MicroRNAs Synthesis, mechanism, function, and rec.pdf:application/pdf;ScienceDirect Snapshot:/mnt/data/Google Drive/Zotero/storage/DP52PSGJ/S0167488910001837.html:text/html}
}

@article{mittalSeqCLIPMiRNA2014,
	title = {Seq and {CLIP} through the {miRNA} world},
	volume = {15},
	issn = {1474-760X},
	doi = {10.1186/gb4151},
	abstract = {High-throughput sequencing of RNAs crosslinked to Argonaute proteins reveals not only a multitude of atypical miRNA binding sites but also of miRNA targets with atypical functions, and can be used to infer quantitative models of miRNA-target interaction strength.},
	language = {eng},
	number = {1},
	journal = {Genome Biology},
	author = {Mittal, Nitish and Zavolan, Mihaela},
	month = jan,
	year = {2014},
	pmid = {24460822},
	pmcid = {PMC4053862},
	keywords = {Humans, Animals, RNA, Untranslated, Immunoprecipitation, Sequence Analysis, RNA, High-Throughput Nucleotide Sequencing, MicroRNAs, Argonaute Proteins, Binding Sites},
	pages = {202},
	file = {Full Text:/mnt/data/Google Drive/Zotero/storage/ZH9SALWU/Mittal and Zavolan - 2014 - Seq and CLIP through the miRNA world.pdf:application/pdf}
}

@article{matopradoInvestigatingMiRNAmRNARegulatory2016,
	title = {Investigating {miRNA}-{mRNA} regulatory networks using crosslinking immunoprecipitation methods for biomarker and target discovery in cancer},
	volume = {16},
	issn = {1744-8352},
	doi = {10.1080/14737159.2016.1239532},
	abstract = {INTRODUCTION: MicroRNAs (miRNAs) are small non-coding RNAs that regulate gene expression at the post-transcriptional level. Recently, different experimental approaches, such as RNA Sequencing, crosslinking immunoprecipitation (CLIP) methods and its variations, together with computational approaches have been developed to elucidate the miRNA-mRNA targetome. Areas covered: This report focuses on comparing the different experimental and computational approaches, describing their advantages and disadvantages and providing several examples of preclinical (in vitro and in vivo) and clinical studies that have identified miRNA target genes in various tumour types, including breast, ovary, colorectal and pancreas. Expert commentary: The combination of CLIP methods with bioinformatic analyses is essential to better predict miRNA-mRNA interactions and associate their specific pathways within the extensive regulatory network. Nevertheless, further studies are needed to overcome the difficulties these methods have, in order to find a gold standard method that identifies, without any bias, the regulatory association between miRNAs and their target mRNAs.},
	language = {eng},
	number = {11},
	journal = {Expert Review of Molecular Diagnostics},
	author = {Mato Prado, Mireia and Frampton, Adam E. and Giovannetti, Elisa and Stebbing, Justin and Castellano, Leandro and Krell, Jonathan},
	year = {2016},
	pmid = {27784183},
	keywords = {Humans, RNA, Messenger, Animals, Immunoprecipitation, Sequence Analysis, RNA, High-Throughput Nucleotide Sequencing, Computational Biology, MicroRNAs, biomarker, Biomarkers, Tumor, cancer, CLASH, Gene Regulatory Networks, microRNA, Neoplasms, pancreatic Cancer, PAR-CLIP, RNA Interference, RNA sequencing, targetome, therapy},
	pages = {1155--1162}
}

@article{renSmallRNAsMeet2014,
	title = {Small {RNAs} meet their targets: when methylation defends {miRNAs} from uridylation},
	volume = {11},
	issn = {1555-8584},
	shorttitle = {Small {RNAs} meet their targets},
	doi = {10.4161/rna.36243},
	abstract = {Small RNAs are incorporated into Argonaute protein-containing complexes to guide the silencing of target RNAs in both animals and plants. The abundance of endogenous small RNAs is precisely controlled at multiple levels including transcription, processing and Argonaute loading. In addition to these processes, 3' end modification of small RNAs, the topic of a research area that has rapidly evolved over the last several years, adds another layer of regulation of their abundance, diversity and function. Here, we review our recent understanding of small RNA 3' end methylation and tailing.},
	language = {eng},
	number = {9},
	journal = {RNA biology},
	author = {Ren, Guodong and Chen, Xuemei and Yu, Bin},
	year = {2014},
	pmid = {25483033},
	pmcid = {PMC4615765},
	keywords = {Humans, Animals, Methylation, MicroRNAs, Argonaute Proteins, argonaute, methylation, RNA 3' End Processing, RNA, Small Interfering, small RNA, Uridine, uridylation},
	pages = {1099--1104},
	file = {Full Text:/mnt/data/Google Drive/Zotero/storage/MKJ7WUQC/Ren et al. - 2014 - Small RNAs meet their targets when methylation de.pdf:application/pdf}
}

@article{floresDifferentialRISCAssociation2014,
	title = {Differential {RISC} association of endogenous human {microRNAs} predicts their inhibitory potential},
	volume = {42},
	issn = {0305-1048},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3985621/},
	doi = {10.1093/nar/gkt1393},
	abstract = {It has previously been assumed that the generally high stability of microRNAs (miRNAs) reflects their tight association with Argonaute (Ago) proteins, essential components of the RNA-induced silencing complex (RISC). However, recent data have suggested that the majority of mature miRNAs are not, in fact, Ago associated. Here, we demonstrate that endogenous human miRNAs vary widely, by {\textgreater}100-fold, in their level of RISC association and show that the level of Ago binding is a better indicator of inhibitory potential than is the total level of miRNA expression. While miRNAs of closely similar sequence showed comparable levels of RISC association in the same cell line, these varied between different cell types. Moreover, the level of RISC association could be modulated by overexpression of complementary target mRNAs. Together, these data indicate that the level of RISC association of a given endogenous miRNA is regulated by the available RNA targetome and predicts miRNA function.},
	number = {7},
	urldate = {2019-05-22},
	journal = {Nucleic Acids Research},
	author = {Flores, Omar and Kennedy, Edward M. and Skalsky, Rebecca L. and Cullen, Bryan R.},
	month = apr,
	year = {2014},
	pmid = {24464996},
	pmcid = {PMC3985621},
	pages = {4629--4639},
	file = {PubMed Central Full Text PDF:/mnt/data/Google Drive/Zotero/storage/R2LLUIRP/Flores et al. - 2014 - Differential RISC association of endogenous human .pdf:application/pdf}
}

@article{melnykovDistributionPosteriorProbabilities2013,
	title = {On the distribution of posterior probabilities in finite mixture models with application in clustering},
	volume = {122},
	issn = {0047-259X},
	url = {http://www.sciencedirect.com/science/article/pii/S0047259X13001462},
	doi = {10.1016/j.jmva.2013.07.014},
	abstract = {The paper discusses an approach based on the multivariate Delta method for approximating the distribution of posterior probabilities in finite mixture models. It can be used for developing distributions of many other characteristics involving posterior probabilities such as the entropy of fuzzy classification or expected cluster sizes. An application of the proposed methodology to clustering through merging mixture components is proposed and discussed. The methodology is studied and illustrated on simulated and well-known classification datasets with good results.},
	urldate = {2019-05-29},
	journal = {Journal of Multivariate Analysis},
	author = {Melnykov, Volodymyr},
	month = nov,
	year = {2013},
	keywords = {BIC, Delta method, Distribution of posterior probabilities, Entropy, ICL, Model-based clustering, Multivariate Gaussian mixtures},
	pages = {175--189}
}

@article{leekSvaseqRemovingBatch2014a,
	title = {svaseq: removing batch effects and other unwanted noise from sequencing data},
	volume = {42},
	issn = {1362-4962},
	shorttitle = {svaseq},
	doi = {10.1093/nar/gku864},
	abstract = {It is now known that unwanted noise and unmodeled artifacts such as batch effects can dramatically reduce the accuracy of statistical inference in genomic experiments. These sources of noise must be modeled and removed to accurately measure biological variability and to obtain correct statistical inference when performing high-throughput genomic analysis. We introduced surrogate variable analysis (sva) for estimating these artifacts by (i) identifying the part of the genomic data only affected by artifacts and (ii) estimating the artifacts with principal components or singular vectors of the subset of the data matrix. The resulting estimates of artifacts can be used in subsequent analyses as adjustment factors to correct analyses. Here I describe a version of the sva approach specifically created for count data or FPKMs from sequencing experiments based on appropriate data transformation. I also describe the addition of supervised sva (ssva) for using control probes to identify the part of the genomic data only affected by artifacts. I present a comparison between these versions of sva and other methods for batch effect estimation on simulated data, real count-based data and FPKM-based data. These updates are available through the sva Bioconductor package and I have made fully reproducible analysis using these methods available from: https://github.com/jtleek/svaseq.},
	language = {eng},
	number = {21},
	journal = {Nucleic Acids Research},
	author = {Leek, Jeffrey T.},
	month = dec,
	year = {2014},
	pmid = {25294822},
	pmcid = {PMC4245966},
	keywords = {Gene Expression Profiling, Animals, Algorithms, Software, Genomics, High-Throughput Nucleotide Sequencing, Artifacts, Zebrafish},
	file = {Full Text:/mnt/data/Google Drive/Zotero/storage/MTE8EDKU/Leek - 2014 - svaseq removing batch effects and other unwanted .pdf:application/pdf}
}

@book{jamesIntroductionStatisticalLearning2013,
	address = {New York},
	series = {Springer texts in statistics},
	title = {An introduction to statistical learning: with applications in {R}},
	isbn = {978-1-4614-7137-0},
	shorttitle = {An introduction to statistical learning},
	language = {en},
	number = {103},
	publisher = {Springer},
	editor = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
	year = {2013},
	note = {OCLC: ocn828488009},
	keywords = {Mathematical models, Mathematical statistics, Problems, exercises, etc, R (Computer program language), Statistics},
	file = {James et al. - 2013 - An introduction to statistical learning with appl.pdf:/mnt/data/Google Drive/Zotero/storage/4KUB43ZS/James et al. - 2013 - An introduction to statistical learning with appl.pdf:application/pdf}
}

@article{nauNotesNonseasonalARIMA,
	title = {Notes on nonseasonal {ARIMA} models},
	language = {en},
	author = {Nau, Robert},
	pages = {21},
	file = {Nau - Notes on nonseasonal ARIMA models.pdf:/mnt/data/Google Drive/Zotero/storage/WVBL3IIT/Nau - Notes on nonseasonal ARIMA models.pdf:application/pdf}
}

@article{nauFittingNonseasonalARIMA,
	title = {Fitting nonseasonal {ARIMA} models},
	language = {en},
	author = {Nau, Robert},
	pages = {21},
	file = {Nau - Notes on nonseasonal ARIMA models.pdf:/mnt/data/Google Drive/Zotero/storage/4DN5KZGH/Nau - Notes on nonseasonal ARIMA models.pdf:application/pdf}
}

@techreport{engleAssetPricingFactor1988,
	address = {Cambridge, MA},
	title = {Asset {Pricing} with a {Factor} {Arch} {Covariance} {Structure}: {Empirical} {Estimates} for {Treasury} {Bills}},
	shorttitle = {Asset {Pricing} with a {Factor} {Arch} {Covariance} {Structure}},
	url = {http://www.nber.org/papers/t0065.pdf},
	language = {en},
	number = {t0065},
	urldate = {2019-06-19},
	institution = {National Bureau of Economic Research},
	author = {Engle, Robert and Ng, Victor and Rothschild, Michael},
	month = nov,
	year = {1988},
	doi = {10.3386/t0065},
	pages = {t0065},
	file = {Engle et al. - 1988 - Asset Pricing with a Factor Arch Covariance Struct.pdf:/mnt/data/Google Drive/Zotero/storage/PC5TWX6C/Engle et al. - 1988 - Asset Pricing with a Factor Arch Covariance Struct.pdf:application/pdf}
}

@article{susmelHourlyVolatilitySpillovers1994,
	title = {Hourly volatility spillovers between international equity markets},
	volume = {13},
	issn = {0261-5606},
	url = {http://www.sciencedirect.com/science/article/pii/0261560694900213},
	doi = {10.1016/0261-5606(94)90021-3},
	abstract = {This paper examines the timing of mean and volatility spillovers between New York and London equity markets. Using an ARCH model it is found that the evidence of volatility spillovers between these markets is minimal and have a duration which lasts only an hour or so. The most significant effects surround the movement of share prices around the New York opening, but these results are not strong. Several new ARCH models are estimated including an asymmetric or ‘leverage’ model and a non-linear model which allows big shocks to have a different impact from small shocks.},
	number = {1},
	urldate = {2019-06-19},
	journal = {Journal of International Money and Finance},
	author = {Susmel, Raul and Engle, Robert F.},
	month = feb,
	year = {1994},
	pages = {3--25},
	file = {ScienceDirect Snapshot:/mnt/data/Google Drive/Zotero/storage/TTLMA9SS/0261560694900213.html:text/html}
}

@misc{DoesVolatilitySpillover,
	title = {Does volatility spillover among stock markets varies from normal to turbulent periods? {Evidence} from emerging markets of {Asia} {\textbar} {Elsevier} {Enhanced} {Reader}},
	shorttitle = {Does volatility spillover among stock markets varies from normal to turbulent periods?},
	url = {https://reader.elsevier.com/reader/sd/pii/S2405918816300460?token=77145D57F9055ED9A36C71F8C94856863CFB047BE05FBAA3C23B5B16A0A5BD7036680A0E48325643BD7AF835C0631943},
	language = {en},
	urldate = {2019-06-19},
	doi = {10.1016/j.jfds.2017.06.001},
	file = {Snapshot:/mnt/data/Google Drive/Zotero/storage/ZXVTA2XL/S2405918816300460.html:text/html}
}

@misc{ExaminingMeanvolatilitySpillovers,
	title = {Examining mean-volatility spillovers across national stock markets {\textbar} {Elsevier} {Enhanced} {Reader}},
	url = {https://reader.elsevier.com/reader/sd/pii/S207718861400002X?token=6DA00691A761E41EB35C21F376C3C3281321CD1F69F90989FCA29E289EFBB8FE37B90212E853B2C264E39B775C329237},
	language = {en},
	urldate = {2019-06-19},
	doi = {10.1016/j.jefas.2014.01.001},
	file = {Full Text:/mnt/data/Google Drive/Zotero/storage/GC4N5CAA/Examining mean-volatility spillovers across nation.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/PYEJW9LM/S207718861400002X.html:text/html}
}

@book{mcelreathStatisticalRethinkingBayesian2018a,
	edition = {1},
	title = {Statistical {Rethinking}: {A} {Bayesian} {Course} with {Examples} in {R} and {Stan}},
	isbn = {978-1-315-37249-5},
	shorttitle = {Statistical {Rethinking}},
	url = {https://www.taylorfrancis.com/books/9781315362618},
	language = {en},
	urldate = {2019-06-21},
	publisher = {Chapman and Hall/CRC},
	author = {McElreath, Richard},
	month = jan,
	year = {2018},
	doi = {10.1201/9781315372495},
	file = {McElreath - 2018 - Statistical Rethinking A Bayesian Course with Exa.pdf:/mnt/data/Google Drive/Zotero/storage/L4IC4K2A/McElreath - 2018 - Statistical Rethinking A Bayesian Course with Exa.pdf:application/pdf}
}

@article{betancourtConceptualIntroductionHamiltonian2017,
	title = {A {Conceptual} {Introduction} to {Hamiltonian} {Monte} {Carlo}},
	url = {http://arxiv.org/abs/1701.02434},
	abstract = {Hamiltonian Monte Carlo has proven a remarkable empirical success, but only recently have we begun to develop a rigorous understanding of why it performs so well on difficult problems and how it is best applied in practice. Unfortunately, that understanding is confined within the mathematics of differential geometry which has limited its dissemination, especially to the applied communities for which it is particularly important. In this review I provide a comprehensive conceptual account of these theoretical foundations, focusing on developing a principled intuition behind the method and its optimal implementations rather of any exhaustive rigor. Whether a practitioner or a statistician, the dedicated reader will acquire a solid grasp of how Hamiltonian Monte Carlo works, when it succeeds, and, perhaps most importantly, when it fails.},
	urldate = {2019-07-01},
	journal = {arXiv:1701.02434 [stat]},
	author = {Betancourt, Michael},
	month = jan,
	year = {2017},
	note = {arXiv: 1701.02434},
	keywords = {Statistics - Methodology},
	annote = {Comment: 60 pages, 42 figures},
	file = {arXiv\:1701.02434 PDF:/mnt/data/Google Drive/Zotero/storage/UMKJ7NJ7/Betancourt - 2017 - A Conceptual Introduction to Hamiltonian Monte Car.pdf:application/pdf;arXiv.org Snapshot:/mnt/data/Google Drive/Zotero/storage/UJKZW85Y/1701.html:text/html}
}

@article{shiComprehensiveTimeSeriesRegression2014,
	title = {Comprehensive {Time}-{Series} {Regression} {Models} {Using} {Gretl} {U}.{S}. {GDP} and {Government} {Consumption} {Expenditures} \& {Gross} {Investment} from 1980 to 2013},
	issn = {1556-5068},
	url = {http://www.ssrn.com/abstract=2540535},
	doi = {10.2139/ssrn.2540535},
	abstract = {Using Gretl, I apply ARMA, Vector ARMA, VAR, state-space model with a Kalman filter, transfer-function and intervention models, unit root tests, cointegration test, volatility models (ARCH, GARCH, ARCH-M, GARCH-M, Taylor-Schwert GARCH, GJR, TARCH, NARCH, APARCH, EGARCH) to analyze quarterly time series of GDP and Government Consumption Expenditures \& Gross Investment (GCEGI) from 1980 to 2013. The article is organized in three sections: (I) Definition; (II) Regression Models; (III) Discussion [Summary of Major Findings and Their Managerial Implications, Comparison of Empirical Results, Contributions to Literature, Limitations and Future Research, Gretl Scripts]. Additionally, I discovered a unique interaction between GDP and GCEGI in both the short-run and the long-run and provided policy makers with some suggestions. For example in the short run, GDP responded positively and very significantly (0.00248) to GCEGI, while GCEGI reacted positively but not too significantly (0.08051) to GDP. In the long run, current GDP responded negatively and permanently (0.09229) to a shock in past GCEGI, while current GCEGI reacted negatively yet temporarily (0.29821) to a shock in past GDP. Therefore, policy makers should not adjust current GCEGI based merely on the condition of current and past GDP. Although increasing GCEGI does help GDP in the short-term, significantly abrupt increase in GCEGI might not be good to the long-term health of GDP. Instead, a balanced, sustainable, and economically viable solution is recommended, so that the short-term benefits to the current economy from increasing GCEGI often largely secured by the long-term loan outweigh or at least equal to the negative effect to the future economy from the long-term debt incurred by the loan. Finally, I found that non-normally distributed volatility models generally perform better than normally distributed ones. More specifically, TARCH-GED performs the best in the group of non-normally distributed, while GARCH-M does the best in the group of normally distributed.},
	language = {en},
	urldate = {2019-07-10},
	journal = {SSRN Electronic Journal},
	author = {Shi, Juehui},
	year = {2014},
	file = {Shi - 2014 - Comprehensive Time-Series Regression Models Using .pdf:/mnt/data/Google Drive/Zotero/storage/U2FRLU2I/Shi - 2014 - Comprehensive Time-Series Regression Models Using .pdf:application/pdf}
}

@article{tibshiraniValeriePatrickHastie,
	title = {Valerie and {Patrick} {Hastie}},
	language = {en},
	author = {Tibshirani, Sami and Friedman, Harry},
	pages = {764},
	file = {Tibshirani and Friedman - Valerie and Patrick Hastie.pdf:/mnt/data/Google Drive/Zotero/storage/UZCINHYZ/Tibshirani and Friedman - Valerie and Patrick Hastie.pdf:application/pdf}
}

@book{shumwayTimeSeriesAnalysis2011,
	address = {New York},
	edition = {3rd ed},
	series = {Springer texts in statistics},
	title = {Time series analysis and its applications: with {R} examples},
	isbn = {978-1-4419-7864-6},
	shorttitle = {Time series analysis and its applications},
	language = {en},
	publisher = {Springer},
	author = {Shumway, Robert H. and Stoffer, David S.},
	year = {2011},
	keywords = {Time-series analysis},
	file = {Shumway and Stoffer - 2011 - Time series analysis and its applications with R .pdf:/mnt/data/Google Drive/Zotero/storage/9XNWKECM/Shumway and Stoffer - 2011 - Time series analysis and its applications with R .pdf:application/pdf;tsa4.pdf:/mnt/data/Google Drive/Zotero/storage/LE5GIQU9/tsa4.pdf:application/pdf}
}

@article{tibshiraniElementsStatisticalLearning,
	title = {Elements of {Statistical} {Learning}},
	language = {en},
	author = {Tibshirani, Sami and Friedman, Harry},
	pages = {764},
	file = {Tibshirani and Friedman - Valerie and Patrick Hastie.pdf:/mnt/data/Google Drive/Zotero/storage/XDEHIRQM/Tibshirani and Friedman - Valerie and Patrick Hastie.pdf:application/pdf}
}

@book{jamesIntroductionStatisticalLearning2013a,
	address = {New York},
	series = {Springer texts in statistics},
	title = {An introduction to statistical learning: with applications in {R}},
	isbn = {978-1-4614-7137-0},
	shorttitle = {An introduction to statistical learning},
	language = {en},
	number = {103},
	publisher = {Springer},
	editor = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
	year = {2013},
	note = {OCLC: ocn828488009},
	keywords = {Mathematical models, Mathematical statistics, Problems, exercises, etc, R (Computer program language), Statistics},
	file = {James et al. - 2013 - An introduction to statistical learning with appl.pdf:/mnt/data/Google Drive/Zotero/storage/ZYJAHZCY/James et al. - 2013 - An introduction to statistical learning with appl.pdf:application/pdf}
}

@book{fahrmeirRegressionModelsMethods2013,
	address = {New York},
	title = {Regression: models, methods and applications},
	isbn = {978-3-642-34332-2},
	shorttitle = {Regression},
	language = {en},
	publisher = {Springer},
	author = {Fahrmeir, Ludwig and {Kneib, Thomas}},
	year = {2013},
	file = {Fahrmeir - 2013 - Regression models, methods and applications.pdf:/mnt/data/Google Drive/Zotero/storage/UDRKAU26/Fahrmeir - 2013 - Regression models, methods and applications.pdf:application/pdf}
}

@article{chenGMPRRobustNormalization2018,
	title = {{GMPR}: {A} robust normalization method for zero-inflated count data with application to microbiome sequencing data},
	volume = {6},
	issn = {2167-8359},
	shorttitle = {{GMPR}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5885979/},
	doi = {10.7717/peerj.4600},
	abstract = {Normalization is the first critical step in microbiome sequencing data analysis used to account for variable library sizes. Current RNA-Seq based normalization methods that have been adapted for microbiome data fail to consider the unique characteristics of microbiome data, which contain a vast number of zeros due to the physical absence or under-sampling of the microbes. Normalization methods that specifically address the zero-inflation remain largely undeveloped. Here we propose geometric mean of pairwise ratios—a simple but effective normalization method—for zero-inflated sequencing data such as microbiome data. Simulation studies and real datasets analyses demonstrate that the proposed method is more robust than competing methods, leading to more powerful detection of differentially abundant taxa and higher reproducibility of the relative abundances of taxa.},
	urldate = {2019-08-14},
	journal = {PeerJ},
	author = {Chen, Li and Reeve, James and Zhang, Lujun and Huang, Shengbing and Wang, Xuefeng and Chen, Jun},
	month = apr,
	year = {2018},
	pmid = {29629248},
	pmcid = {PMC5885979},
	file = {PubMed Central Full Text PDF:/mnt/data/Google Drive/Zotero/storage/ZEF84UIL/Chen et al. - 2018 - GMPR A robust normalization method for zero-infla.pdf:application/pdf}
}

@misc{KarrierenBeiDeloitte,
	title = {Karrieren bei {Deloitte}},
	url = {https://jobs.deloitte.de/search/?q=&optionsFacetsDD_facility=&optionsFacetsDD_shifttype==},
	urldate = {2019-08-15},
	file = {Karrieren bei Deloitte:/mnt/data/Google Drive/Zotero/storage/U364IZ5W/search.html:text/html}
}

@article{bondtDoesStockMarket1985,
	title = {Does the {Stock} {Market} {Overreact}?},
	volume = {40},
	copyright = {© 1985 the American Finance Association},
	issn = {1540-6261},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1540-6261.1985.tb05004.x},
	doi = {10.1111/j.1540-6261.1985.tb05004.x},
	abstract = {Research in experimental psychology suggests that, in violation of Bayes' rule, most people tend to “overreact” to unexpected and dramatic news events. This study of market efficiency investigates whether such behavior affects stock prices. The empirical evidence, based on CRSP monthly return data, is consistent with the overreaction hypothesis. Substantial weak form market inefficiencies are discovered. The results also shed new light on the January returns earned by prior “winners” and “losers.” Portfolios of losers experience exceptionally large January returns as late as five years after portfolio formation.},
	language = {en},
	number = {3},
	urldate = {2019-08-26},
	journal = {The Journal of Finance},
	author = {Bondt, WERNER F. M. De and Thaler, Richard},
	year = {1985},
	pages = {793--805},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/P4WU9PUE/Bondt and Thaler - 1985 - Does the Stock Market Overreact.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/37TBKMKU/j.1540-6261.1985.tb05004.html:text/html}
}

@article{balversMomentumMeanReversion2006,
	title = {Momentum and mean reversion across national equity markets},
	volume = {13},
	issn = {0927-5398},
	url = {http://www.sciencedirect.com/science/article/pii/S0927539805000708},
	doi = {10.1016/j.jempfin.2005.05.001},
	abstract = {Numerous studies have separately identified mean reversion and momentum. This paper considers these effects jointly. Our empirical model assumes that only global equity price index shocks can have permanent components. This is motivated in a production-based asset pricing context, given that production levels converge across developed countries. Combination momentum-contrarian strategies, used to select from among 18 developed equity markets at a monthly frequency, outperform both pure momentum and pure contrarian strategies. The results continue to hold after corrections for factor sensitivities and transaction costs. They reveal the importance of controlling for mean reversion in exploiting momentum and vice versa.},
	number = {1},
	urldate = {2019-08-26},
	journal = {Journal of Empirical Finance},
	author = {Balvers, Ronald J. and Wu, Yangru},
	month = jan,
	year = {2006},
	keywords = {International asset pricing, Investment strategies, Mean Reversion, Momentum},
	pages = {24--48},
	file = {ScienceDirect Full Text PDF:/mnt/data/Google Drive/Zotero/storage/LH3XYYQK/Balvers and Wu - 2006 - Momentum and mean reversion across national equity.pdf:application/pdf;ScienceDirect Snapshot:/mnt/data/Google Drive/Zotero/storage/VVDQF252/S0927539805000708.html:text/html}
}

@article{jegadeeshReturnsBuyingWinners1993,
	title = {Returns to {Buying} {Winners} and {Selling} {Losers}: {Implications} for {Stock} {Market} {Efficiency}},
	volume = {48},
	issn = {00221082},
	shorttitle = {Returns to {Buying} {Winners} and {Selling} {Losers}},
	url = {http://doi.wiley.com/10.1111/j.1540-6261.1993.tb04702.x},
	doi = {10.1111/j.1540-6261.1993.tb04702.x},
	abstract = {This paper documentsthat strategies which buy stocks that have performedwell in the past and sell stocks that have performedpoorlyin the past generate significant positive returns over 3- to 12-month holding periods. We find that the profitability of these strategies are not due to their systematic risk or to delayed stock price reactions to common factors. However, part of the abnormal returns generated in the first year after portfolio formation dissipates in the following two years. A similar pattern of returns around the earnings announcements of past winners and losers is also documented.},
	language = {en},
	number = {1},
	urldate = {2019-08-26},
	journal = {The Journal of Finance},
	author = {Jegadeesh, Narasimhan and Titman, Sheridan},
	month = mar,
	year = {1993},
	pages = {65--91},
	file = {Jegadeesh and Titman - 1993 - Returns to Buying Winners and Selling Losers Impl.pdf:/mnt/data/Google Drive/Zotero/storage/TX45FB84/Jegadeesh and Titman - 1993 - Returns to Buying Winners and Selling Losers Impl.pdf:application/pdf}
}

@article{famaCrossSectionExpectedStock1992,
	title = {The {Cross}-{Section} of {Expected} {Stock} {Returns}},
	volume = {47},
	copyright = {© 1992 the American Finance Association},
	issn = {1540-6261},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1540-6261.1992.tb04398.x},
	doi = {10.1111/j.1540-6261.1992.tb04398.x},
	abstract = {Two easily measured variables, size and book-to-market equity, combine to capture the cross-sectional variation in average stock returns associated with market β, size, leverage, book-to-market equity, and earnings-price ratios. Moreover, when the tests allow for variation in β that is unrelated to size, the relation between market β and average return is flat, even when β is the only explanatory variable.},
	language = {en},
	number = {2},
	urldate = {2019-08-26},
	journal = {The Journal of Finance},
	author = {Fama, Eugene F. and French, Kenneth R.},
	year = {1992},
	pages = {427--465},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/B3MVEEPL/Fama and French - 1992 - The Cross-Section of Expected Stock Returns.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/46WM9BFL/j.1540-6261.1992.tb04398.html:text/html}
}

@article{serbanCombiningMeanReversion2010,
	title = {Combining mean reversion and momentum trading strategies in foreign exchange markets},
	volume = {34},
	issn = {0378-4266},
	url = {http://www.sciencedirect.com/science/article/pii/S0378426610001883},
	doi = {10.1016/j.jbankfin.2010.05.011},
	abstract = {The literature on equity markets documents the existence of mean reversion and momentum phenomena. Researchers in foreign exchange markets find that foreign exchange rates also display behaviors akin to momentum and mean reversion. This paper implements a trading strategy combining mean reversion and momentum in foreign exchange markets. The strategy was originally designed for equity markets, but it also generates abnormal returns when applied to uncovered interest parity deviations for five countries. I find that the pattern for the positions thus created in the foreign exchange markets is qualitatively similar to that found in the equity markets. Quantitatively, this strategy performs better in foreign exchange markets than in equity markets. Also, it outperforms traditional foreign exchange trading strategies, such as carry trades and moving average rules.},
	number = {11},
	urldate = {2019-08-26},
	journal = {Journal of Banking \& Finance},
	author = {Serban, Alina F.},
	month = nov,
	year = {2010},
	keywords = {Momentum, Foreign exchange, Mean reversion, Trading strategies, Uncovered interest parity},
	pages = {2720--2727},
	file = {ScienceDirect Snapshot:/mnt/data/Google Drive/Zotero/storage/52H6S9H7/S0378426610001883.html:text/html}
}

@article{serbanCombiningMeanReversion2010a,
	title = {Combining mean reversion and momentum trading strategies in foreign exchange markets},
	volume = {34},
	issn = {0378-4266},
	url = {http://www.sciencedirect.com/science/article/pii/S0378426610001883},
	doi = {10.1016/j.jbankfin.2010.05.011},
	abstract = {The literature on equity markets documents the existence of mean reversion and momentum phenomena. Researchers in foreign exchange markets find that foreign exchange rates also display behaviors akin to momentum and mean reversion. This paper implements a trading strategy combining mean reversion and momentum in foreign exchange markets. The strategy was originally designed for equity markets, but it also generates abnormal returns when applied to uncovered interest parity deviations for five countries. I find that the pattern for the positions thus created in the foreign exchange markets is qualitatively similar to that found in the equity markets. Quantitatively, this strategy performs better in foreign exchange markets than in equity markets. Also, it outperforms traditional foreign exchange trading strategies, such as carry trades and moving average rules.},
	number = {11},
	urldate = {2019-08-26},
	journal = {Journal of Banking \& Finance},
	author = {Serban, Alina F.},
	month = nov,
	year = {2010},
	keywords = {Momentum, Foreign exchange, Mean reversion, Trading strategies, Uncovered interest parity},
	pages = {2720--2727},
	file = {ScienceDirect Full Text PDF:/mnt/data/Google Drive/Zotero/storage/F3DV92YS/Serban - 2010 - Combining mean reversion and momentum trading stra.pdf:application/pdf;ScienceDirect Snapshot:/mnt/data/Google Drive/Zotero/storage/QVJ5UTZH/S0378426610001883.html:text/html}
}

@article{herwartzStockReturnPrediction2017,
	title = {Stock return prediction under {GARCH} — {An} empirical assessment},
	volume = {33},
	issn = {01692070},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0169207017300079},
	doi = {10.1016/j.ijforecast.2017.01.002},
	abstract = {The GARCH model and its numerous variants have been widely applied in the ﬁnancial literature and practice. For purposes of quasi maximum likelihood estimation, innovations to GARCH processes are typically assumed to be identically and independently distributed with mean zero and unit variance (strong GARCH). Under less restrictive assumptions (absence of unconditional correlation, weak GARCH), higher order dependence patterns might be exploited for ex-ante forecasting of GARCH innovations and, hence, stock returns. In this paper, rolling windows of empirical stock returns are subjected to testing independence of consecutive GARCH innovations. Rolling p-values from independence testing reﬂect time variation of serial dependence, and entail useful information to signal one step ahead directions of stock price changes. Ex-ante forecasting gains are documented for non-parametric innovation predictions, in particular, if the sign of innovation predictors is combined with independence diagnostics (p-values) and/or the sign of linear return forecasts.},
	language = {en},
	number = {3},
	urldate = {2019-08-27},
	journal = {International Journal of Forecasting},
	author = {Herwartz, Helmut},
	month = jul,
	year = {2017},
	pages = {569--580},
	file = {Herwartz - 2017 - Stock return prediction under GARCH — An empirical.pdf:/mnt/data/Google Drive/Zotero/storage/UBBW7DJK/Herwartz - 2017 - Stock return prediction under GARCH — An empirical.pdf:application/pdf}
}

@book{shumwayTimeSeriesAnalysis2011a,
	address = {New York},
	edition = {3rd ed},
	series = {Springer texts in statistics},
	title = {Time series analysis and its applications: with {R} examples},
	isbn = {978-1-4419-7864-6},
	shorttitle = {Time series analysis and its applications},
	language = {en},
	publisher = {Springer},
	author = {Shumway, Robert H. and Stoffer, David S.},
	year = {2011},
	keywords = {Time-series analysis}
}

@article{saidTestingUnitRoots1984,
	title = {Testing for unit roots in autoregressive-moving average models of unknown order},
	volume = {71},
	issn = {0006-3444},
	url = {https://academic.oup.com/biomet/article/71/3/599/258758},
	doi = {10.1093/biomet/71.3.599},
	abstract = {AbstractSUMMARY.  Recently, methods for detecting unit roots in autoregressive and autoregressive-moving average time series have been proposed. The presence of},
	language = {en},
	number = {3},
	urldate = {2019-09-12},
	journal = {Biometrika},
	author = {Said, Said E. and Dickey, David A.},
	month = dec,
	year = {1984},
	pages = {599--607},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/BC4ZW997/Said and Dickey - 1984 - Testing for unit roots in autoregressive-moving av.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/IN2NT2WC/258758.html:text/html}
}

@article{engleAutoregressiveConditionalHeteroscedasticity1982,
	title = {Autoregressive {Conditional} {Heteroscedasticity} with {Estimates} of the {Variance} of {United} {Kingdom} {Inflation}},
	volume = {50},
	issn = {0012-9682},
	url = {https://www.jstor.org/stable/1912773},
	doi = {10.2307/1912773},
	abstract = {Traditional econometric models assume a constant one-period forecast variance. To generalize this implausible assumption, a new class of stochastic processes called autoregressive conditional heteroscedastic (ARCH) processes are introduced in this paper. These are mean zero, serially uncorrelated processes with nonconstant variances conditional on the past, but constant unconditional variances. For such processes, the recent past gives information about the one-period forecast variance. A regression model is then introduced with disturbances following an ARCH process. Maximum likelihood estimators are described and a simple scoring iteration formulated. Ordinary least squares maintains its optimality properties in this set-up, but maximum likelihood is more efficient. The relative efficiency is calculated and can be infinite. To test whether the disturbances follow an ARCH process, the Lagrange multiplier procedure is employed. The test is based simply on the autocorrelation of the squared OLS residuals. This model is used to estimate the means and variances of inflation in the U.K. The ARCH effect is found to be significant and the estimated variances increase substantially during the chaotic seventies.},
	number = {4},
	urldate = {2019-09-12},
	journal = {Econometrica},
	author = {Engle, Robert F.},
	year = {1982},
	pages = {987--1007}
}

@article{engleAutoregressiveConditionalHeteroscedasticity1982a,
	title = {Autoregressive {Conditional} {Heteroscedasticity} with {Estimates} of the {Variance} of {United} {Kingdom} {Inflation}},
	volume = {50},
	issn = {00129682},
	url = {https://www.jstor.org/stable/1912773?origin=crossref},
	doi = {10.2307/1912773},
	language = {en},
	number = {4},
	urldate = {2019-09-12},
	journal = {Econometrica},
	author = {Engle, Robert F.},
	month = jul,
	year = {1982},
	pages = {987},
	file = {Engle - 1982 - Autoregressive Conditional Heteroscedasticity with.pdf:/mnt/data/Google Drive/Zotero/storage/MHIXEV2L/Engle - 1982 - Autoregressive Conditional Heteroscedasticity with.pdf:application/pdf}
}

@article{bollerslevGeneralizedAutoregressiveConditional1986,
	title = {Generalized autoregressive conditional heteroskedasticity},
	volume = {31},
	issn = {0304-4076},
	url = {http://www.sciencedirect.com/science/article/pii/0304407686900631},
	doi = {10.1016/0304-4076(86)90063-1},
	abstract = {A natural generalization of the ARCH (Autoregressive Conditional Heteroskedastic) process introduced in Engle (1982) to allow for past conditional variances in the current conditional variance equation is proposed. Stationarity conditions and autocorrelation structure for this new class of parametric models are derived. Maximum likelihood estimation and testing are also considered. Finally an empirical example relating to the uncertainty of the inflation rate is presented.},
	number = {3},
	urldate = {2019-09-12},
	journal = {Journal of Econometrics},
	author = {Bollerslev, Tim},
	month = apr,
	year = {1986},
	pages = {307--327},
	file = {ScienceDirect Full Text PDF:/mnt/data/Google Drive/Zotero/storage/PKUCUB9C/Bollerslev - 1986 - Generalized autoregressive conditional heteroskeda.pdf:application/pdf;ScienceDirect Snapshot:/mnt/data/Google Drive/Zotero/storage/HQAPZT2C/0304407686900631.html:text/html}
}

@article{glostenRelationExpectedValue1993,
	title = {On the {Relation} between the {Expected} {Value} and the {Volatility} of the {Nominal} {Excess} {Return} on {Stocks}},
	volume = {48},
	issn = {00221082},
	url = {http://doi.wiley.com/10.1111/j.1540-6261.1993.tb05128.x},
	doi = {10.1111/j.1540-6261.1993.tb05128.x},
	abstract = {We find support for a negative relation between conditional expected monthly return and conditional variance of monthly return, using a GARCH-Mmodel modified by allowing (1) seasonal patterns in volatility, (2) positive and negative innovations to returns having different impacts on conditional volatility, and (3) nominalinterest rates to predictconditionalvariance.Using the modifiedGARCH-M model, we also show that monthly conditionalvolatility may not be as persistent as was thought. Positive unanticipated returns appear to result in a downward revision of the conditional volatility whereas negative unanticipated returns result in an upward revision of conditional volatility.},
	language = {en},
	number = {5},
	urldate = {2019-09-12},
	journal = {The Journal of Finance},
	author = {Glosten, Lawrence R. and Jagannathan, Ravi and Runkle, David E.},
	month = dec,
	year = {1993},
	pages = {1779--1801},
	file = {Glosten et al. - 1993 - On the Relation between the Expected Value and the.pdf:/mnt/data/Google Drive/Zotero/storage/8WA5HJU7/Glosten et al. - 1993 - On the Relation between the Expected Value and the.pdf:application/pdf}
}

@article{ljungMeasureLackFit1978,
	title = {On a measure of lack of fit in time series models},
	volume = {65},
	issn = {0006-3444},
	url = {https://academic.oup.com/biomet/article/65/2/297/236869},
	doi = {10.1093/biomet/65.2.297},
	abstract = {Abstract.  The overall test for lack of fit in autoregressive-moving average models proposed by Box \&amp; Pierce (1970) is considered. It is shown that a substa},
	language = {en},
	number = {2},
	urldate = {2019-09-13},
	journal = {Biometrika},
	author = {Ljung, G. M. and Box, G. E. P.},
	month = aug,
	year = {1978},
	pages = {297--303},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/BJQA25WH/Ljung and Box - 1978 - On a measure of lack of fit in time series models.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/Z7P63DIX/236869.html:text/html}
}

@misc{StatsModelsStatisticsPython,
	title = {{StatsModels}: {Statistics} in {Python} — statsmodels v0.10.1 documentation},
	url = {https://www.statsmodels.org/stable/index.html},
	urldate = {2019-09-13},
	file = {StatsModels\: Statistics in Python — statsmodels v0.10.1 documentation:/mnt/data/Google Drive/Zotero/storage/5FQPJPM2/index.html:text/html}
}

@article{leeLagrangeMultiplierTest1991,
	title = {A {Lagrange} multiplier test for {GARCH} models},
	volume = {37},
	issn = {0165-1765},
	url = {http://www.sciencedirect.com/science/article/pii/0165176591902216},
	doi = {10.1016/0165-1765(91)90221-6},
	abstract = {This paper extends the Lagrange multiplier (LM) test to testing noise disturbances against GARCH disturbances in the linear regression model. The resulting LM test for the GARCH alternative is identical to the LM test for an ARCH alternative},
	number = {3},
	urldate = {2019-09-13},
	journal = {Economics Letters},
	author = {Lee, John H. H.},
	month = nov,
	year = {1991},
	pages = {265--271},
	file = {ScienceDirect Snapshot:/mnt/data/Google Drive/Zotero/storage/RB6X4696/0165176591902216.html:text/html}
}

@article{leeLagrangeMultiplierTest1991a,
	title = {A {Lagrange} multiplier test for {GARCH} models},
	volume = {37},
	issn = {0165-1765},
	url = {http://www.sciencedirect.com/science/article/pii/0165176591902216},
	doi = {10.1016/0165-1765(91)90221-6},
	abstract = {This paper extends the Lagrange multiplier (LM) test to testing noise disturbances against GARCH disturbances in the linear regression model. The resulting LM test for the GARCH alternative is identical to the LM test for an ARCH alternative},
	number = {3},
	urldate = {2019-09-13},
	journal = {Economics Letters},
	author = {Lee, John H. H.},
	month = nov,
	year = {1991},
	pages = {265--271},
	file = {ScienceDirect Full Text PDF:/mnt/data/Google Drive/Zotero/storage/HDT3EYZK/Lee - 1991 - A Lagrange multiplier test for GARCH models.pdf:application/pdf;ScienceDirect Snapshot:/mnt/data/Google Drive/Zotero/storage/PUVV7FFW/0165176591902216.html:text/html}
}

@article{hansenForecastComparisonVolatility2005,
	title = {A forecast comparison of volatility models: does anything beat a {GARCH}(1,1)?},
	volume = {20},
	copyright = {Copyright © 2005 John Wiley \& Sons, Ltd.},
	issn = {1099-1255},
	shorttitle = {A forecast comparison of volatility models},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jae.800},
	doi = {10.1002/jae.800},
	abstract = {We compare 330 ARCH-type models in terms of their ability to describe the conditional variance. The models are compared out-of-sample using DM–\$ exchange rate data and IBM return data, where the latter is based on a new data set of realized variance. We find no evidence that a GARCH(1,1) is outperformed by more sophisticated models in our analysis of exchange rates, whereas the GARCH(1,1) is clearly inferior to models that can accommodate a leverage effect in our analysis of IBM returns. The models are compared with the test for superior predictive ability (SPA) and the reality check for data snooping (RC). Our empirical results show that the RC lacks power to an extent that makes it unable to distinguish ‘good’ and ‘bad’ models in our analysis. Copyright © 2005 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {7},
	urldate = {2019-09-13},
	journal = {Journal of Applied Econometrics},
	author = {Hansen, Peter R. and Lunde, Asger},
	year = {2005},
	pages = {873--889},
	file = {Full Text:/mnt/data/Google Drive/Zotero/storage/K7BM4QPB/Hansen and Lunde - 2005 - A forecast comparison of volatility models does a.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/6UVZC85E/jae.html:text/html}
}

@misc{alexiosDoesAnythingNOT2013,
	title = {Does anything {NOT} beat the {GARCH}(1,1)?},
	url = {http://www.unstarched.net/2013/01/07/does-anything-not-beat-the-garch11/},
	abstract = {In their paper on GARCH model comparison, Hansen and Lunde (2005) present evidence that among 330 different models, and using daily data on the DM/\$ rate and IBM stock returns, no model does signif…},
	language = {en-US},
	urldate = {2019-09-13},
	journal = {unstarched},
	author = {{alexios}},
	month = jan,
	year = {2013},
	file = {Snapshot:/mnt/data/Google Drive/Zotero/storage/JBRPH7Y6/does-anything-not-beat-the-garch11.html:text/html}
}

@article{reschenhoferDoesAnyoneNeed2013,
	title = {Does {Anyone} {Need} a {GARCH}(1,1)?},
	volume = {1},
	copyright = {© 2014 Science and Education Publishing},
	url = {http://pubs.sciepub.com/jfa/1/2/2/abstract.html},
	doi = {10.12691/jfa-1-2-2},
	abstract = {Hansen and Lunde [16] posed the question Does anything beat a GARCH(1,1)? and compared a large number of parametric volatility models in an extensive empirical study. They found that no other model provides significantly better forecasts than the GARCH(1,1) model. In contrast, this paper arrives at the conclusion that simple robust estimators such as weighted medians of past (squared) returns outperform the GARCH(1,1) model both in-sample as well as out-of-sample. This conclusion is based on theoretical arguments as well as on empirical evidence.},
	language = {en},
	number = {2},
	urldate = {2019-09-13},
	journal = {Journal of Finance and Accounting},
	author = {Reschenhofer, Erhard},
	month = jan,
	year = {2013},
	pages = {48--53},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/XFEPCCHP/Reschenhofer - 2013 - Does Anyone Need a GARCH(1,1).pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/ZG5Q5NGX/index.html:text/html}
}

@article{engleGARCH101Use2001,
	title = {{GARCH} 101: {The} {Use} of {ARCH}/{GARCH} {Models} in {Applied} {Econometrics}},
	volume = {15},
	issn = {0895-3309},
	shorttitle = {{GARCH} 101},
	url = {http://pubs.aeaweb.org/doi/10.1257/jep.15.4.157},
	doi = {10.1257/jep.15.4.157},
	language = {en},
	number = {4},
	urldate = {2019-09-13},
	journal = {Journal of Economic Perspectives},
	author = {Engle, Robert},
	month = nov,
	year = {2001},
	pages = {157--168},
	file = {Engle - 2001 - GARCH 101 The Use of ARCHGARCH Models in Applied.pdf:/mnt/data/Google Drive/Zotero/storage/EZFXGULV/Engle - 2001 - GARCH 101 The Use of ARCHGARCH Models in Applied.pdf:application/pdf}
}

@article{famaCommonRiskFactors1993,
	title = {Common risk factors in the returns on stocks and bonds},
	volume = {33},
	issn = {0304-405X},
	url = {http://www.sciencedirect.com/science/article/pii/0304405X93900235},
	doi = {10.1016/0304-405X(93)90023-5},
	abstract = {This paper identifies five common risk factors in the returns on stocks and bonds. There are three stock-market factors: an overall market factor and factors related to firm size and book-to-market equity. There are two bond-market factors, related to maturity and default risks. Stock returns have shared variation due to the stock-market factors, and they are linked to bond returns through shared variation in the bond-market factors. Except for low-grade corporates, the bond-market factors capture the common variation in bond returns. Most important, the five factors seem to explain average returns on stocks and bonds.},
	number = {1},
	urldate = {2019-09-14},
	journal = {Journal of Financial Economics},
	author = {Fama, Eugene F. and French, Kenneth R.},
	month = feb,
	year = {1993},
	pages = {3--56},
	file = {ScienceDirect Full Text PDF:/mnt/data/Google Drive/Zotero/storage/ZZ9HLL7W/Fama and French - 1993 - Common risk factors in the returns on stocks and b.pdf:application/pdf;ScienceDirect Snapshot:/mnt/data/Google Drive/Zotero/storage/6233MJJ5/0304405X93900235.html:text/html}
}

@article{kanamuraApplicationPairsTrading,
	title = {The {Application} of {Pairs} {Trading} to {Energy} {Futures} {Markets}},
	abstract = {This paper investigates the usefulness of a hedge fund trading strategy known as “pairs trading” applied to energy futures markets. The proﬁt of a simpliﬁed pairs trading strategy is modeled by using a mean-reverting process of the futures price spread. According to the comparative statics of the model, the strong mean-reversion and high volatility of the spread give rise to the high expected return from trading. Analyzing energy futures (more speciﬁcally, WTI crude oil, heating oil, and natural gas futures) traded on the New York Mercantile Exchange, we present empirical evidence that pairs trading can produce a relatively stable proﬁt. We also identify the sources of the proﬁt from a pair trading strategy, focusing on the characteristics of energy. The results suggest that natural gas futures trading may be more proﬁtable than WTI crude oil and heating oil due to its strong mean-reversion and high volatility, while seasonality may also be relevant to the strategy’s proﬁtability. Moreover, natural gas futures trading may be more vulnerable to event risk (as measured by price spikes) than the others. Finally, we investigate the proﬁtability of cross commodities pairs trading.},
	language = {en},
	author = {Kanamura, Takashi and Rachev, Svetlozar T and Fabozzi, Frank J},
	pages = {39},
	file = {Kanamura et al. - The Application of Pairs Trading to Energy Futures.pdf:/mnt/data/Google Drive/Zotero/storage/L4HZPQQN/Kanamura et al. - The Application of Pairs Trading to Energy Futures.pdf:application/pdf}
}

@article{engleCoIntegrationErrorCorrection1987,
	title = {Co-{Integration} and {Error} {Correction}: {Representation}, {Estimation}, and {Testing}},
	volume = {55},
	issn = {0012-9682},
	shorttitle = {Co-{Integration} and {Error} {Correction}},
	url = {https://www.jstor.org/stable/1913236},
	doi = {10.2307/1913236},
	abstract = {The relationship between co-integration and error correction models, first suggested in Granger (1981), is here extended and used to develop estimation procedures, tests, and empirical examples. If each element of a vector of time series x$_{\textrm{t}}$ first achieves stationarity after differencing, but a linear combination {\textless}tex-math{\textgreater}\${\textbackslash}alpha {\textasciicircum}\{{\textbackslash}prime \}x\_\{t\}\${\textless}/tex-math{\textgreater} is already stationary, the time series x$_{\textrm{t}}$ are said to be co-integrated with co-integrating vector α. There may be several such co-integrating vectors so that α becomes a matrix. Interpreting {\textless}tex-math{\textgreater}\${\textbackslash}alpha {\textasciicircum}\{{\textbackslash}prime \}x\_\{t\}=0\${\textless}/tex-math{\textgreater} as a long run equilibrium, co-integration implies that deviations from equilibrium are stationary, with finite variance, even though the series themselves are nonstationary and have infinite variance. The paper presents a representation theorem based on Granger (1983), which connects the moving average, autoregressive, and error correction representations for co-integrated systems. A vector autoregression in differenced variables is incompatible with these representations. Estimation of these models is discussed and a simple but asymptotically efficient two-step estimator is proposed. Testing for co-integration combines the problems of unit root tests and tests with parameters unidentified under the null. Seven statistics are formulated and analyzed. The critical values of these statistics are calculated based on a Monte Carlo simulation. Using these critical values, the power properties of the tests are examined and one test procedure is recommended for application. In a series of examples it is found that consumption and income are co-integrated, wages and prices are not, short and long interest rates are, and nominal GNP is co-integrated with M2, but not M1, M3, or aggregate liquid assets.},
	number = {2},
	urldate = {2019-09-14},
	journal = {Econometrica},
	author = {Engle, Robert F. and Granger, C. W. J.},
	year = {1987},
	pages = {251--276}
}

@article{CoIntegrationErrorCorrection,
	title = {Co-{Integration} and {Error} {Correction}: {Representation}, {Estimation}, and {Testing}},
	language = {en},
	pages = {27},
	file = {Co-Integration and Error Correction Representatio.pdf:/mnt/data/Google Drive/Zotero/storage/TAINM6HK/Co-Integration and Error Correction Representatio.pdf:application/pdf}
}

@misc{ganapathyvidyamurthyPairsTradingQuantitative2004,
	title = {Pairs {Trading}: {Quantitative} {Methods} and {Analysis} {\textbar} {Finance} \& {Investments} {Special} {Topics} {\textbar} {General} {Finance} \& {Investments} {\textbar} {Subjects} {\textbar} {Wiley}},
	shorttitle = {Pairs {Trading}},
	url = {https://www.wiley.com/en-us/Pairs+Trading%3A+Quantitative+Methods+and+Analysis-p-9780471460671},
	abstract = {The first in-depth analysis of pairs trading Pairs trading is a market-neutral strategy in its most simple form. The strategy involves being long (or bullish) one asset and short (or bearish) another. If properly performed, the investor will gain if the market rises or falls. Pairs Trading reveals the secrets of this rigorous quantitative analysis program to provide individuals and investment houses with the tools they need to successfully implement and profit from this proven trading methodology. Pairs Trading contains specific and tested formulas for identifying and investing in pairs, and answers important questions such as what ratio should be used to construct the pairs properly. Ganapathy Vidyamurthy (Stamford, CT) is currently a quantitative software analyst and developer at a major New York City hedge fund.},
	language = {en-us},
	urldate = {2019-09-14},
	journal = {Wiley.com},
	author = {{Ganapathy Vidyamurthy}},
	year = {2004},
	file = {Ganapathy Vidyamurthy - 2004 - Pairs Trading Quantitative Methods and Analysis .pdf:/mnt/data/Google Drive/Zotero/storage/MEJKQP2I/Ganapathy Vidyamurthy - 2004 - Pairs Trading Quantitative Methods and Analysis .pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/GNX2X9ZK/Pairs+Trading+Quantitative+Methods+and+Analysis-p-9780471460671.html:text/html}
}

@article{doNewApproachModeling2006,
	title = {A {New} {Approach} to {Modeling} and {Estimation} for {Pairs} {Trading}},
	abstract = {Pairs trading is an speculative investment strategy based on relative mispricing between a pair of stocks. Essentially, the strategy involves choosing a pair of stocks that historically move together. By taking a long-short position on this pair when they diverge, a proﬁt will be made when they next converge to the mean by unwinding the position. Literature on this topic is rare due to its proprietary nature. Where it does exist, the strategies are either adhoc or applicable to special cases only, with little theoretical veriﬁcation. This paper analyzes these existing methods in detail and proposes a general approach to modeling relative mispricing for pairs trading purposes, with reference to the mainstream asset pricing theory. Several estimation techniques are discussed and tested for state space formulation, with Expectation Maximization producing stable results. Initial empirical evidence shows clear mean reversion behavior in selected pairs’ relative pricing.},
	language = {en},
	journal = {2006},
	author = {Do, Binh and Faﬀ, Robert and Hamza, Kais},
	year = {2006},
	pages = {31},
	file = {Do et al. - A New Approach to Modeling and Estimation for Pair.pdf:/mnt/data/Google Drive/Zotero/storage/KE55LP3V/Do et al. - A New Approach to Modeling and Estimation for Pair.pdf:application/pdf}
}

@article{radProfitabilityPairsTrading2016,
	title = {The profitability of pairs trading strategies: distance, cointegration and copula methods},
	volume = {16},
	issn = {1469-7688},
	shorttitle = {The profitability of pairs trading strategies},
	url = {https://doi.org/10.1080/14697688.2016.1164337},
	doi = {10.1080/14697688.2016.1164337},
	abstract = {We perform an extensive and robust study of the performance of three different pairs trading strategies—the distance, cointegration and copula methods—on the entire US equity market from 1962 to 2014 with time-varying trading costs. For the cointegration and copula methods, we design a computationally efficient two-step pairs trading strategy. In terms of economic outcomes, the distance, cointegration and copula methods show a mean monthly excess return of 91, 85 and 43 bps (38, 33 and 5 bps) before transaction costs (after transaction costs), respectively. In terms of continued profitability, from 2009, the frequency of trading opportunities via the distance and cointegration methods is reduced considerably, whereas this frequency remains stable for the copula method. Further, the copula method shows better performance for its unconverged trades compared to those of the other methods. While the liquidity factor is negatively correlated to all strategies’ returns, we find no evidence of their correlation to market excess returns. All strategies show positive and significant alphas after accounting for various risk-factors. We also find that in addition to all strategies performing better during periods of significant volatility, the cointegration method is the superior strategy during turbulent market conditions.},
	number = {10},
	urldate = {2019-09-14},
	journal = {Quantitative Finance},
	author = {Rad, Hossein and Low, Rand Kwong Yew and Faff, Robert},
	month = oct,
	year = {2016},
	keywords = {Cointegration, Copula, G11, G12, G14, Pairs trading, Quantitative strategies, Statistical arbitrage},
	pages = {1541--1558},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/WKCU8TDD/Rad et al. - 2016 - The profitability of pairs trading strategies dis.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/JRE6IUKF/14697688.2016.html:text/html}
}

@inproceedings{gatevPairsTradingPerformance1999,
	title = {Pairs {Trading}: {Performance} of a {Relative} {Value} {Arbitrage} {Rule}},
	shorttitle = {Pairs {Trading}},
	doi = {10.1093/rfs/hhj020},
	abstract = {We test a Wall Street investment strategy known as pairs trading' with daily data over the period 1962 through 1997. Stocks are matched into pairs according to minimum distance in historical normalized price space. We test the profitability of several trading rules with six-month trading periods over the 1962-1997 period, and find average annualized excess returns of up to 12 percent for a number of self-financing portfolios of top pairs. Part of these profits may be due to market microstructure effects. Nevertheless, our historical trading profits exceed a conservative estimate of transaction costs through most of the period. We bootstrap random pairs in order to distinguish pairs trading from pure mean-reversion strategies. The bootstrap results suggest that the pairs' effect differs from previously documented mean reversion profits.},
	author = {Gatev, Evan and Goetzmann, William N. and Rouwenhorst, K. Geert},
	year = {1999},
	keywords = {Estimated, Offset binary, Risk measure},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/Z48GWP4T/Gatev et al. - 1999 - Pairs Trading Performance of a Relative Value Arb.pdf:application/pdf}
}

@article{phillipsAsymptoticPropertiesResidual1990,
	title = {Asymptotic {Properties} of {Residual} {Based} {Tests} for {Cointegration}},
	volume = {58},
	issn = {0012-9682},
	url = {https://www.jstor.org/stable/2938339},
	doi = {10.2307/2938339},
	abstract = {This paper develops an asymptotic theory for residual based tests for cointegration. These tests involve procedures that are designed to detect the presence of a unit root in the residuals of (cointegrating) regressions among the levels of economic time series. Attention is given to the augmented Dickey-Fuller (ADF) test that is recommended by Engle-Granger (1987) and the Z$_{\textrm{α}}$ and Z$_{\textrm{t}}$ unit root tests recently proposed by Phillips (1987). Two new tests are also introduced, one of which is invariant to the normalization of the cointegrating regression. All of these tests are shown to be asymptotically similar and simple representations of their limiting distributions are given in terms of standard Brownian motion. The ADF and Z$_{\textrm{t}}$ tests are asymptotically equivalent. Power properties of the tests are also studied. The analysis shows that all the tests are consistent if suitably constructed but that the ADF and Z$_{\textrm{t}}$ tests have slower rates of divergence under cointegration than the other tests. This indicates that, at least in large samples, the Z$_{\textrm{α}}$ test should have superior power properties. The paper concludes by addressing the larger issue of test formulation. Some major pitfalls are discovered in procedures that are designed to test a null of cointegration (rather than no cointegration). These defects provide strong arguments against the indiscriminate use of such test formulations and support the continuing use of residual based unit root tests. A full set of critical values for residual based tests is included. These allow for demeaned and detrended data and cointegrating regressions with up to five variables.},
	number = {1},
	urldate = {2019-09-14},
	journal = {Econometrica},
	author = {Phillips, P. C. B. and Ouliaris, S.},
	year = {1990},
	pages = {165--193},
	file = {Submitted Version:/mnt/data/Google Drive/Zotero/storage/6AEVGRF9/Phillips and Ouliaris - 1990 - Asymptotic Properties of Residual Based Tests for .pdf:application/pdf}
}

@misc{BurtonMalkielRandom,
	title = {Burton {Malkiel}'s {A} {Random} {Walk} {Down} {Wall} {Street}},
	url = {https://www.crcpress.com/Burton-Malkiels-A-Random-Walk-Down-Wall-Street/Burton/p/book/9781912128822},
	abstract = {Burton Malkiel’s 1973 A Random Walk Down Wall Street was an explosive contribution to debates about how to reap a good return on investing in stocks and shares. Reissued and updated many times since, Malkiel’s text remains an indispensable contribution to the world of investment strategy – one},
	language = {en},
	urldate = {2019-09-14},
	journal = {CRC Press},
	file = {Snapshot:/mnt/data/Google Drive/Zotero/storage/PN3SUKFL/9781912128822.html:text/html}
}

@article{shmueliExplainPredict2010,
	title = {To {Explain} or to {Predict}?},
	volume = {25},
	issn = {0883-4237},
	url = {http://projecteuclid.org/euclid.ss/1294167961},
	doi = {10.1214/10-STS330},
	abstract = {Statistical modeling is a powerful tool for developing and testing theories by way of causal explanation, prediction, and description. In many disciplines there is near-exclusive use of statistical modeling for causal explanation and the assumption that models with high explanatory power are inherently of high predictive power. Conﬂation between explanation and prediction is common, yet the distinction must be understood for progressing scientiﬁc knowledge. While this distinction has been recognized in the philosophy of science, the statistical literature lacks a thorough discussion of the many differences that arise in the process of modeling for an explanatory versus a predictive goal. The purpose of this article is to clarify the distinction between explanatory and predictive modeling, to discuss its sources, and to reveal the practical implications of the distinction to each step in the modeling process.},
	language = {en},
	number = {3},
	urldate = {2019-09-16},
	journal = {Statistical Science},
	author = {Shmueli, Galit},
	month = aug,
	year = {2010},
	pages = {289--310},
	file = {Shmueli - 2010 - To Explain or to Predict.pdf:/mnt/data/Google Drive/Zotero/storage/5G26ZWRE/Shmueli - 2010 - To Explain or to Predict.pdf:application/pdf}
}

@article{namugayaModellingVolatilityStock2014,
	title = {Modelling {Volatility} of {Stock} {Returns}: {Is} {GARCH}(1,1) {Enough}?},
	volume = {16},
	abstract = {In this paper, we apply the Generalized Autoregressive Conditional Heteroscedasticity (GARCH) model of different lag order to model volatility of stock returns on Uganda Securities Exchange (USE). We use the Quasi Maximum Likelihood Estimation (QMLE) method to estimate the models. Akaike Information Criteria (AIC) and Bayesian Information Criteria (BIC) are used to select the best GARCH(p,q) model. From the empirical results, it has been found that USE returns are non-normal, positively skewed and stationary. Overall, GARCH(1,1) outperformed the other GARCH(p,q) models in modeling volatility of USE returns.},
	language = {en},
	number = {2},
	journal = {International Journal of Sciences},
	author = {Namugaya, Jalira and Weke, Patrick G O and Charles, W M},
	year = {2014},
	pages = {8},
	file = {Namugaya et al. - 2014 - Modelling Volatility of Stock Returns Is GARCH(1,.pdf:/mnt/data/Google Drive/Zotero/storage/BX25AUH9/Namugaya et al. - 2014 - Modelling Volatility of Stock Returns Is GARCH(1,.pdf:application/pdf}
}

@article{buttonPowerFailureWhy2013,
	title = {Power failure: why small sample size undermines the reliability of neuroscience},
	volume = {14},
	issn = {1471-003X, 1471-0048},
	shorttitle = {Power failure},
	url = {http://www.nature.com/articles/nrn3475},
	doi = {10.1038/nrn3475},
	abstract = {A study with low statistical power has a reduced chance of detecting a true effect, but it is less well appreciated that low power also reduces the likelihood that a statistically significant result reflects a true effect. Here, we show that the average statistical power of studies in the neurosciences is very low. The consequences of this include overestimates of effect size and low reproducibility of results. There are also ethical dimensions to this problem, as unreliable research is inefficient and wasteful. Improving reproducibility in neuroscience is a key priority and requires attention to well-established but often ignored methodological principles.},
	language = {en},
	number = {5},
	urldate = {2019-09-16},
	journal = {Nature Reviews Neuroscience},
	author = {Button, Katherine S. and Ioannidis, John P. A. and Mokrysz, Claire and Nosek, Brian A. and Flint, Jonathan and Robinson, Emma S. J. and Munafò, Marcus R.},
	month = may,
	year = {2013},
	pages = {365--376},
	file = {Button et al. - 2013 - Power failure why small sample size undermines th.pdf:/mnt/data/Google Drive/Zotero/storage/BPDAKYEF/Button et al. - 2013 - Power failure why small sample size undermines th.pdf:application/pdf}
}

@techreport{kneibPrimerBayesianDistributional2017,
	type = {Working {Paper}},
	title = {A primer on {Bayesian} distributional regression},
	copyright = {http://www.econstor.eu/dspace/Nutzungsbedingungen},
	url = {https://www.econstor.eu/handle/10419/180164},
	abstract = {Bayesian methods have become increasingly popular in the past two decades. With the constant rise of computational power even very complex models can be estimated on virtually any modern computer. Moreover, interest has shifted from conditional mean models to probabilistic distributional models capturing location, scale, shape and other aspects of a response distribution, where covariate effects can have flexible forms, e.g., linear, nonlinear, spatial or random effects. This tutorial paper discusses how to select models in the Bayesian distributional regression setting, how to monitor convergence of the Markov chains, evaluate relevance of effects using simultaneous credible intervals and how to use simulation-based inference also for quantities derived from the original model parameterisation. We exemplify the work flow using daily weather data on (i) temperatures on Germany's highest mountain and (ii) extreme values of precipitation all over Germany.},
	language = {eng},
	number = {2017-13},
	urldate = {2019-09-16},
	institution = {Working Papers in Economics and Statistics},
	author = {Kneib, Thomas and Umlauf, Nikolaus},
	year = {2017},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/UPCP2WD6/Kneib and Umlauf - 2017 - A primer on Bayesian distributional regression.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/U2TEPYD2/180164.html:text/html}
}

@article{heldProbabilisticForecastingInfectious2017,
	title = {Probabilistic forecasting in infectious disease epidemiology: the 13th {Armitage} lecture},
	volume = {36},
	copyright = {Copyright © 2017 John Wiley \& Sons, Ltd.},
	issn = {1097-0258},
	shorttitle = {Probabilistic forecasting in infectious disease epidemiology},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.7363},
	doi = {10.1002/sim.7363},
	abstract = {Routine surveillance of notifiable infectious diseases gives rise to daily or weekly counts of reported cases stratified by region and age group. From a public health perspective, forecasts of infectious disease spread are of central importance. We argue that such forecasts need to properly incorporate the attached uncertainty, so they should be probabilistic in nature. However, forecasts also need to take into account temporal dependencies inherent to communicable diseases, spatial dynamics through human travel and social contact patterns between age groups. We describe a multivariate time series model for weekly surveillance counts on norovirus gastroenteritis from the 12 city districts of Berlin, in six age groups, from week 2011/27 to week 2015/26. The following year (2015/27 to 2016/26) is used to assess the quality of the predictions. Probabilistic forecasts of the total number of cases can be derived through Monte Carlo simulation, but first and second moments are also available analytically. Final size forecasts as well as multivariate forecasts of the total number of cases by age group, by district and by week are compared across different models of varying complexity. This leads to a more general discussion of issues regarding modelling, prediction and evaluation of public health surveillance data. Copyright © 2017 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {22},
	urldate = {2019-09-16},
	journal = {Statistics in Medicine},
	author = {Held, Leonhard and Meyer, Sebastian and Bracher, Johannes},
	year = {2017},
	keywords = {age-structured contact matrix, endemic–epidemic modelling, multivariate probabilistic forecasting, proper scoring rules, spatio-temporal surveillance data},
	pages = {3443--3460},
	file = {Held et al. - 2017 - Probabilistic forecasting in infectious disease ep.pdf:/mnt/data/Google Drive/Zotero/storage/DIJH7TNP/Held et al. - 2017 - Probabilistic forecasting in infectious disease ep.pdf:application/pdf}
}

@article{heldProbabilisticForecastingInfectious2017a,
	title = {Probabilistic forecasting in infectious disease epidemiology: the 13th {Armitage} lecture},
	volume = {36},
	copyright = {Copyright © 2017 John Wiley \& Sons, Ltd.},
	issn = {1097-0258},
	shorttitle = {Probabilistic forecasting in infectious disease epidemiology},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.7363},
	doi = {10.1002/sim.7363},
	abstract = {Routine surveillance of notifiable infectious diseases gives rise to daily or weekly counts of reported cases stratified by region and age group. From a public health perspective, forecasts of infectious disease spread are of central importance. We argue that such forecasts need to properly incorporate the attached uncertainty, so they should be probabilistic in nature. However, forecasts also need to take into account temporal dependencies inherent to communicable diseases, spatial dynamics through human travel and social contact patterns between age groups. We describe a multivariate time series model for weekly surveillance counts on norovirus gastroenteritis from the 12 city districts of Berlin, in six age groups, from week 2011/27 to week 2015/26. The following year (2015/27 to 2016/26) is used to assess the quality of the predictions. Probabilistic forecasts of the total number of cases can be derived through Monte Carlo simulation, but first and second moments are also available analytically. Final size forecasts as well as multivariate forecasts of the total number of cases by age group, by district and by week are compared across different models of varying complexity. This leads to a more general discussion of issues regarding modelling, prediction and evaluation of public health surveillance data. Copyright © 2017 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {22},
	urldate = {2019-09-16},
	journal = {Statistics in Medicine},
	author = {Held, Leonhard and Meyer, Sebastian and Bracher, Johannes},
	year = {2017},
	keywords = {age-structured contact matrix, endemic–epidemic modelling, multivariate probabilistic forecasting, proper scoring rules, spatio-temporal surveillance data},
	pages = {3443--3460},
	file = {Snapshot:/mnt/data/Google Drive/Zotero/storage/TR4PHLGF/sim.html:text/html}
}

@article{heldProbabilisticForecastingInfectious2017b,
	title = {Probabilistic forecasting in infectious disease epidemiology: the 13th {Armitage} lecture},
	volume = {36},
	issn = {1097-0258},
	shorttitle = {Probabilistic forecasting in infectious disease epidemiology},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/sim.7363},
	doi = {10.1002/sim.7363},
	abstract = {Routine surveillance of notifiable infectious diseases gives rise to daily or weekly counts of reported cases stratified by region and age group. From a public health perspective, forecasts of infect...},
	language = {en},
	number = {22},
	urldate = {2019-09-16},
	journal = {Statistics in Medicine},
	author = {Held, Leonhard and Meyer, Sebastian and Bracher, Johannes},
	month = sep,
	year = {2017},
	pages = {3443--3460},
	file = {Snapshot:/mnt/data/Google Drive/Zotero/storage/WZARB3AM/sim.html:text/html}
}

@article{funkAssessingPerformanceRealtime2019,
	title = {Assessing the performance of real-time epidemic forecasts: {A} case study of {Ebola} in the {Western} {Area} region of {Sierra} {Leone}, 2014-15},
	volume = {15},
	issn = {1553-7358},
	shorttitle = {Assessing the performance of real-time epidemic forecasts},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006785},
	doi = {10.1371/journal.pcbi.1006785},
	abstract = {Real-time forecasts based on mathematical models can inform critical decision-making during infectious disease outbreaks. Yet, epidemic forecasts are rarely evaluated during or after the event, and there is little guidance on the best metrics for assessment. Here, we propose an evaluation approach that disentangles different components of forecasting ability using metrics that separately assess the calibration, sharpness and bias of forecasts. This makes it possible to assess not just how close a forecast was to reality but also how well uncertainty has been quantified. We used this approach to analyse the performance of weekly forecasts we generated in real time for Western Area, Sierra Leone, during the 2013–16 Ebola epidemic in West Africa. We investigated a range of forecast model variants based on the model fits generated at the time with a semi-mechanistic model, and found that good probabilistic calibration was achievable at short time horizons of one or two weeks ahead but model predictions were increasingly unreliable at longer forecasting horizons. This suggests that forecasts may have been of good enough quality to inform decision making based on predictions a few weeks ahead of time but not longer, reflecting the high level of uncertainty in the processes driving the trajectory of the epidemic. Comparing forecasts based on the semi-mechanistic model to simpler null models showed that the best semi-mechanistic model variant performed better than the null models with respect to probabilistic calibration, and that this would have been identified from the earliest stages of the outbreak. As forecasts become a routine part of the toolkit in public health, standards for evaluation of performance will be important for assessing quality and improving credibility of mathematical models, and for elucidating difficulties and trade-offs when aiming to make the most useful and reliable forecasts.},
	language = {en},
	number = {2},
	urldate = {2019-09-16},
	journal = {PLOS Computational Biology},
	author = {Funk, Sebastian and Camacho, Anton and Kucharski, Adam J. and Lowe, Rachel and Eggo, Rosalind M. and Edmunds, W. John},
	month = feb,
	year = {2019},
	keywords = {Epidemiology, Infectious disease epidemiology, Probability distribution, Mathematical models, Forecasting, Infectious diseases, Public and occupational health, Sierra Leone},
	pages = {e1006785},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/X6Z9PIFT/Funk et al. - 2019 - Assessing the performance of real-time epidemic fo.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/JN28VVKF/article.html:text/html}
}

@misc{UtilizingGeneralHuman,
	title = {Utilizing general human movement models to predict the spread of emerging infectious diseases in resource poor settings {\textbar} {Scientific} {Reports}},
	url = {https://www.nature.com/articles/s41598-019-41192-3},
	urldate = {2019-09-16},
	file = {Utilizing general human movement models to predict the spread of emerging infectious diseases in resource poor settings | Scientific Reports:/mnt/data/Google Drive/Zotero/storage/I9FTYTFG/s41598-019-41192-3.html:text/html}
}

@article{morganHowDecisionMakers2019,
	title = {How decision makers can use quantitative approaches to guide outbreak responses},
	volume = {374},
	url = {https://royalsocietypublishing.org/doi/10.1098/rstb.2018.0365},
	doi = {10.1098/rstb.2018.0365},
	abstract = {Decision makers are responsible for directing staffing, logistics, selecting public health interventions, communicating to professionals and the public, planning future response needs, and establishing strategic and tactical priorities along with their funding requirements. Decision makers need to rapidly synthesize data from different experts across multiple disciplines, bridge data gaps and translate epidemiological analysis into an operational set of decisions for disease control. Analytic approaches can be defined for specific response phases: investigation, scale-up and control. These approaches include: improved applications of quantitative methods to generate insightful epidemiological descriptions of outbreaks; robust investigations of causal agents and risk factors; tools to assess response needs; identifying and monitoring optimal interventions or combinations of interventions; and forecasting for response planning. Data science and quantitative approaches can improve decision-making in outbreak response. To realize these benefits, we need to develop a structured approach that will improve the quality and timeliness of data collected during outbreaks, establish analytic teams within the response structure and define a research agenda for data analytics in outbreak response.This article is part of the theme issue ‘Modelling infectious disease outbreaks in humans, animals and plants: epidemic forecasting and control’. This theme issue is linked with the earlier issue ‘Modelling infectious disease outbreaks in humans, animals and plants: approaches and important themes’.},
	number = {1776},
	urldate = {2019-09-16},
	journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {Morgan, Oliver},
	month = jul,
	year = {2019},
	pages = {20180365},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/L6K9JPT7/Morgan - 2019 - How decision makers can use quantitative approache.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/CXQI7XFI/rstb.2018.html:text/html}
}

@article{kraemerUtilizingGeneralHuman2019,
	title = {Utilizing general human movement models to predict the spread of emerging infectious diseases in resource poor settings},
	volume = {9},
	issn = {2045-2322},
	url = {http://www.nature.com/articles/s41598-019-41192-3},
	doi = {10.1038/s41598-019-41192-3},
	language = {en},
	number = {1},
	urldate = {2019-09-16},
	journal = {Scientific Reports},
	author = {Kraemer, M. U. G. and Golding, N. and Bisanzio, D. and Bhatt, S. and Pigott, D. M. and Ray, S. E. and Brady, O. J. and Brownstein, J. S. and Faria, N. R. and Cummings, D. A. T. and Pybus, O. G. and Smith, D. L. and Tatem, A. J. and Hay, S. I. and Reiner, R. C.},
	month = dec,
	year = {2019},
	pages = {5151},
	file = {Kraemer et al. - 2019 - Utilizing general human movement models to predict.pdf:/mnt/data/Google Drive/Zotero/storage/Y98P356R/Kraemer et al. - 2019 - Utilizing general human movement models to predict.pdf:application/pdf}
}

@article{wangDeepLearningSpatioTemporal2019,
	title = {Deep {Learning} for {Spatio}-{Temporal} {Data} {Mining}: {A} {Survey}},
	shorttitle = {Deep {Learning} for {Spatio}-{Temporal} {Data} {Mining}},
	url = {https://arxiv.org/abs/1906.04928v2},
	abstract = {With the fast development of various positioning techniques such as Global
Position System (GPS), mobile devices and remote sensing, spatio-temporal data
has become increasingly available nowadays. Mining valuable knowledge from
spatio-temporal data is critically important to many real world applications
including human mobility understanding, smart transportation, urban planning,
public safety, health care and environmental management. As the number, volume
and resolution of spatio-temporal datasets increase rapidly, traditional data
mining methods, especially statistics based methods for dealing with such data
are becoming overwhelmed. Recently, with the advances of deep learning
techniques, deep leaning models such as convolutional neural network (CNN) and
recurrent neural network (RNN) have enjoyed considerable success in various
machine learning tasks due to their powerful hierarchical feature learning
ability in both spatial and temporal domains, and have been widely applied in
various spatio-temporal data mining (STDM) tasks such as predictive learning,
representation learning, anomaly detection and classification. In this paper,
we provide a comprehensive survey on recent progress in applying deep learning
techniques for STDM. We first categorize the types of spatio-temporal data and
briefly introduce the popular deep learning models that are used in STDM. Then
a framework is introduced to show a general pipeline of the utilization of deep
learning models for STDM. Next we classify existing literatures based on the
types of ST data, the data mining tasks, and the deep learning models, followed
by the applications of deep learning for STDM in different domains including
transportation, climate science, human mobility, location based social network,
crime analysis, and neuroscience. Finally, we conclude the limitations of
current research and point out future research directions.},
	language = {en},
	urldate = {2019-09-19},
	author = {Wang, Senzhang and Cao, Jiannong and Yu, Philip S.},
	month = jun,
	year = {2019},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/D7VKGB9B/Wang et al. - 2019 - Deep Learning for Spatio-Temporal Data Mining A S.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/KQZYU92I/1906.html:text/html}
}

@article{wangDeepLearningSpatioTemporal2019a,
	title = {Deep {Learning} for {Spatio}-{Temporal} {Data} {Mining}: {A} {Survey}},
	shorttitle = {Deep {Learning} for {Spatio}-{Temporal} {Data} {Mining}},
	url = {http://arxiv.org/abs/1906.04928},
	abstract = {With the fast development of various positioning techniques such as Global Position System (GPS), mobile devices and remote sensing, spatio-temporal data has become increasingly available nowadays. Mining valuable knowledge from spatio-temporal data is critically important to many real world applications including human mobility understanding, smart transportation, urban planning, public safety, health care and environmental management. As the number, volume and resolution of spatio-temporal datasets increase rapidly, traditional data mining methods, especially statistics based methods for dealing with such data are becoming overwhelmed. Recently, with the advances of deep learning techniques, deep leaning models such as convolutional neural network (CNN) and recurrent neural network (RNN) have enjoyed considerable success in various machine learning tasks due to their powerful hierarchical feature learning ability in both spatial and temporal domains, and have been widely applied in various spatio-temporal data mining (STDM) tasks such as predictive learning, representation learning, anomaly detection and classiﬁcation. In this paper, we provide a comprehensive survey on recent progress in applying deep learning techniques for STDM. We ﬁrst categorize the types of spatio-temporal data and brieﬂy introduce the popular deep learning models that are used in STDM. Then a framework is introduced to show a general pipeline of the utilization of deep learning models for STDM. Next we classify existing literatures based on the types of ST data, the data mining tasks, and the deep learning models, followed by the applications of deep learning for STDM in different domains including transportation, climate science, human mobility, location based social network, crime analysis, and neuroscience. Finally, we conclude the limitations of current research and point out future research directions.},
	language = {en},
	urldate = {2019-09-19},
	journal = {arXiv:1906.04928 [cs, stat]},
	author = {Wang, Senzhang and Cao, Jiannong and Yu, Philip S.},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.04928},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: arXiv admin note: text overlap with arXiv:1711.04710 by other authors},
	file = {Wang et al. - 2019 - Deep Learning for Spatio-Temporal Data Mining A S.pdf:/mnt/data/Google Drive/Zotero/storage/NG8DKUBP/Wang et al. - 2019 - Deep Learning for Spatio-Temporal Data Mining A S.pdf:application/pdf}
}

@inproceedings{jainStructuralRNNDeepLearning2016,
	address = {Las Vegas, NV, USA},
	title = {Structural-{RNN}: {Deep} {Learning} on {Spatio}-{Temporal} {Graphs}},
	isbn = {978-1-4673-8851-1},
	shorttitle = {Structural-{RNN}},
	url = {http://ieeexplore.ieee.org/document/7780942/},
	doi = {10.1109/CVPR.2016.573},
	abstract = {Deep Recurrent Neural Network architectures, though remarkably capable at modeling sequences, lack an intuitive high-level spatio-temporal structure. That is while many problems in computer vision inherently have an underlying high-level structure and can beneﬁt from it. Spatiotemporal graphs are a popular tool for imposing such highlevel intuitions in the formulation of real world problems. In this paper, we propose an approach for combining the power of high-level spatio-temporal graphs and sequence learning success of Recurrent Neural Networks (RNNs). We develop a scalable method for casting an arbitrary spatiotemporal graph as a rich RNN mixture that is feedforward, fully differentiable, and jointly trainable. The proposed method is generic and principled as it can be used for transforming any spatio-temporal graph through employing a certain set of well deﬁned steps. The evaluations of the proposed approach on a diverse set of problems, ranging from modeling human motion to object interactions, shows improvement over the state-of-the-art with a large margin. We expect this method to empower new approaches to problem formulation through high-level spatio-temporal graphs and Recurrent Neural Networks.},
	language = {en},
	urldate = {2019-09-19},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Jain, Ashesh and Zamir, Amir R. and Savarese, Silvio and Saxena, Ashutosh},
	month = jun,
	year = {2016},
	pages = {5308--5317},
	file = {2019.12.27.889212v1.full.pdf:/mnt/data/Google Drive/Zotero/storage/PGL5S7MA/2019.12.27.889212v1.full.pdf:application/pdf;Jain et al. - 2016 - Structural-RNN Deep Learning on Spatio-Temporal G.pdf:/mnt/data/Google Drive/Zotero/storage/4B2UEDPF/Jain et al. - 2016 - Structural-RNN Deep Learning on Spatio-Temporal G.pdf:application/pdf}
}

@misc{tangSTLSTMDeepLearning2019,
	type = {Research article},
	title = {{ST}-{LSTM}: {A} {Deep} {Learning} {Approach} {Combined} {Spatio}-{Temporal} {Features} for {Short}-{Term} {Forecast} in {Rail} {Transit}},
	shorttitle = {{ST}-{LSTM}},
	url = {https://www.hindawi.com/journals/jat/2019/8392592/},
	abstract = {The short-term forecast of rail transit is one of the most essential issues in urban intelligent transportation system (ITS). Accurate forecast result can provide support for the forewarning of flow outburst and enables passengers to make an appropriate travel plan. Therefore, it is significant to develop a more accurate forecast model. Long short-term memory (LSTM) network has been proved to be effective on data with temporal features. However, it cannot process the correlation between time and space in rail transit. As a result, a novel forecast model combining spatio-temporal features based on LSTM network (ST-LSTM) is proposed. Different from other forecast methods, ST-LSTM network uses a new method to extract spatio-temporal features from the data and combines them together as the input. Compared with other conventional models, ST-LSTM network can achieve a better performance in experiments.},
	language = {en},
	urldate = {2019-09-19},
	journal = {Journal of Advanced Transportation},
	author = {Tang, Qicheng and Yang, Mengning and Yang, Ying},
	year = {2019},
	doi = {10.1155/2019/8392592},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/B28NWTUQ/Tang et al. - 2019 - ST-LSTM A Deep Learning Approach Combined Spatio-.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/PFQZ6RKG/8392592.html:application/xhtml+xml}
}

@inproceedings{jainStructuralRNNDeepLearning2016a,
	address = {Las Vegas, NV, USA},
	title = {Structural-{RNN}: {Deep} {Learning} on {Spatio}-{Temporal} {Graphs}},
	isbn = {978-1-4673-8851-1},
	shorttitle = {Structural-{RNN}},
	url = {http://ieeexplore.ieee.org/document/7780942/},
	doi = {10.1109/CVPR.2016.573},
	abstract = {Deep Recurrent Neural Network architectures, though remarkably capable at modeling sequences, lack an intuitive high-level spatio-temporal structure. That is while many problems in computer vision inherently have an underlying high-level structure and can beneﬁt from it. Spatiotemporal graphs are a popular tool for imposing such highlevel intuitions in the formulation of real world problems. In this paper, we propose an approach for combining the power of high-level spatio-temporal graphs and sequence learning success of Recurrent Neural Networks (RNNs). We develop a scalable method for casting an arbitrary spatiotemporal graph as a rich RNN mixture that is feedforward, fully differentiable, and jointly trainable. The proposed method is generic and principled as it can be used for transforming any spatio-temporal graph through employing a certain set of well deﬁned steps. The evaluations of the proposed approach on a diverse set of problems, ranging from modeling human motion to object interactions, shows improvement over the state-of-the-art with a large margin. We expect this method to empower new approaches to problem formulation through high-level spatio-temporal graphs and Recurrent Neural Networks.},
	language = {en},
	urldate = {2019-09-19},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Jain, Ashesh and Zamir, Amir R. and Savarese, Silvio and Saxena, Ashutosh},
	month = jun,
	year = {2016},
	keywords = {not relevant},
	pages = {5308--5317},
	file = {Jain et al. - 2016 - Structural-RNN Deep Learning on Spatio-Temporal G.pdf:/mnt/data/Google Drive/Zotero/storage/NHME8GMH/Jain et al. - 2016 - Structural-RNN Deep Learning on Spatio-Temporal G.pdf:application/pdf}
}

@misc{leeGeoslegendDeepLearningforSpatiotemporalPrediction2019,
	title = {geoslegend/{Deep}-{Learning}-for-{Spatio}-temporal-{Prediction}},
	url = {https://github.com/geoslegend/Deep-Learning-for-Spatio-temporal-Prediction},
	abstract = {Contribute to geoslegend/Deep-Learning-for-Spatio-temporal-Prediction development by creating an account on GitHub.},
	urldate = {2019-09-19},
	author = {Lee, Seongkyu},
	month = sep,
	year = {2019},
	note = {original-date: 2018-11-01T02:02:00Z}
}

@article{jaskiewiczArgonauteCLIPMethod2012,
	title = {Argonaute {CLIP} – {A} method to identify in vivo targets of {miRNAs}},
	volume = {58},
	issn = {10462023},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1046202312002460},
	doi = {10.1016/j.ymeth.2012.09.006},
	language = {en},
	number = {2},
	urldate = {2019-09-21},
	journal = {Methods},
	author = {Jaskiewicz, Lukasz and Bilen, Biter and Hausser, Jean and Zavolan, Mihaela},
	month = oct,
	year = {2012},
	pages = {106--112},
	file = {Jaskiewicz et al. - 2012 - Argonaute CLIP – A method to identify in vivo targ.pdf:/mnt/data/Google Drive/Zotero/storage/G3WF2A6I/Jaskiewicz et al. - 2012 - Argonaute CLIP – A method to identify in vivo targ.pdf:application/pdf}
}

@article{heldProbabilisticForecastingInfectious2017c,
	title = {Probabilistic forecasting in infectious disease epidemiology: the 13th {Armitage} lecture: {L}. {HELD}, {S}. {MEYER} {AND} {J}. {BRACHER}},
	volume = {36},
	issn = {02776715},
	shorttitle = {Probabilistic forecasting in infectious disease epidemiology},
	url = {http://doi.wiley.com/10.1002/sim.7363},
	doi = {10.1002/sim.7363},
	language = {en},
	number = {22},
	urldate = {2019-09-21},
	journal = {Statistics in Medicine},
	author = {Held, Leonhard and Meyer, Sebastian and Bracher, Johannes},
	month = sep,
	year = {2017},
	pages = {3443--3460},
	file = {Held et al. - 2017 - Probabilistic forecasting in infectious disease ep.pdf:/mnt/data/Google Drive/Zotero/storage/4GBAAE6J/Held et al. - 2017 - Probabilistic forecasting in infectious disease ep.pdf:application/pdf}
}

@article{ehlertSupervisedDrThomas,
	title = {supervised by {Dr}. {Thomas} {Dimpﬂ}},
	language = {en},
	author = {Ehlert, Jan},
	pages = {73},
	file = {Ehlert - supervised by Dr. Thomas Dimpﬂ.pdf:/mnt/data/Google Drive/Zotero/storage/TIGQ5S6C/Ehlert - supervised by Dr. Thomas Dimpﬂ.pdf:application/pdf}
}

@article{schobelStatisticalInferenceforPropagation,
	title = {Statistical {Inferencefor} {Propagation} {Processeson} {Complex} {Networks}},
	language = {en},
	author = {Schöbel, Dr Anita and Göttingen, Georg-August-Universität and Kneib, Dr Thomas},
	pages = {200},
	file = {Statistical Inferencefor Propagation Processeson Complex Networks.pdf:/mnt/data/Google Drive/Zotero/storage/L2YVK83S/Schöbel et al. - Mitglieder der Prüfungskommision.pdf:application/pdf}
}

@article{cooperMethodDetectingCharacterizing2015,
	title = {A method for detecting and characterizing outbreaks of infectious disease from clinical reports},
	volume = {53},
	issn = {15320464},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1532046414001920},
	doi = {10.1016/j.jbi.2014.08.011},
	abstract = {Outbreaks of infectious disease can pose a signiﬁcant threat to human health. Thus, detecting and characterizing outbreaks quickly and accurately remains an important problem. This paper describes a Bayesian framework that links clinical diagnosis of individuals in a population to epidemiological modeling of disease outbreaks in the population. Computer-based diagnosis of individuals who seek healthcare is used to guide the search for epidemiological models of population disease that explain the pattern of diagnoses well. We applied this framework to develop a system that detects inﬂuenza outbreaks from emergency department (ED) reports. The system diagnoses inﬂuenza in individuals probabilistically from evidence in ED reports that are extracted using natural language processing. These diagnoses guide the search for epidemiological models of inﬂuenza that explain the pattern of diagnoses well. Those epidemiological models with a high posterior probability determine the most likely outbreaks of speciﬁc diseases; the models are also used to characterize properties of an outbreak, such as its expected peak day and estimated size. We evaluated the method using both simulated data and data from a real inﬂuenza outbreak. The results provide support that the approach can detect and characterize outbreaks early and well enough to be valuable. We describe several extensions to the approach that appear promising.},
	language = {en},
	urldate = {2019-10-08},
	journal = {Journal of Biomedical Informatics},
	author = {Cooper, Gregory F. and Villamarin, Ricardo and (Rich) Tsui, Fu-Chiang and Millett, Nicholas and Espino, Jeremy U. and Wagner, Michael M.},
	month = feb,
	year = {2015},
	pages = {15--26},
	file = {Cooper et al. - 2015 - A method for detecting and characterizing outbreak.pdf:/mnt/data/Google Drive/Zotero/storage/TPYMSQ39/Cooper et al. - 2015 - A method for detecting and characterizing outbreak.pdf:application/pdf}
}

@misc{HowShapeWeakly,
	title = {How the {Shape} of a {Weakly} {Informative} {Prior} {Affects} {Inferences}},
	url = {https://mc-stan.org/users/documentation/case-studies/weakly_informative_shapes.html},
	urldate = {2019-10-22},
	file = {How the Shape of a Weakly Informative Prior Affects Inferences:/mnt/data/Google Drive/Zotero/storage/D49TJY8H/weakly_informative_shapes.html:text/html}
}

@article{nortonIgnoranceIndifference2008,
	title = {Ignorance and {Indifference}*},
	volume = {75},
	issn = {0031-8248, 1539-767X},
	url = {https://www.journals.uchicago.edu/doi/10.1086/587822},
	doi = {10.1086/587822},
	language = {en},
	number = {1},
	urldate = {2019-10-22},
	journal = {Philosophy of Science},
	author = {Norton, John D.},
	month = jan,
	year = {2008},
	pages = {45--68},
	file = {Norton - 2008 - Ignorance and Indifference.pdf:/mnt/data/Google Drive/Zotero/storage/LXYCPXK9/Norton - 2008 - Ignorance and Indifference.pdf:application/pdf}
}

@article{meyerHhh4EndemicepidemicModeling,
	title = {hhh4: {Endemic}-epidemic modeling of areal count time series},
	abstract = {The availability of geocoded health data and the inherent temporal structure of communicable diseases have led to an increased interest in statistical models and software for spatio-temporal data with epidemic features. The R package surveillance can handle various levels of aggregation at which infective events have been recorded. This vignette illustrates the analysis of area-level time series of counts using the endemic-epidemic multivariate time-series model “hhh4” described in, e.g., Meyer and Held (2014, Section 3). See vignette("hhh4") for a more general introduction to hhh4 models, including the univariate and non-spatial bivariate case. We ﬁrst describe the general modeling approach and then exemplify data handling, model ﬁtting, visualization, and simulation methods for weekly counts of measles infections by district in the Weser-Ems region of Lower Saxony, Germany, 2001–2002.},
	language = {en},
	author = {Meyer, Sebastian and Held, Leonhard and Höhle, Michael},
	pages = {23},
	file = {Meyer et al. - hhh4 Endemic-epidemic modeling of areal count tim.pdf:/mnt/data/Google Drive/Zotero/storage/UJLPF42G/Meyer et al. - hhh4 Endemic-epidemic modeling of areal count tim.pdf:application/pdf}
}

@article{meyerSpatioTemporalAnalysisEpidemic2017,
	title = {Spatio-{Temporal} {Analysis} of {Epidemic} {Phenomena} {Using} the {R} {Package} surveillance},
	volume = {77},
	copyright = {Copyright (c) 2017 Sebastian Meyer, Leonhard Held, Michael Höhle},
	issn = {1548-7660},
	url = {https://www.jstatsoft.org/index.php/jss/article/view/v077i11},
	doi = {10.18637/jss.v077.i11},
	language = {en},
	number = {1},
	urldate = {2019-10-29},
	journal = {Journal of Statistical Software},
	author = {Meyer, Sebastian and Held, Leonhard and Höhle, Michael},
	month = may,
	year = {2017},
	keywords = {spatio-temporal surveillance data, branching process with immigration, endemic-epidemic modeling, infectious disease epidemiology, multivariate time series of counts, self-exciting point process},
	pages = {1--55},
	file = {Full Text:/mnt/data/Google Drive/Zotero/storage/AJL3KXFX/Meyer et al. - 2017 - Spatio-Temporal Analysis of Epidemic Phenomena Usi.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/K3RTGPW8/v077i11.html:text/html}
}

@article{lesslerMappingBurdenCholera2018,
	title = {Mapping the burden of cholera in sub-{Saharan} {Africa} and implications for control: an analysis of data across geographical scales},
	volume = {391},
	issn = {0140-6736, 1474-547X},
	shorttitle = {Mapping the burden of cholera in sub-{Saharan} {Africa} and implications for control},
	url = {https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(17)33050-7/abstract},
	doi = {10.1016/S0140-6736(17)33050-7},
	abstract = {{\textless}h2{\textgreater}Summary{\textless}/h2{\textgreater}{\textless}h3{\textgreater}Background{\textless}/h3{\textgreater}{\textless}p{\textgreater}Cholera remains a persistent health problem in sub-Saharan Africa and worldwide. Cholera can be controlled through appropriate water and sanitation, or by oral cholera vaccination, which provides transient (∼3 years) protection, although vaccine supplies remain scarce. We aimed to map cholera burden in sub-Saharan Africa and assess how geographical targeting could lead to more efficient interventions.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Methods{\textless}/h3{\textgreater}{\textless}p{\textgreater}We combined information on cholera incidence in sub-Saharan Africa (excluding Djibouti and Eritrea) from 2010 to 2016 from datasets from WHO, Médecins Sans Frontières, ProMED, ReliefWeb, ministries of health, and the scientific literature. We divided the study region into 20 km × 20 km grid cells and modelled annual cholera incidence in each grid cell assuming a Poisson process adjusted for covariates and spatially correlated random effects. We combined these findings with data on population distribution to estimate the number of people living in areas of high cholera incidence ({\textgreater}1 case per 1000 people per year). We further estimated the reduction in cholera incidence that could be achieved by targeting cholera prevention and control interventions at areas of high cholera incidence.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Findings{\textless}/h3{\textgreater}{\textless}p{\textgreater}We included 279 datasets covering 2283 locations in our analyses. In sub-Saharan Africa (excluding Djibouti and Eritrea), a mean of 141 918 cholera cases (95\% credible interval [CrI] 141 538–146 505) were reported per year. 4·0\% (95\% CrI 1·7–16·8) of districts, home to 87·2 million people (95\% CrI 60·3 million to 118·9 million), have high cholera incidence. By focusing on the highest incidence districts first, effective targeted interventions could eliminate 50\% of the region's cholera by covering 35·3 million people (95\% CrI 26·3 million to 62·0 million), which is less than 4\% of the total population.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Interpretation{\textless}/h3{\textgreater}{\textless}p{\textgreater}Although cholera occurs throughout sub-Saharan Africa, its highest incidence is concentrated in a small proportion of the continent. Prioritising high-risk areas could substantially increase the efficiency of cholera control programmes.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Funding{\textless}/h3{\textgreater}{\textless}p{\textgreater}The Bill \& Melinda Gates Foundation.{\textless}/p{\textgreater}},
	language = {English},
	number = {10133},
	urldate = {2019-10-29},
	journal = {The Lancet},
	author = {Lessler, Justin and Moore, Sean M. and Luquero, Francisco J. and McKay, Heather S. and Grais, Rebecca and Henkens, Myriam and Mengel, Martin and Dunoyer, Jessica and M'bangombe, Maurice and Lee, Elizabeth C. and Djingarey, Mamoudou Harouna and Sudre, Bertrand and Bompangue, Didier and Fraser, Robert S. M. and Abubakar, Abdinasir and Perea, William and Legros, Dominique and Azman, Andrew S.},
	month = may,
	year = {2018},
	pmid = {29502905},
	pages = {1908--1915},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/R34ESHXT/Lessler et al. - 2018 - Mapping the burden of cholera in sub-Saharan Afric.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/G9WXKBP6/fulltext.html:text/html}
}

@article{badkundriForecasting20172018Yemen2019,
	title = {Forecasting the 2017-2018 {Yemen} {Cholera} {Outbreak} with {Machine} {Learning}},
	url = {http://arxiv.org/abs/1902.06739},
	abstract = {The ongoing Yemen cholera outbreak has been deemed one of the worst cholera outbreaks in history, with over a million people impacted and thousands dead. Triggered by a civil war, the outbreak has been shaped by various political, environmental, and epidemiological factors and continues to worsen. While cholera has several effective treatments, the untimely and inefficient distribution of existing medicines has been the primary cause of cholera mortality. With the hope of facilitating resource allocation, various mathematical models have been created to track the Yemeni outbreak and identify at-risk administrative divisions, called governorates. Existing models are not powerful enough to accurately and consistently forecast cholera cases per governorate over multiple timeframes. To address the need for a complex, reliable model, we offer the Cholera Artificial Learning Model (CALM); a system of 4 extreme-gradient-boosting (XGBoost) machine learning models that forecast the number of new cholera cases a Yemeni governorate will experience from a time range of 2 weeks to 2 months. CALM provides a novel machine learning approach that makes use of rainfall data, past cholera cases and deaths data, civil war fatalities, and inter-governorate interactions represented across multiple time frames. Additionally, the use of machine learning, along with extensive feature engineering, allows CALM to easily learn complex non-linear relations apparent in an epidemiological phenomenon. CALM is able to forecast cholera incidence 2 weeks to 2 months in advance within a margin of just 5 cholera cases per 10,000 people in real-world simulation.},
	urldate = {2019-10-29},
	journal = {arXiv:1902.06739 [cs, q-bio]},
	author = {Badkundri, Rohil and Valbuena, Victor and Pinnamareddy, Srikusmanjali and Cantrell, Brittney and Standeven, Janet},
	month = feb,
	year = {2019},
	note = {arXiv: 1902.06739},
	keywords = {Computer Science - Machine Learning, 68T01, Computer Science - Computers and Society, Quantitative Biology - Quantitative Methods},
	annote = {Comment: Originally completed as part of the iGEM competition (see http://2018.igem.org/Team:Lambert\_GA/Software); 3431 words, 1 table, 2 manuscript figures, 2 supplementary figures},
	file = {arXiv Fulltext PDF:/mnt/data/Google Drive/Zotero/storage/CIJMPID3/Badkundri et al. - 2019 - Forecasting the 2017-2018 Yemen Cholera Outbreak w.pdf:application/pdf;arXiv.org Snapshot:/mnt/data/Google Drive/Zotero/storage/I8V88CEC/1902.html:text/html}
}

@article{camachoCholeraEpidemicYemen2018,
	title = {Cholera epidemic in {Yemen}, 2016–18: an analysis of surveillance data},
	volume = {6},
	issn = {2214-109X},
	shorttitle = {Cholera epidemic in {Yemen}, 2016–18},
	url = {https://www.thelancet.com/journals/langlo/article/PIIS2214-109X(18)30230-4/abstract},
	doi = {10.1016/S2214-109X(18)30230-4},
	abstract = {{\textless}h2{\textgreater}Summary{\textless}/h2{\textgreater}{\textless}h3{\textgreater}Background{\textless}/h3{\textgreater}{\textless}p{\textgreater}In war-torn Yemen, reports of confirmed cholera started in late September, 2016. The disease continues to plague Yemen today in what has become the largest documented cholera epidemic of modern times. We aimed to describe the key epidemiological features of this epidemic, including the drivers of cholera transmission during the outbreak.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Methods{\textless}/h3{\textgreater}{\textless}p{\textgreater}The Yemen Health Authorities set up a national cholera surveillance system to collect information on suspected cholera cases presenting at health facilities. Individual variables included symptom onset date, age, severity of dehydration, and rapid diagnostic test result. Suspected cholera cases were confirmed by culture, and a subset of samples had additional phenotypic and genotypic analysis. We first conducted descriptive analyses at national and governorate levels. We divided the epidemic into three time periods: the first wave (Sept 28, 2016, to April 23, 2017), the increasing phase of the second wave (April 24, 2017, to July 2, 2017), and the decreasing phase of the second wave (July 3, 2017, to March 12, 2018). We reconstructed the changes in cholera transmission over time by estimating the instantaneous reproduction number, \textit{R}$_{\textrm{t}}$. Finally, we estimated the association between rainfall and the daily cholera incidence during the increasing phase of the second epidemic wave by fitting a spatiotemporal regression model.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Findings{\textless}/h3{\textgreater}{\textless}p{\textgreater}From Sept 28, 2016, to March 12, 2018, 1 103 683 suspected cholera cases (attack rate 3·69\%) and 2385 deaths (case fatality risk 0·22\%) were reported countrywide. The epidemic consisted of two distinct waves with a surge in transmission in May, 2017, corresponding to a median \textit{R}$_{\textrm{t}}$ of more than 2 in 13 of 23 governorates. Microbiological analyses suggested that the same \textit{Vibrio cholerae} O1 Ogawa strain circulated in both waves. We found a positive, non-linear, association between weekly rainfall and suspected cholera incidence in the following 10 days; the relative risk of cholera after a weekly rainfall of 25 mm was 1·42 (95\% CI 1·31–1·55) compared with a week without rain.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Interpretation{\textless}/h3{\textgreater}{\textless}p{\textgreater}Our analysis suggests that the small first cholera epidemic wave seeded cholera across Yemen during the dry season. When the rains returned in April, 2017, they triggered widespread cholera transmission that led to the large second wave. These results suggest that cholera could resurge during the ongoing 2018 rainy season if transmission remains active. Therefore, health authorities and partners should immediately enhance current control efforts to mitigate the risk of a new cholera epidemic wave in Yemen.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Funding{\textless}/h3{\textgreater}{\textless}p{\textgreater}Health Authorities of Yemen, WHO, and Médecins Sans Frontières.{\textless}/p{\textgreater}},
	language = {English},
	number = {6},
	urldate = {2019-10-29},
	journal = {The Lancet Global Health},
	author = {Camacho, Anton and Bouhenia, Malika and Alyusfi, Reema and Alkohlani, Abdulhakeem and Naji, Munna Abdulla Mohammed and Radiguès, Xavier de and Abubakar, Abdinasir M. and Almoalmi, Abdulkareem and Seguin, Caroline and Sagrado, Maria Jose and Poncin, Marc and McRae, Melissa and Musoke, Mohammed and Rakesh, Ankur and Porten, Klaudia and Haskew, Christopher and Atkins, Katherine E. and Eggo, Rosalind M. and Azman, Andrew S. and Broekhuijsen, Marije and Saatcioglu, Mehmet Akif and Pezzoli, Lorenzo and Quilici, Marie-Laure and Al-Mesbahy, Abdul Rahman and Zagaria, Nevio and Luquero, Francisco J.},
	month = jun,
	year = {2018},
	pmid = {29731398},
	pages = {e680--e690},
	file = {1-s2.0-S2214109X18302304-mmc1.pdf:/mnt/data/Google Drive/Zotero/storage/3LTL4IF4/1-s2.0-S2214109X18302304-mmc1.pdf:application/pdf;Full Text PDF:/mnt/data/Google Drive/Zotero/storage/5X397UB8/Camacho et al. - 2018 - Cholera epidemic in Yemen, 2016–18 an analysis of.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/7F6Q9SUT/fulltext.html:text/html}
}

@article{wellsExacerbationEbolaOutbreaks2019,
	title = {The exacerbation of {Ebola} outbreaks by conflict in the {Democratic} {Republic} of the {Congo}},
	copyright = {© 2019 . https://www.pnas.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/early/2019/10/15/1913980116},
	doi = {10.1073/pnas.1913980116},
	abstract = {The interplay between civil unrest and disease transmission is not well understood. Violence targeting healthcare workers and Ebola treatment centers in the Democratic Republic of the Congo (DRC) has been thwarting the case isolation, treatment, and vaccination efforts. The extent to which conflict impedes public health response and contributes to incidence has not previously been evaluated. We construct a timeline of conflict events throughout the course of the epidemic and provide an ethnographic appraisal of the local conditions that preceded and followed conflict events. Informed by temporal incidence and conflict data as well as the ethnographic evidence, we developed a model of Ebola transmission and control to assess the impact of conflict on the epidemic in the eastern DRC from April 30, 2018, to June 23, 2019. We found that both the rapidity of case isolation and the population-level effectiveness of vaccination varied notably as a result of preceding unrest and subsequent impact of conflict events. Furthermore, conflict events were found to reverse an otherwise declining phase of the epidemic trajectory. Our model framework can be extended to other infectious diseases in the same and other regions of the world experiencing conflict and violence.},
	language = {en},
	urldate = {2019-10-30},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Wells, Chad R. and Pandey, Abhishek and Mbah, Martial L. Ndeffo and Gaüzère, Bernard-A. and Malvy, Denis and Singer, Burton H. and Galvani, Alison P.},
	month = oct,
	year = {2019},
	pmid = {31636188},
	keywords = {epidemiology, healthcare workers, humanitarian crisis, insecurity},
	pages = {201913980},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/ULSEZQQ3/Wells et al. - 2019 - The exacerbation of Ebola outbreaks by conflict in.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/EH3MGU3D/1913980116.html:text/html}
}

@article{coriNewFrameworkSoftware2013,
	title = {A {New} {Framework} and {Software} to {Estimate} {Time}-{Varying} {Reproduction} {Numbers} {During} {Epidemics}},
	volume = {178},
	issn = {0002-9262},
	url = {https://academic.oup.com/aje/article/178/9/1505/89262},
	doi = {10.1093/aje/kwt133},
	abstract = {Abstract.  The quantification of transmissibility during epidemics is essential to designing and adjusting public health responses. Transmissibility can be meas},
	language = {en},
	number = {9},
	urldate = {2019-10-30},
	journal = {American Journal of Epidemiology},
	author = {Cori, Anne and Ferguson, Neil M. and Fraser, Christophe and Cauchemez, Simon},
	month = nov,
	year = {2013},
	pages = {1505--1512},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/827JXSHL/Cori et al. - 2013 - A New Framework and Software to Estimate Time-Vary.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/UY7M5Z9A/89262.html:text/html}
}

@article{thompsonImprovedInferenceTimevarying2019,
	title = {Improved inference of time-varying reproduction numbers during infectious disease outbreaks},
	issn = {1755-4365},
	url = {http://www.sciencedirect.com/science/article/pii/S1755436519300350},
	doi = {10.1016/j.epidem.2019.100356},
	abstract = {Accurate estimation of the parameters characterising infectious disease transmission is vital for optimising control interventions during epidemics. A valuable metric for assessing the current threat posed by an outbreak is the time-dependent reproduction number, i.e. the expected number of secondary cases caused by each infected individual. This quantity can be estimated using data on the numbers of observed new cases at successive times during an epidemic and the distribution of the serial interval (the time between symptomatic cases in a transmission chain). Some methods for estimating the reproduction number rely on pre-existing estimates of the serial interval distribution and assume that the entire outbreak is driven by local transmission. Here we show that accurate inference of current transmissibility, and the uncertainty associated with this estimate, requires: (i) up-to-date observations of the serial interval to be included, and; (ii) cases arising from local transmission to be distinguished from those imported from elsewhere. We demonstrate how pathogen transmissibility can be inferred appropriately using datasets from outbreaks of H1N1 influenza, Ebola virus disease and Middle-East Respiratory Syndrome. We present a tool for estimating the reproduction number in real-time during infectious disease outbreaks accurately, which is available as an R software package (EpiEstim 2.2). It is also accessible as an interactive, user-friendly online interface (EpiEstim App), permitting its use by non-specialists. Our tool is easy to apply for assessing the transmission potential, and hence informing control, during future outbreaks of a wide range of invading pathogens.},
	language = {en},
	urldate = {2019-10-30},
	journal = {Epidemics},
	author = {Thompson, R. N. and Stockwin, J. E. and van Gaalen, R. D. and Polonsky, J. A. and Kamvar, Z. N. and Demarsh, P. A. and Dahlqwist, E. and Li, S. and Miguel, E. and Jombart, T. and Lessler, J. and Cauchemez, S. and Cori, A.},
	month = aug,
	year = {2019},
	keywords = {Infectious disease epidemiology, Disease control, Mathematical modelling, Parameter inference, Reproduction number, Serial interval},
	pages = {100356},
	file = {ScienceDirect Full Text PDF:/mnt/data/Google Drive/Zotero/storage/QLQPSF74/Thompson et al. - 2019 - Improved inference of time-varying reproduction nu.pdf:application/pdf;ScienceDirect Snapshot:/mnt/data/Google Drive/Zotero/storage/Y8GML2T9/S1755436519300350.html:text/html}
}

@article{gasparriniPenalizedFrameworkDistributed2017,
	title = {A penalized framework for distributed lag non-linear models},
	volume = {73},
	issn = {1541-0420},
	doi = {10.1111/biom.12645},
	abstract = {Distributed lag non-linear models (DLNMs) are a modelling tool for describing potentially non-linear and delayed dependencies. Here, we illustrate an extension of the DLNM framework through the use of penalized splines within generalized additive models (GAM). This extension offers built-in model selection procedures and the possibility of accommodating assumptions on the shape of the lag structure through specific penalties. In addition, this framework includes, as special cases, simpler models previously proposed for linear relationships (DLMs). Alternative versions of penalized DLNMs are compared with each other and with the standard unpenalized version in a simulation study. Results show that this penalized extension to the DLNM class provides greater flexibility and improved inferential properties. The framework exploits recent theoretical developments of GAMs and is implemented using efficient routines within freely available software. Real-data applications are illustrated through two reproducible examples in time series and survival analysis.},
	language = {eng},
	number = {3},
	journal = {Biometrics},
	author = {Gasparrini, Antonio and Scheipl, Fabian and Armstrong, Ben and Kenward, Michael G.},
	year = {2017},
	pmid = {28134978},
	keywords = {Software, Distributed lag, Exposure-lag-response, Generalized additive models, Latency, Nonlinear Dynamics, Penalized splines},
	pages = {938--948},
	file = {Full Text:/mnt/data/Google Drive/Zotero/storage/2MRP9PLR/Gasparrini et al. - 2017 - A penalized framework for distributed lag non-line.pdf:application/pdf}
}

@misc{2016GasparriniEpidem,
	title = {2016\_gasparrini\_Epidem},
	url = {http://www.ag-myresearch.com/2016_gasparrini_epidem.html},
	abstract = {Gasparrini A Epidemiology . 2016; 27 (6):835-842},
	language = {en},
	urldate = {2019-10-31},
	journal = {Antonio Gasparrini: my research},
	annote = {Added R code
github.com/gasparrini/2016\_gasparrini\_Epidem\_Rcode},
	file = {2016_gasparrini_Epidem.pdf:/mnt/data/Google Drive/Zotero/storage/GFY3ANQU/2016_gasparrini_Epidem.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/YRVFWKQT/2016_gasparrini_epidem.html:text/html}
}

@article{gasparriniDistributedLagLinear2011,
	title = {Distributed {Lag} {Linear} and {Non}-{Linear} {Models} in \textit{{R}} : {The} {Package} \textbf{dlnm}},
	volume = {43},
	issn = {1548-7660},
	shorttitle = {Distributed {Lag} {Linear} and {Non}-{Linear} {Models} in \textit{{R}}},
	url = {http://www.jstatsoft.org/v43/i08/},
	doi = {10.18637/jss.v043.i08},
	abstract = {Distributed lag non-linear models (DLNMs) represent a modeling framework to ﬂexibly describe associations showing potentially non-linear and delayed eﬀects in time series data. This methodology rests on the deﬁnition of a crossbasis, a bi-dimensional functional space expressed by the combination of two sets of basis functions, which specify the relationships in the dimensions of predictor and lags, respectively. This framework is implemented in the R package dlnm, which provides functions to perform the broad range of models within the DLNM family and then to help interpret the results, with an emphasis on graphical representation. This paper oﬀers an overview of the capabilities of the package, describing the conceptual and practical steps to specify and interpret DLNMs with an example of application to real data.},
	language = {en},
	number = {8},
	urldate = {2019-10-31},
	journal = {Journal of Statistical Software},
	author = {Gasparrini, Antonio},
	year = {2011},
	file = {Gasparrini - 2011 - Distributed Lag Linear and Non-Linear Models in i.pdf:/mnt/data/Google Drive/Zotero/storage/J4FFTYLL/Gasparrini - 2011 - Distributed Lag Linear and Non-Linear Models in i.pdf:application/pdf}
}

@article{funkRealtimeForecastingInfectious2018,
	title = {Real-time forecasting of infectious disease dynamics with a stochastic semi-mechanistic model},
	volume = {22},
	issn = {1755-4365},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5871642/},
	doi = {10.1016/j.epidem.2016.11.003},
	abstract = {•
              A Bayesian semi-mechanistic model was applied to the Ebola Forecasting Challenge.
            
            
              •
              Model fits to simulated data were obtained from particle Markov-chain Monte Carlo.
            
            
              •
              Posterior samples of model parameters were used to generate forecast trajectories.
            
            
              •
              The forecasts were assessed using subsequently released simulation points.
            
          
        , Real-time forecasts of infectious diseases can help public health planning, especially during outbreaks. If forecasts are generated from mechanistic models, they can be further used to target resources or to compare the impact of possible interventions. However, paremeterising such models is often difficult in real time, when information on behavioural changes, interventions and routes of transmission are not readily available. Here, we present a semi-mechanistic model of infectious disease dynamics that was used in real time during the 2013–2016 West African Ebola epidemic, and show fits to a Ebola Forecasting Challenge conducted in late 2015 with simulated data mimicking the true epidemic. We assess the performance of the model in different situations and identify strengths and shortcomings of our approach. Models such as the one presented here which combine the power of mechanistic models with the flexibility to include uncertainty about the precise outbreak dynamics may be an important tool in combating future outbreaks.},
	urldate = {2019-11-01},
	journal = {Epidemics},
	author = {Funk, Sebastian and Camacho, Anton and Kucharski, Adam J. and Eggo, Rosalind M. and Edmunds, W. John},
	month = mar,
	year = {2018},
	pmid = {28038870},
	pmcid = {PMC5871642},
	pages = {56--61},
	file = {Full Text:/mnt/data/Google Drive/Zotero/storage/E3U9WS5C/Funk et al. - 2018 - Real-time forecasting of infectious disease dynami.pdf:application/pdf}
}

@article{coriNewFrameworkSoftware2013a,
	title = {A {New} {Framework} and {Software} to {Estimate} {Time}-{Varying} {Reproduction} {Numbers} {During} {Epidemics}},
	volume = {178},
	issn = {0002-9262},
	url = {https://academic.oup.com/aje/article/178/9/1505/89262},
	doi = {10.1093/aje/kwt133},
	abstract = {Abstract.  The quantification of transmissibility during epidemics is essential to designing and adjusting public health responses. Transmissibility can be meas},
	language = {en},
	number = {9},
	urldate = {2019-11-01},
	journal = {American Journal of Epidemiology},
	author = {Cori, Anne and Ferguson, Neil M. and Fraser, Christophe and Cauchemez, Simon},
	month = nov,
	year = {2013},
	pages = {1505--1512},
	file = {Full Text:/mnt/data/Google Drive/Zotero/storage/V5NSW7DF/Cori et al. - 2013 - A New Framework and Software to Estimate Time-Vary.pdf:application/pdf;kwt133supp.doc:/mnt/data/Google Drive/Zotero/storage/3HRUA5K9/kwt133supp.doc:application/msword;Snapshot:/mnt/data/Google Drive/Zotero/storage/FMCT4D4Z/governor.html:text/html}
}

@misc{linkFittingBayesianStructural,
	title = {Fitting {Bayesian} structural time series with the bsts {R} package},
	url = {http://www.unofficialgoogledatascience.com/2017/07/fitting-bayesian-structural-time-series.html},
	abstract = {by STEVEN L. SCOTT   Time series data are everywhere, but time series modeling is a fairly specialized area within statistics and data scien...},
	urldate = {2019-11-04},
	author = {link, Get and Facebook and Twitter and Pinterest and Email and Apps, Other},
	file = {Snapshot:/mnt/data/Google Drive/Zotero/storage/6I3IP6MX/fitting-bayesian-structural-time-series.html:text/html}
}

@book{arnoldStateSpaceModels,
	title = {State {Space} {Models} in {Stan}},
	url = {https://jrnold.github.io/ssmodels-in-stan/index.html},
	abstract = {Documentation for State Space Models in Stan.},
	urldate = {2019-11-05},
	author = {Arnold, Jeffrey B.},
	file = {Snapshot:/mnt/data/Google Drive/Zotero/storage/5VIFYEF5/index.html:text/html}
}

@article{chatzilenaContemporaryStatisticalInference2019,
	title = {Contemporary statistical inference for infectious disease models using {Stan}},
	issn = {1755-4365},
	url = {http://www.sciencedirect.com/science/article/pii/S1755436519300325},
	doi = {10.1016/j.epidem.2019.100367},
	abstract = {This paper is concerned with the application of recent statistical advances to inference of infectious disease dynamics. We describe the fitting of a class of epidemic models using Hamiltonian Monte Carlo and variational inference as implemented in the freely available Stan software. We apply the two methods to real data from outbreaks as well as routinely collected observations. Our results suggest that both inference methods are computationally feasible in this context, and show a trade-off between statistical efficiency versus computational speed. The latter appears particularly relevant for real-time applications.},
	language = {en},
	urldate = {2019-11-06},
	journal = {Epidemics},
	author = {Chatzilena, Anastasia and van Leeuwen, Edwin and Ratmann, Oliver and Baguelin, Marc and Demiris, Nikolaos},
	month = oct,
	year = {2019},
	keywords = {Automatic differentiation variational inference, Epidemic models, Hamiltonian Monte Carlo, No-U-turn sampler, Stan},
	pages = {100367},
	file = {ScienceDirect Full Text PDF:/mnt/data/Google Drive/Zotero/storage/VUJUVH3M/Chatzilena et al. - 2019 - Contemporary statistical inference for infectious .pdf:application/pdf;ScienceDirect Snapshot:/mnt/data/Google Drive/Zotero/storage/VPECFYK6/S1755436519300325.html:text/html}
}

@article{luLearningPredictMiRNAmRNA2016,
	title = {Learning to {Predict} {miRNA}-{mRNA} {Interactions} from {AGO} {CLIP} {Sequencing} and {CLASH} {Data}},
	volume = {12},
	issn = {1553-734X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4954643/},
	doi = {10.1371/journal.pcbi.1005026},
	abstract = {Recent technologies like AGO CLIP sequencing and CLASH enable direct transcriptome-wide identification of AGO binding and miRNA target sites, but the most widely used miRNA target prediction algorithms do not exploit these data. Here we use discriminative learning on AGO CLIP and CLASH interactions to train a novel miRNA target prediction model. Our method combines two SVM classifiers, one to predict miRNA-mRNA duplexes and a second to learn a binding model of AGO’s local UTR sequence preferences and positional bias in 3’UTR isoforms. The duplex SVM model enables the prediction of non-canonical target sites and more accurately resolves miRNA interactions from AGO CLIP data than previous methods. The binding model is trained using a multi-task strategy to learn context-specific and common AGO sequence preferences. The duplex and common AGO binding models together outperform existing miRNA target prediction algorithms on held-out binding data. Open source code is available at https://bitbucket.org/leslielab/chimiric., MicroRNAs (or miRNAs) are a family of small RNA molecules that guide Argonaute (AGO) to specific target sites within mRNAs and regulate numerous biological processes in normal cells and in disease. Despite years of research, the principles of miRNA targeting are incompletely understood, and computational miRNA target prediction methods still achieve only modest performance. Most previous target prediction work has been based on indirect measurements of miRNA regulation, such as mRNA expression changes upon miRNA perturbation, without mapping actual binding sites, which limits accuracy and precludes discovery of more subtle miRNA targeting rules. The recent introduction of CLIP (UV crosslinking followed by immunoprecipitation) sequencing technologies enables direct identification of interactions between miRNAs and mRNAs. However, the data generated from these assays has not been fully exploited in target prediction. Here, we present a model to predict miRNA-mRNA interactions solely based on their sequences, using new technologies to map AGO and miRNA binding interactions with machine learning techniques. Our algorithm produces more accurate predictions than state-of-the-art methods based on indirect measurements. Moreover, interpretation of the learned model reveals novel features of miRNA-mRNA interactions, including potential cooperativity with specific RNA-binding proteins.},
	number = {7},
	urldate = {2019-11-06},
	journal = {PLoS Computational Biology},
	author = {Lu, Yuheng and Leslie, Christina S.},
	month = jul,
	year = {2016},
	pmid = {27438777},
	pmcid = {PMC4954643},
	file = {PubMed Central Full Text PDF:/mnt/data/Google Drive/Zotero/storage/J5RBW55G/Lu and Leslie - 2016 - Learning to Predict miRNA-mRNA Interactions from A.pdf:application/pdf}
}

@misc{MetalogDistributions,
	title = {The {Metalog} {Distributions}},
	url = {http://metalogdistributions.com/publications.html},
	urldate = {2019-11-06},
	file = {The Metalog Distributions:/mnt/data/Google Drive/Zotero/storage/37KXJM2V/publications.html:text/html}
}

@article{keelinMetalogDistributions2016,
	title = {The {Metalog} {Distributions}},
	volume = {13},
	issn = {1545-8490, 1545-8504},
	url = {http://pubsonline.informs.org/doi/10.1287/deca.2016.0338},
	doi = {10.1287/deca.2016.0338},
	abstract = {The metalog distributions constitute a new system of continuous univariate probability distributions designed for flexibility, simplicity, and ease/speed of use in practice. The system is comprised of unbounded, semi-bounded, and bounded distributions, each of which offers nearly unlimited shape flexibility compared to Pearson, Johnson, and other traditional systems of distributions. Explicit shapeflexibility comparisons are provided. Unlike other distributions that require non-linear optimization for parameter estimation, the metalog quantile functions and PDFs have simple closed-form expressions that are quantile-parameterized linearly by CDF data. Applications in fish biology and hydrology show how metalogs may aid data and distribution research by imposing fewer shape constraints than other commonly used distributions. Applications in decision analysis show how the metalog system can be specified with three assessed quantiles, how it facilities Monte Carlo simulation, and how applying it aided an actual decision that would have been made wrongly based on commonly-used discrete methods.},
	language = {en},
	number = {4},
	urldate = {2019-11-06},
	journal = {Decision Analysis},
	author = {Keelin, Thomas W.},
	month = dec,
	year = {2016},
	pages = {243--277},
	file = {Keelin - 2016 - The Metalog Distributions.pdf:/mnt/data/Google Drive/Zotero/storage/VC6KP5LD/Keelin - 2016 - The Metalog Distributions.pdf:application/pdf}
}

@article{ibrahimPowerPriorTheory2015,
	title = {The power prior: theory and applications},
	volume = {34},
	issn = {1097-0258},
	shorttitle = {The power prior},
	doi = {10.1002/sim.6728},
	abstract = {The power prior has been widely used in many applications covering a large number of disciplines. The power prior is intended to be an informative prior constructed from historical data. It has been used in clinical trials, genetics, health care, psychology, environmental health, engineering, economics, and business. It has also been applied for a wide variety of models and settings, both in the experimental design and analysis contexts. In this review article, we give an A-to-Z exposition of the power prior and its applications to date. We review its theoretical properties, variations in its formulation, statistical contexts for which it has been used, applications, and its advantages over other informative priors. We review models for which it has been used, including generalized linear models, survival models, and random effects models. Statistical areas where the power prior has been used include model selection, experimental design, hierarchical modeling, and conjugate priors. Frequentist properties of power priors in posterior inference are established, and a simulation study is conducted to further examine the empirical performance of the posterior estimates with power priors. Real data analyses are given illustrating the power prior as well as the use of the power prior in the Bayesian design of clinical trials.},
	language = {eng},
	number = {28},
	journal = {Statistics in Medicine},
	author = {Ibrahim, Joseph G. and Chen, Ming-Hui and Gwon, Yeongjin and Chen, Fang},
	month = dec,
	year = {2015},
	pmid = {26346180},
	pmcid = {PMC4626399},
	keywords = {Research Design, Bayes Theorem, Linear Models, Models, Statistical, Statistics as Topic, Bayesian design, borrowing, clinical trials, Clinical Trials as Topic, discounting, historical data, Historically Controlled Study, informative prior},
	pages = {3724--3749},
	file = {Accepted Version:/mnt/data/Google Drive/Zotero/storage/86JFHRSQ/Ibrahim et al. - 2015 - The power prior theory and applications.pdf:application/pdf}
}

@article{buhlmannStatisticalSignificanceHighdimensional2013,
	title = {Statistical significance in high-dimensional linear models},
	volume = {19},
	issn = {1350-7265},
	url = {http://arxiv.org/abs/1202.1377},
	doi = {10.3150/12-BEJSP11},
	abstract = {We propose a method for constructing p-values for general hypotheses in a high-dimensional linear model. The hypotheses can be local for testing a single regression parameter or they may be more global involving several up to all parameters. Furthermore, when considering many hypotheses, we show how to adjust for multiple testing taking dependence among the p-values into account. Our technique is based on Ridge estimation with an additional correction term due to a substantial projection bias in high dimensions. We prove strong error control for our p-values and provide sufficient conditions for detection: for the former, we do not make any assumption on the size of the true underlying regression coefficients while regarding the latter, our procedure might not be optimal in terms of power. We demonstrate the method in simulated examples and a real data application.},
	language = {en},
	number = {4},
	urldate = {2019-11-19},
	journal = {Bernoulli},
	author = {Bühlmann, Peter},
	month = sep,
	year = {2013},
	note = {arXiv: 1202.1377},
	keywords = {Statistics - Methodology, Mathematics - Statistics Theory},
	pages = {1212--1242},
	annote = {Comment: Published in at http://dx.doi.org/10.3150/12-BEJSP11 the Bernoulli (http://isi.cbs.nl/bernoulli/) by the International Statistical Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)},
	file = {Bühlmann - 2013 - Statistical significance in high-dimensional linea.pdf:/mnt/data/Google Drive/Zotero/storage/YTRN93FG/Bühlmann - 2013 - Statistical significance in high-dimensional linea.pdf:application/pdf}
}

@article{mnihPlayingAtariDeep,
	title = {Playing {Atari} with {Deep} {Reinforcement} {Learning}},
	abstract = {We present the ﬁrst deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We ﬁnd that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
	language = {en},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
	pages = {9},
	file = {Mnih et al. - Playing Atari with Deep Reinforcement Learning.pdf:/mnt/data/Google Drive/Zotero/storage/M372IDGU/Mnih et al. - Playing Atari with Deep Reinforcement Learning.pdf:application/pdf}
}

@article{christianoDeepReinforcementLearning2017,
	title = {Deep reinforcement learning from human preferences},
	url = {http://arxiv.org/abs/1706.03741},
	abstract = {For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.},
	urldate = {2019-12-05},
	journal = {arXiv:1706.03741 [cs, stat]},
	author = {Christiano, Paul and Leike, Jan and Brown, Tom B. and Martic, Miljan and Legg, Shane and Amodei, Dario},
	month = jul,
	year = {2017},
	note = {arXiv: 1706.03741},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
	file = {arXiv Fulltext PDF:/mnt/data/Google Drive/Zotero/storage/7I5CFBB6/Christiano et al. - 2017 - Deep reinforcement learning from human preferences.pdf:application/pdf;arXiv.org Snapshot:/mnt/data/Google Drive/Zotero/storage/73CK9WFQ/1706.html:text/html}
}

@article{christianoSupervisingStrongLearners2018,
	title = {Supervising strong learners by amplifying weak experts},
	url = {http://arxiv.org/abs/1810.08575},
	abstract = {Many real world learning tasks involve complex or hard-to-specify objectives, and using an easier-to-specify proxy can lead to poor performance or misaligned behavior. One solution is to have humans provide a training signal by demonstrating or judging performance, but this approach fails if the task is too complicated for a human to directly evaluate. We propose Iterated Ampliﬁcation, an alternative training strategy which progressively builds up a training signal for difﬁcult problems by combining solutions to easier subproblems. Iterated Ampliﬁcation is closely related to Expert Iteration (Anthony et al., 2017; Silver et al., 2017b), except that it uses no external reward function. We present results in algorithmic environments, showing that Iterated Ampliﬁcation can efﬁciently learn complex behaviors.},
	language = {en},
	urldate = {2019-12-05},
	journal = {arXiv:1810.08575 [cs, stat]},
	author = {Christiano, Paul and Shlegeris, Buck and Amodei, Dario},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.08575},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Christiano et al. - 2018 - Supervising strong learners by amplifying weak exp.pdf:/mnt/data/Google Drive/Zotero/storage/AGR998GI/Christiano et al. - 2018 - Supervising strong learners by amplifying weak exp.pdf:application/pdf}
}

@article{amodeiConcreteProblemsAI2016,
	title = {Concrete {Problems} in {AI} {Safety}},
	url = {http://arxiv.org/abs/1606.06565},
	abstract = {Rapid progress in machine learning and artiﬁcial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, deﬁned as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of ﬁve practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function (“avoiding side eﬀects” and “avoiding reward hacking”), an objective function that is too expensive to evaluate frequently (“scalable supervision”), or undesirable behavior during the learning process (“safe exploration” and “distributional shift”). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.},
	language = {en},
	urldate = {2019-12-06},
	journal = {arXiv:1606.06565 [cs]},
	author = {Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Mané, Dan},
	month = jul,
	year = {2016},
	note = {arXiv: 1606.06565},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: 29 pages},
	file = {Amodei et al. - 2016 - Concrete Problems in AI Safety.pdf:/mnt/data/Google Drive/Zotero/storage/2FP42T6I/Amodei et al. - 2016 - Concrete Problems in AI Safety.pdf:application/pdf}
}

@article{zhangVariableSelectionProcedures2019,
	title = {Variable selection procedures from multiple testing},
	volume = {62},
	issn = {1674-7283, 1869-1862},
	url = {http://link.springer.com/10.1007/s11425-016-9186-x},
	doi = {10.1007/s11425-016-9186-x},
	abstract = {Variable selection has played an important role in statistical learning and scientiﬁc discoveries during the past ten years, and multiple testing is a fundamental problem in statistical inference and also has wide applications in many scientiﬁc ﬁelds. Signiﬁcant advances have been achieved in both areas. This study attempts to ﬁnd a connection between the adaptive LASSO (least absolute shrinkage and selection operator) and multiple testing procedures in linear regression models. We also propose procedures based on multiple testing methods to select variables and control the selection error rate, i.e., the false discovery rate. Simulation studies demonstrate that the proposed methods show good performance relative to controlling the selection error rate under a wide range of settings.},
	language = {en},
	number = {4},
	urldate = {2020-01-13},
	journal = {Science China Mathematics},
	author = {Zhang, Baoxue and Cheng, Guanghui and Zhang, Chunming and Zheng, Shurong},
	month = apr,
	year = {2019},
	pages = {771--782},
	file = {Zhang et al. - 2019 - Variable selection procedures from multiple testin.pdf:/mnt/data/Google Drive/Zotero/storage/5VXXSYUD/Zhang et al. - 2019 - Variable selection procedures from multiple testin.pdf:application/pdf}
}

@article{kneibNikosBosseFelix,
	title = {Nikos {Bosse} {Felix} {Süttmann}},
	language = {en},
	author = {Kneib, Thomas and Röder, Jan},
	pages = {56},
	file = {Kneib and Röder - Nikos Bosse Felix Süttmann.pdf:/mnt/data/Google Drive/Zotero/storage/LBNBMGPU/Kneib and Röder - Nikos Bosse Felix Süttmann.pdf:application/pdf}
}

@article{IBMSPSSAdvanced,
	title = {{IBM} {SPSS} {Advanced} {Statistics} 25},
	language = {de},
	pages = {134},
	file = {IBM SPSS Advanced Statistics 25.pdf:/mnt/data/Google Drive/Zotero/storage/R69D2Z8Q/IBM SPSS Advanced Statistics 25.pdf:application/pdf}
}

@article{gabryVisualizationBayesianWorkflow2019,
	title = {Visualization in {Bayesian} workflow},
	volume = {182},
	issn = {09641998},
	url = {http://arxiv.org/abs/1709.01449},
	doi = {10.1111/rssa.12378},
	abstract = {Bayesian data analysis is about more than just computing a posterior distribution, and Bayesian visualization is about more than trace plots of Markov chains. Practical Bayesian data analysis, like all data analysis, is an iterative process of model building, inference, model checking and evaluation, and model expansion. Visualization is helpful in each of these stages of the Bayesian workﬂow and it is indispensable when drawing inferences from the types of modern, high-dimensional models that are used by applied researchers.},
	language = {en},
	number = {2},
	urldate = {2020-01-13},
	journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
	author = {Gabry, Jonah and Simpson, Daniel and Vehtari, Aki and Betancourt, Michael and Gelman, Andrew},
	month = feb,
	year = {2019},
	note = {arXiv: 1709.01449},
	keywords = {Statistics - Methodology, Statistics - Applications},
	pages = {389--402},
	annote = {Comment: 17 pages, 11 Figures. Includes supplementary material},
	file = {Gabry et al. - 2019 - Visualization in Bayesian workflow.pdf:/mnt/data/Google Drive/Zotero/storage/B6BPIY9I/Gabry et al. - 2019 - Visualization in Bayesian workflow.pdf:application/pdf}
}

@book{arnoldStateSpaceModelsa,
	title = {State {Space} {Models} in {Stan}},
	url = {https://jrnold.github.io/ssmodels-in-stan/stan-functions.html#simulation-smoothers-1},
	abstract = {Documentation for State Space Models in Stan.},
	urldate = {2020-01-13},
	author = {Arnold, Jeffrey B.},
	file = {Snapshot:/mnt/data/Google Drive/Zotero/storage/QMZCUAWY/stan-functions.html:text/html}
}

@misc{linkFittingBayesianStructurala,
	title = {Fitting {Bayesian} structural time series with the bsts {R} package},
	url = {http://www.unofficialgoogledatascience.com/2017/07/fitting-bayesian-structural-time-series.html},
	abstract = {by STEVEN L. SCOTT   Time series data are everywhere, but time series modeling is a fairly specialized area within statistics and data scien...},
	urldate = {2020-01-13},
	author = {link, Get and Facebook and Twitter and Pinterest and Email and Apps, Other},
	file = {Snapshot:/mnt/data/Google Drive/Zotero/storage/RI92BKFZ/fitting-bayesian-structural-time-series.html:text/html}
}

@misc{leeGeoslegendDeepLearningforSpatiotemporalPrediction2020,
	title = {geoslegend/{Deep}-{Learning}-for-{Spatio}-temporal-{Prediction}},
	url = {https://github.com/geoslegend/Deep-Learning-for-Spatio-temporal-Prediction},
	abstract = {Contribute to geoslegend/Deep-Learning-for-Spatio-temporal-Prediction development by creating an account on GitHub.},
	urldate = {2020-01-13},
	author = {Lee, Seongkyu},
	month = jan,
	year = {2020},
	note = {original-date: 2018-11-01T02:02:00Z}
}

@misc{EbolaCasesDeaths,
	title = {Ebola {Cases} and {Deaths} in the {North} {Kivu} {Ebola} {Outbreak} in the {Democratic} {Republic} of the {Congo} ({DRC}) - {Humanitarian} {Data} {Exchange}},
	url = {https://data.humdata.org/dataset/ebola-cases-and-deaths-drc-north-kivu},
	urldate = {2020-01-13},
	file = {Ebola Cases and Deaths in the North Kivu Ebola Outbreak in the Democratic Republic of the Congo (DRC) - Humanitarian Data Exchange:/mnt/data/Google Drive/Zotero/storage/TBIT49SJ/ebola-cases-and-deaths-drc-north-kivu.html:text/html}
}

@misc{roGulfaMscEbola2019,
	title = {Gulfa/msc\_ebola},
	copyright = {GPL-3.0},
	url = {https://github.com/Gulfa/msc_ebola},
	abstract = {Ebola modelling MSC project. Contribute to Gulfa/msc\_ebola development by creating an account on GitHub.},
	urldate = {2020-01-13},
	author = {Ro, Gunnar},
	month = oct,
	year = {2019},
	note = {original-date: 2019-04-14T11:20:17Z}
}

@article{meyerHhh4EndemicepidemicModelinga,
	title = {hhh4: {Endemic}-epidemic modeling of areal count time series},
	abstract = {The availability of geocoded health data and the inherent temporal structure of communicable diseases have led to an increased interest in statistical models and software for spatio-temporal data with epidemic features. The R package surveillance can handle various levels of aggregation at which infective events have been recorded. This vignette illustrates the analysis of area-level time series of counts using the endemic-epidemic multivariate time-series model “hhh4” described in, e.g., Meyer and Held (2014, Section 3). See vignette("hhh4") for a more general introduction to hhh4 models, including the univariate and non-spatial bivariate case. We ﬁrst describe the general modeling approach and then exemplify data handling, model ﬁtting, visualization, and simulation methods for weekly counts of measles infections by district in the Weser-Ems region of Lower Saxony, Germany, 2001–2002.},
	language = {en},
	author = {Meyer, Sebastian and Held, Leonhard and Höhle, Michael},
	pages = {23},
	file = {Meyer et al. - hhh4 Endemic-epidemic modeling of areal count tim.pdf:/mnt/data/Google Drive/Zotero/storage/2BBGK8KT/Meyer et al. - hhh4 Endemic-epidemic modeling of areal count tim.pdf:application/pdf}
}

@misc{QuickNoteWhat2018,
	title = {A quick note what {I} infer from p\_loo and {Pareto} k values},
	url = {https://discourse.mc-stan.org/t/a-quick-note-what-i-infer-from-p-loo-and-pareto-k-values/3446/2},
	abstract = {Nice!  A quick question: How do you define \# of parameters in hierarchical models? Does the \# of poarameters in this setting depend on a sparse (very few parameters due to pooling) and a dense (many parameters) data situation?  It would be nice to discuss this bit specifically in the case study given how ubiquotus hierarchical models are used.},
	language = {en-US},
	urldate = {2020-01-14},
	journal = {The Stan Forums},
	month = mar,
	year = {2018},
	file = {Snapshot:/mnt/data/Google Drive/Zotero/storage/Z4AL42QY/3446.html:text/html}
}

@article{wellsExacerbationEbolaOutbreaks2019a,
	title = {The exacerbation of {Ebola} outbreaks by conflict in the {Democratic} {Republic} of the {Congo}},
	volume = {116},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1913980116},
	doi = {10.1073/pnas.1913980116},
	abstract = {The interplay between civil unrest and disease transmission is not well understood. Violence targeting healthcare workers and Ebola treatment centers in the Democratic Republic of the Congo (DRC) has been thwarting the case isolation, treatment, and vaccination efforts. The extent to which conflict impedes public health response and contributes to incidence has not previously been evaluated. We construct a timeline of conflict events throughout the course of the epidemic and provide an ethnographic appraisal of the local conditions that preceded and followed conflict events. Informed by temporal incidence and conflict data as well as the ethnographic evidence, we developed a model of Ebola transmission and control to assess the impact of conflict on the epidemic in the eastern DRC from April 30, 2018, to June 23, 2019. We found that both the rapidity of case isolation and the population-level effectiveness of vaccination varied notably as a result of preceding unrest and subsequent impact of conflict events. Furthermore, conflict events were found to reverse an otherwise declining phase of the epidemic trajectory. Our model framework can be extended to other infectious diseases in the same and other regions of the world experiencing conflict and violence.},
	language = {en},
	number = {48},
	urldate = {2020-01-16},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Wells, Chad R. and Pandey, Abhishek and Ndeffo Mbah, Martial L. and Gaüzère, Bernard-A. and Malvy, Denis and Singer, Burton H. and Galvani, Alison P.},
	month = nov,
	year = {2019},
	pages = {24366--24372},
	file = {Wells et al. - 2019 - The exacerbation of Ebola outbreaks by conflict in.pdf:/mnt/data/Google Drive/Zotero/storage/4GPMKJMW/Wells et al. - 2019 - The exacerbation of Ebola outbreaks by conflict in.pdf:application/pdf}
}

@article{demetrescuBiasCorrectionsExponentially,
	title = {Bias {Corrections} for {Exponentially} {Transformed} {Forecasts}: is it worth the eﬀort?},
	abstract = {In many economic applications the log transformation of the process of interest allows to model and to forecast log values as linear time series. However, a reverse transformation of the log forecasts introduces a bias which should accounted for. In this paper we compare diﬀerent bias correction methods for the reverse transformation of log series following a linear autoregressive process. We ﬁnd that the correction method to choose in ﬁnite samples depends much on the empirical error distribution whereby for some cases no bias correction is advantageous. Our results are illustrated both in Monte Carlo simulations and in an empirical study.},
	language = {en},
	author = {Demetrescu, Matei and Kiel, CAU and Golosnoy, Vasyl and Bochum, Ruhr-University},
	pages = {22},
	file = {Demetrescu et al. - Bias Corrections for Exponentially Transformed For.pdf:/mnt/data/Google Drive/Zotero/storage/Q7WTUXP3/Demetrescu et al. - Bias Corrections for Exponentially Transformed For.pdf:application/pdf}
}

@article{kucharskiEffectivenessRingVaccination,
	title = {Effectiveness of {Ring} {Vaccination} as {Control} {Strategy} for {Ebola} {Virus} {Disease} - {Volume} 22, {Number} 1—{January} 2016 - {Emerging} {Infectious} {Diseases} journal - {CDC}},
	url = {https://wwwnc.cdc.gov/eid/article/22/1/15-1410_article},
	doi = {10.3201/eid2201.151410},
	abstract = {Using an Ebola virus disease transmission model, we found that addition of ring vaccination at the outset of the West Africa epidemic might not have l...},
	language = {en-us},
	urldate = {2020-01-27},
	author = {Kucharski, Adam J. and Eggo, Rosalind M. and Watson, Conall and Camacho, Anton and Funk, Sebastian and Edmunds, W. John},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/GABQRTK5/Kucharski et al. - Effectiveness of Ring Vaccination as Control Strat.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/GKN58IHT/15-1410_article.html:text/html}
}

@article{yangDurationUrinationDoes2014,
	title = {Duration of urination does not change with body size},
	volume = {111},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/111/33/11932},
	doi = {10.1073/pnas.1402289111},
	abstract = {Many urological studies rely on models of animals, such as rats and pigs, but their relation to the human urinary system is poorly understood. Here, we elucidate the hydrodynamics of urination across five orders of magnitude in body mass. Using high-speed videography and flow-rate measurement obtained at Zoo Atlanta, we discover that all mammals above 3 kg in weight empty their bladders over nearly constant duration of 21 ± 13 s. This feat is possible, because larger animals have longer urethras and thus, higher gravitational force and higher flow speed. Smaller mammals are challenged during urination by high viscous and capillary forces that limit their urine to single drops. Our findings reveal that the urethra is a flow-enhancing device, enabling the urinary system to be scaled up by a factor of 3,600 in volume without compromising its function. This study may help to diagnose urinary problems in animals as well as inspire the design of scalable hydrodynamic systems based on those in nature.},
	language = {en},
	number = {33},
	urldate = {2020-01-27},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Yang, Patricia J. and Pham, Jonathan and Choo, Jerome and Hu, David L.},
	month = aug,
	year = {2014},
	pmid = {24969420},
	keywords = {allometry, Bernoulli's principle, scaling, urology},
	pages = {11932--11937},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/XSC7XIRG/Yang et al. - 2014 - Duration of urination does not change with body si.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/R58JV3WZ/11932.html:text/html}
}

@article{jombartCostInsecurityFlareup2020,
	title = {The cost of insecurity: from flare-up to control of a major {Ebola} virus disease hotspot during the outbreak in the {Democratic} {Republic} of the {Congo}, 2019},
	volume = {25},
	issn = {1560-7917},
	shorttitle = {The cost of insecurity},
	url = {https://www.eurosurveillance.org/content/10.2807/1560-7917.ES.2020.25.2.1900735},
	doi = {10.2807/1560-7917.ES.2020.25.2.1900735},
	abstract = {The ongoing Ebola outbreak in the eastern Democratic Republic of the Congo is facing unprecedented levels of insecurity and violence. We evaluate the likely impact in terms of added transmissibility and cases of major security incidents in the Butembo coordination hub. We also show that despite this additional burden, an adapted response strategy involving enlarged ring vaccination around clusters of cases and enhanced community engagement managed to bring this main hotspot under control.},
	language = {en},
	number = {2},
	urldate = {2020-01-28},
	journal = {Eurosurveillance},
	author = {Jombart, Thibaut and Jarvis, Christopher I. and Mesfin, Samuel and Tabal, Nabil and Mossoko, Mathias and Mpia, Luigino Minikulu and Abedi, Aaron Aruna and Chene, Sonia and Forbin, Ekokobe Elias and Belizaire, Marie Roseline D. and Radiguès, Xavier de and Ngombo, Richy and Tutu, Yannick and Finger, Flavio and Crowe, Madeleine and Edmunds, W. John and Nsio, Justus and Yam, Abdoulaye and Diallo, Boubacar and Gueye, Abdou Salam and Ahuka-Mundeke, Steve and Yao, Michel and Fall, Ibrahima Socé},
	month = jan,
	year = {2020},
	pages = {1900735},
	file = {Full Text:/mnt/data/Google Drive/Zotero/storage/PZWFGHLK/Jombart et al. - 2020 - The cost of insecurity from flare-up to control o.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/U2AUSPSS/1560-7917.ES.2020.25.2.html:text/html}
}

@article{gasparriniDistributedLagLinear,
	title = {Distributed lag linear and non-linear models for time series data},
	language = {en},
	author = {Gasparrini, Antonio},
	pages = {12},
	file = {Gasparrini - Distributed lag linear and non-linear models for t.pdf:/mnt/data/Google Drive/Zotero/storage/HAUGWLZV/Gasparrini - Distributed lag linear and non-linear models for t.pdf:application/pdf}
}

@article{betancourtDiagnosingSuboptimalCotangent2016,
	title = {Diagnosing {Suboptimal} {Cotangent} {Disintegrations} in {Hamiltonian} {Monte} {Carlo}},
	url = {http://arxiv.org/abs/1604.00695},
	abstract = {When properly tuned, Hamiltonian Monte Carlo scales to some of the most challenging high-dimensional problems at the frontiers of applied statistics, but when that tuning is suboptimal the performance leaves much to be desired. In this paper I show how suboptimal choices of one critical degree of freedom, the cotangent disintegration, manifest in readily observed diagnostics that facilitate the robust application of the algorithm.},
	urldate = {2020-01-28},
	journal = {arXiv:1604.00695 [stat]},
	author = {Betancourt, Michael},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.00695},
	keywords = {Statistics - Methodology},
	annote = {Comment: 17 pages, 9 figures},
	file = {arXiv Fulltext PDF:/mnt/data/Google Drive/Zotero/storage/QC7GG6QV/Betancourt - 2016 - Diagnosing Suboptimal Cotangent Disintegrations in.pdf:application/pdf;arXiv.org Snapshot:/mnt/data/Google Drive/Zotero/storage/IHVMNDXE/1604.html:text/html}
}

@misc{BriefGuideStan,
	title = {Brief {Guide} to {Stan}’s {Warnings}},
	url = {https://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded},
	urldate = {2020-01-28},
	file = {Brief Guide to Stan’s Warnings:/mnt/data/Google Drive/Zotero/storage/YDGRAKPL/warnings.html:text/html}
}

@article{lindenUsingNegativeBinomial2011,
	title = {Using the negative binomial distribution to model overdispersion in ecological count data},
	volume = {92},
	issn = {0012-9658},
	url = {https://esajournals.onlinelibrary.wiley.com/doi/10.1890/10-1831.1},
	doi = {10.1890/10-1831.1},
	abstract = {A Poisson process is a commonly used starting point for modeling stochastic variation of ecological count data around a theoretical expectation. However, data typically show more variation than implied by the Poisson distribution. Such overdispersion is often accounted for by using models with different assumptions about how the variance changes with the expectation. The choice of these assumptions can naturally have apparent consequences for statistical inference. We propose a parameterization of the negative binomial distribution, where two overdispersion parameters are introduced to allow for various quadratic mean?variance relationships, including the ones assumed in the most commonly used approaches. Using bird migration as an example, we present hypothetical scenarios on how overdispersion can arise due to sampling, flocking behavior or aggregation, environmental variability, or combinations of these factors. For all considered scenarios, mean?variance relationships can be appropriately described by the negative binomial distribution with two overdispersion parameters. To illustrate, we apply the model to empirical migration data with a high level of overdispersion, gaining clearly different model fits with different assumptions about mean?variance relationships. The proposed framework can be a useful approximation for modeling marginal distributions of independent count data in likelihood-based analyses.},
	number = {7},
	urldate = {2020-01-29},
	journal = {Ecology},
	author = {Lindén, Andreas and Mäntyniemi, Samu},
	month = jul,
	year = {2011},
	keywords = {bird migration, count data, environmental stochasticity, flocking, generalized linear models, mean–variance relationship, negative binomial distribution, overdispersion, Poisson process, sampling error},
	pages = {1414--1421},
	file = {Snapshot:/mnt/data/Google Drive/Zotero/storage/8ITCFURB/10-1831.html:text/html}
}

@article{marzXGBoostLSSExtensionXGBoost2019,
	title = {{XGBoostLSS} -- {An} extension of {XGBoost} to probabilistic forecasting},
	url = {http://arxiv.org/abs/1907.03178},
	abstract = {We propose a new framework of XGBoost that predicts the entire conditional distribution of a univariate response variable. In particular, XGBoostLSS models all moments of a parametric distribution (i.e., mean, location, scale and shape [LSS]) instead of the conditional mean only. Choosing from a wide range of continuous, discrete and mixed discrete-continuous distribution, modelling and predicting the entire conditional distribution greatly enhances the ﬂexibility of XGBoost, as it allows to gain additional insight into the data generating process, as well as to create probabilistic forecasts from which prediction intervals and quantiles of interest can be derived. We present both a simulation study and real world examples that demonstrate the virtues of our approach.},
	language = {en},
	urldate = {2020-02-06},
	journal = {arXiv:1907.03178 [cs, stat]},
	author = {März, Alexander},
	month = aug,
	year = {2019},
	note = {arXiv: 1907.03178},
	keywords = {Statistics - Methodology, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: Bayesian Optimization; Distributional Modeling; Expectile Regression; GAMLSS; Probabilistic Forecast; Uncertainty Quantification; XGBoost},
	file = {März - 2019 - XGBoostLSS -- An extension of XGBoost to probabili.pdf:/mnt/data/Google Drive/Zotero/storage/NYKHUUHG/März - 2019 - XGBoostLSS -- An extension of XGBoost to probabili.pdf:application/pdf}
}

@misc{funkSbfnkEbolaForecast2020,
	title = {sbfnk/ebola.forecast.wa.sl},
	url = {https://github.com/sbfnk/ebola.forecast.wa.sl},
	abstract = {Code accompanying the manuscript \&quot;Assessing the performance of real-time epidemic forecasts: A case study of Ebola  in the Western Area region of Sierra Leone, 2014-15\&quot; (doi:10.1371/journ...},
	urldate = {2020-02-12},
	author = {Funk, Sebastian},
	month = jan,
	year = {2020},
	note = {original-date: 2018-10-05T06:34:34Z}
}

@article{czadoPredictiveModelAssessment2009,
	title = {Predictive model assessment for count data},
	volume = {65},
	issn = {1541-0420},
	doi = {10.1111/j.1541-0420.2009.01191.x},
	abstract = {We discuss tools for the evaluation of probabilistic forecasts and the critique of statistical models for count data. Our proposals include a nonrandomized version of the probability integral transform, marginal calibration diagrams, and proper scoring rules, such as the predictive deviance. In case studies, we critique count regression models for patent data, and assess the predictive performance of Bayesian age-period-cohort models for larynx cancer counts in Germany. The toolbox applies in Bayesian or classical and parametric or nonparametric settings and to any type of ordered discrete outcomes.},
	language = {eng},
	number = {4},
	journal = {Biometrics},
	author = {Czado, Claudia and Gneiting, Tilmann and Held, Leonhard},
	month = dec,
	year = {2009},
	pmid = {19432783},
	keywords = {Biometry, Humans, Regression Analysis, Bayes Theorem, Models, Statistical, Statistics, Nonparametric, Cohort Studies, Germany, Laryngeal Neoplasms},
	pages = {1254--1261}
}

@article{jordanEvaluatingProbabilisticForecasts2019,
	title = {Evaluating {Probabilistic} {Forecasts} with \textbf{{scoringRules}}},
	volume = {90},
	issn = {1548-7660},
	url = {http://www.jstatsoft.org/v90/i12/},
	doi = {10.18637/jss.v090.i12},
	abstract = {Probabilistic forecasts in the form of probability distributions over future events have become popular in several ﬁelds including meteorology, hydrology, economics, and demography. In typical applications, many alternative statistical models and data sources can be used to produce probabilistic forecasts. Hence, evaluating and selecting among competing methods is an important task. The scoringRules package for R provides functionality for comparative evaluation of probabilistic models based on proper scoring rules, covering a wide range of situations in applied work. This paper discusses implementation and usage details, presents case studies from meteorology and economics, and points to the relevant background literature.},
	language = {en},
	number = {12},
	urldate = {2020-02-13},
	journal = {Journal of Statistical Software},
	author = {Jordan, Alexander and Krüger, Fabian and Lerch, Sebastian},
	year = {2019},
	file = {Jordan et al. - 2019 - Evaluating Probabilistic Forecasts with bscoring.pdf:/mnt/data/Google Drive/Zotero/storage/DSYW6QUF/Jordan et al. - 2019 - Evaluating Probabilistic Forecasts with bscoring.pdf:application/pdf}
}

@article{gneitingProbabilisticForecastsCalibration2007,
	title = {Probabilistic forecasts, calibration and sharpness},
	volume = {69},
	issn = {1467-9868},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2007.00587.x},
	doi = {10.1111/j.1467-9868.2007.00587.x},
	abstract = {Summary. Probabilistic forecasts of continuous variables take the form of predictive densities or predictive cumulative distribution functions. We propose a diagnostic approach to the evaluation of predictive performance that is based on the paradigm of maximizing the sharpness of the predictive distributions subject to calibration. Calibration refers to the statistical consistency between the distributional forecasts and the observations and is a joint property of the predictions and the events that materialize. Sharpness refers to the concentration of the predictive distributions and is a property of the forecasts only. A simple theoretical framework allows us to distinguish between probabilistic calibration, exceedance calibration and marginal calibration. We propose and study tools for checking calibration and sharpness, among them the probability integral transform histogram, marginal calibration plots, the sharpness diagram and proper scoring rules. The diagnostic approach is illustrated by an assessment and ranking of probabilistic forecasts of wind speed at the Stateline wind energy centre in the US Pacific Northwest. In combination with cross-validation or in the time series context, our proposal provides very general, nonparametric alternatives to the use of information criteria for model diagnostics and model selection.},
	language = {en},
	number = {2},
	urldate = {2020-02-17},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Gneiting, Tilmann and Balabdaoui, Fadoua and Raftery, Adrian E.},
	year = {2007},
	keywords = {Cross-validation, Density forecast, Ensemble prediction system, Ex post evaluation, Forecast verification, Model diagnostics, Posterior predictive assessment, Predictive distribution, Prequential principle, Probability integral transform, Proper scoring rule},
	pages = {243--268},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/BUWD6CGT/Gneiting et al. - 2007 - Probabilistic forecasts, calibration and sharpness.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/EUCMSBKN/j.1467-9868.2007.00587.html:text/html}
}

@misc{DESeq2TestingRatio,
	title = {{DESeq2} testing ratio of ratios ({RIP}-{Seq}, {CLIP}-{Seq}, ribosomal profiling)},
	url = {https://support.bioconductor.org/p/61509/},
	urldate = {2020-02-21},
	file = {DESeq2 testing ratio of ratios (RIP-Seq, CLIP-Seq, ribosomal profiling):/mnt/data/Google Drive/Zotero/storage/JN9LPDBE/61509.html:text/html}
}

@article{asselBrierScoreDoes2017,
	title = {The {Brier} score does not evaluate the clinical utility of diagnostic tests or prediction models},
	volume = {1},
	issn = {2397-7523},
	url = {https://doi.org/10.1186/s41512-017-0020-3},
	doi = {10.1186/s41512-017-0020-3},
	abstract = {A variety of statistics have been proposed as tools to help investigators assess the value of diagnostic tests or prediction models. The Brier score has been recommended on the grounds that it is a proper scoring rule that is affected by both discrimination and calibration. However, the Brier score is prevalence dependent in such a way that the rank ordering of tests or models may inappropriately vary by prevalence.},
	number = {1},
	urldate = {2020-02-25},
	journal = {Diagnostic and Prognostic Research},
	author = {Assel, Melissa and Sjoberg, Daniel D. and Vickers, Andrew J.},
	month = dec,
	year = {2017},
	pages = {19},
	file = {Full Text:/mnt/data/Google Drive/Zotero/storage/M3IGKA6P/Assel et al. - 2017 - The Brier score does not evaluate the clinical uti.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/LL74JDGX/s41512-017-0020-3.html:text/html}
}

@article{jordanEvaluatingProbabilisticForecasts2019a,
	title = {Evaluating {Probabilistic} {Forecasts} with \textbf{{scoringRules}}},
	volume = {90},
	issn = {1548-7660},
	url = {http://www.jstatsoft.org/v90/i12/},
	doi = {10.18637/jss.v090.i12},
	abstract = {Probabilistic forecasts in the form of probability distributions over future events have become popular in several ﬁelds including meteorology, hydrology, economics, and demography. In typical applications, many alternative statistical models and data sources can be used to produce probabilistic forecasts. Hence, evaluating and selecting among competing methods is an important task. The scoringRules package for R provides functionality for comparative evaluation of probabilistic models based on proper scoring rules, covering a wide range of situations in applied work. This paper discusses implementation and usage details, presents case studies from meteorology and economics, and points to the relevant background literature.},
	language = {en},
	number = {12},
	urldate = {2020-03-07},
	journal = {Journal of Statistical Software},
	author = {Jordan, Alexander and Krüger, Fabian and Lerch, Sebastian},
	year = {2019},
	file = {Jordan et al. - 2019 - Evaluating Probabilistic Forecasts with bscoring.pdf:/mnt/data/Google Drive/Zotero/storage/K5UPEP6Y/Jordan et al. - 2019 - Evaluating Probabilistic Forecasts with bscoring.pdf:application/pdf}
}

@article{yaoUsingStackingAverage2018,
	title = {Using stacking to average {Bayesian} predictive distributions},
	volume = {13},
	issn = {1936-0975},
	url = {http://arxiv.org/abs/1704.02030},
	doi = {10.1214/17-BA1091},
	abstract = {The widely recommended procedure of Bayesian model averaging is ﬂawed in the M-open setting in which the true data-generating process is not one of the candidate models being ﬁt. We take the idea of stacking from the point estimation literature and generalize to the combination of predictive distributions, extending the utility function to any proper scoring rule, using Pareto smoothed importance sampling to eﬃciently compute the required leave-one-out posterior distributions and regularization to get more stability. We compare stacking of predictive distributions to several alternatives: stacking of means, Bayesian model averaging (BMA), pseudo-BMA using AIC-type weighting, and a variant of pseudo-BMA that is stabilized using the Bayesian bootstrap. Based on simulations and real-data applications, we recommend stacking of predictive distributions, with BB-pseudo-BMA as an approximate alternative when computation cost is an issue.},
	language = {en},
	number = {3},
	urldate = {2020-03-10},
	journal = {Bayesian Analysis},
	author = {Yao, Yuling and Vehtari, Aki and Simpson, Daniel and Gelman, Andrew},
	month = sep,
	year = {2018},
	note = {arXiv: 1704.02030},
	keywords = {Statistics - Methodology, Statistics - Computation},
	pages = {917--1007},
	file = {Yao et al. - 2018 - Using stacking to average Bayesian predictive dist.pdf:/mnt/data/Google Drive/Zotero/storage/4J6RFHS6/Yao et al. - 2018 - Using stacking to average Bayesian predictive dist.pdf:application/pdf}
}

@article{macheteEarlyWarningCalibrated2013,
	title = {Early {Warning} with {Calibrated} and {Sharper} {Probabilistic} {Forecasts}},
	volume = {32},
	issn = {1099-131X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/for.2242},
	doi = {10.1002/for.2242},
	abstract = {ABSTRACTGiven a nonlinear model, a probabilistic forecast may be obtained by Monte Carlo simulations. At a given forecast horizon, Monte Carlo simulations yield sets of discrete forecasts, which can be converted to density forecasts. The resulting density forecasts will inevitably be downgraded by model misspecification. In order to enhance the quality of the density forecasts, one can mix them with the unconditional density. This paper examines the value of combining conditional density forecasts with the unconditional density. The findings have positive implications for issuing early warnings in different disciplines including economics and meteorology, but UK inflation forecasts are considered as an example. Copyright © 2012 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {5},
	urldate = {2020-03-20},
	journal = {Journal of Forecasting},
	author = {Machete, Reason L.},
	year = {2013},
	keywords = {calibration, combining forecasts, density forecasts, scoring rule},
	pages = {452--468},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/FHU93V4Z/Machete - 2013 - Early Warning with Calibrated and Sharper Probabil.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/S554FQ63/for.html:text/html}
}

@article{macheteContrastingProbabilisticScoring2012,
	title = {Contrasting {Probabilistic} {Scoring} {Rules}},
	url = {http://arxiv.org/abs/1112.4530},
	abstract = {There are several scoring rules that one can choose from in order to score probabilistic forecasting models or estimate model parameters. Whilst it is generally agreed that proper scoring rules are preferable, there is no clear criterion for preferring one proper scoring rule above another. This manuscript compares and contrasts some commonly used proper scoring rules and provides guidance on scoring rule selection. In particular, it is shown that the logarithmic scoring rule prefers erring with more uncertainty, the spherical scoring rule prefers erring with lower uncertainty, whereas the other scoring rules are indiﬀerent to either option.},
	language = {en},
	urldate = {2020-03-21},
	journal = {arXiv:1112.4530 [math, stat]},
	author = {Machete, Reason Lesego},
	month = jul,
	year = {2012},
	note = {arXiv: 1112.4530},
	keywords = {Mathematics - Statistics Theory, 62B10, 62C05, 62G05, 62G07, 62F99, 62P05, 62P12, 62P20},
	annote = {Comment: 17 pages, 0 figures},
	file = {Machete - 2012 - Contrasting Probabilistic Scoring Rules.pdf:/mnt/data/Google Drive/Zotero/storage/8FYPC3Y4/Machete - 2012 - Contrasting Probabilistic Scoring Rules.pdf:application/pdf}
}

@article{gneitingStrictlyProperScoring2007,
	title = {Strictly {Proper} {Scoring} {Rules}, {Prediction}, and {Estimation}},
	volume = {102},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1198/016214506000001437},
	doi = {10.1198/016214506000001437},
	language = {en},
	number = {477},
	urldate = {2020-03-22},
	journal = {Journal of the American Statistical Association},
	author = {Gneiting, Tilmann and Raftery, Adrian E},
	month = mar,
	year = {2007},
	pages = {359--378},
	file = {Gneiting and Raftery - 2007 - Strictly Proper Scoring Rules, Prediction, and Est.pdf:/mnt/data/Google Drive/Zotero/storage/P599P5ZY/Gneiting and Raftery - 2007 - Strictly Proper Scoring Rules, Prediction, and Est.pdf:application/pdf}
}

@article{munzelExactPairedRank2002,
	title = {An {Exact} {Paired} {Rank} {Test}},
	volume = {44},
	issn = {1521-4036},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/1521-4036%28200207%2944%3A5%3C584%3A%3AAID-BIMJ584%3E3.0.CO%3B2-9},
	doi = {10.1002/1521-4036(200207)44:5<584::AID-BIMJ584>3.0.CO;2-9},
	abstract = {An exact rank test for two dependent samples based on overall mid-ranks is discussed which can be applied to metric as well as to ordinal data. The exact conditional distribution of the test statistic given the observed vector of rank differences is determined. A recursion formula is given as well as a fast shift algorithm in SAS/IML code. Moreover, it is demonstrated that the paired rank test can be more powerful than other tests for paired samples by means of a simulation study. Finally, the test is applied to a psychiatric trial with longitudinal ordinal data.},
	language = {en},
	number = {5},
	urldate = {2020-03-24},
	journal = {Biometrical Journal},
	author = {Munzel, Ullrich and Brunner, Edgar},
	year = {2002},
	keywords = {Ordered categorical data, Ordinal data, Shift algorithm, Sign test, Ties, Wilcoxon signed rank test},
	pages = {584--593},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/ZSSNJHSV/Munzel and Brunner - 2002 - An Exact Paired Rank Test.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/Z68EMMEQ/1521-4036(200207)445584AID-BIMJ5843.0.html:text/html}
}

@article{nouvelletSimpleApproachMeasure2018,
	series = {The {RAPIDD} {Ebola} {Forecasting} {Challenge}},
	title = {A simple approach to measure transmissibility and forecast incidence},
	volume = {22},
	issn = {1755-4365},
	url = {http://www.sciencedirect.com/science/article/pii/S1755436517300245},
	doi = {10.1016/j.epidem.2017.02.012},
	abstract = {Outbreaks of novel pathogens such as SARS, pandemic influenza and Ebola require substantial investments in reactive interventions, with consequent implementation plans sometimes revised on a weekly basis. Therefore, short-term forecasts of incidence are often of high priority. In light of the recent Ebola epidemic in West Africa, a forecasting exercise was convened by a network of infectious disease modellers. The challenge was to forecast unseen “future” simulated data for four different scenarios at five different time points. In a similar method to that used during the recent Ebola epidemic, we estimated current levels of transmissibility, over variable time-windows chosen in an ad hoc way. Current estimated transmissibility was then used to forecast near-future incidence. We performed well within the challenge and often produced accurate forecasts. A retrospective analysis showed that our subjective method for deciding on the window of time with which to estimate transmissibility often resulted in the optimal choice. However, when near-future trends deviated substantially from exponential patterns, the accuracy of our forecasts was reduced. This exercise highlights the urgent need for infectious disease modellers to develop more robust descriptions of processes – other than the widespread depletion of susceptible individuals – that produce non-exponential patterns of incidence.},
	language = {en},
	urldate = {2020-03-29},
	journal = {Epidemics},
	author = {Nouvellet, Pierre and Cori, Anne and Garske, Tini and Blake, Isobel M. and Dorigatti, Ilaria and Hinsley, Wes and Jombart, Thibaut and Mills, Harriet L. and Nedjati-Gilani, Gemma and Van Kerkhove, Maria D. and Fraser, Christophe and Donnelly, Christl A. and Ferguson, Neil M. and Riley, Steven},
	month = mar,
	year = {2018},
	keywords = {Forecasting, Branching process, MCMC, Rapid response, Renewal equation},
	pages = {29--35},
	file = {ScienceDirect Full Text PDF:/mnt/data/Google Drive/Zotero/storage/6NPQQ8AP/Nouvellet et al. - 2018 - A simple approach to measure transmissibility and .pdf:application/pdf;ScienceDirect Snapshot:/mnt/data/Google Drive/Zotero/storage/AY6K9FY5/S1755436517300245.html:text/html}
}

@misc{DownloadRStudio,
	title = {Download {RStudio}},
	url = {https://rstudio.com/products/rstudio/download/},
	abstract = {RStudio is a set of integrated tools designed to help you be more productive with R. It includes a console, syntax-highlighting editor that supports direct code execution, and a variety of robust tools for plotting, viewing history, debugging and managing your workspace.},
	language = {en},
	urldate = {2020-03-29},
	file = {Snapshot:/mnt/data/Google Drive/Zotero/storage/DJTUDCCL/download.html:text/html}
}

@article{brooksNonmechanisticForecastsSeasonal2018,
	title = {Nonmechanistic forecasts of seasonal influenza with iterative one-week-ahead distributions},
	volume = {14},
	issn = {1553-7358},
	url = {https://dx.plos.org/10.1371/journal.pcbi.1006134},
	doi = {10.1371/journal.pcbi.1006134},
	abstract = {Accurate and reliable forecasts of seasonal epidemics of infectious disease can assist in the design of countermeasures and increase public awareness and preparedness. This article describes two main contributions we made recently toward this goal: a novel approach to probabilistic modeling of surveillance time series based on “delta densities”, and an optimization scheme for combining output from multiple forecasting methods into an adaptively weighted ensemble. Delta densities describe the probability distribution of the change between one observation and the next, conditioned on available data; chaining together nonparametric estimates of these distributions yields a model for an entire trajectory. Corresponding distributional forecasts cover more observed events than alternatives that treat the whole season as a unit, and improve upon multiple evaluation metrics when extracting key targets of interest to public health officials. Adaptively weighted ensembles integrate the results of multiple forecasting methods, such as delta density, using weights that can change from situation to situation. We treat selection of optimal weightings across forecasting methods as a separate estimation task, and describe an estimation procedure based on optimizing cross-validation performance. We consider some details of the data generation process, including data revisions and holiday effects, both in the construction of these forecasting methods and when performing retrospective evaluation. The delta density method and an adaptively weighted ensemble of other forecasting methods each improve significantly on the next best ensemble component when applied separately, and achieve even better cross-validated performance when used in conjunction. We submitted real-time forecasts based on these contributions as part of CDC’s 2015/2016 FluSight Collaborative Comparison. Among the fourteen submissions that season, this system was ranked by CDC as the most accurate.},
	language = {en},
	number = {6},
	urldate = {2020-03-29},
	journal = {PLOS Computational Biology},
	author = {Brooks, Logan C. and Farrow, David C. and Hyun, Sangwon and Tibshirani, Ryan J. and Rosenfeld, Roni},
	editor = {Viboud, Cecile},
	month = jun,
	year = {2018},
	pages = {e1006134},
	file = {Brooks et al. - 2018 - Nonmechanistic forecasts of seasonal influenza wit.pdf:/mnt/data/Google Drive/Zotero/storage/KWJ8KSUS/Brooks et al. - 2018 - Nonmechanistic forecasts of seasonal influenza wit.pdf:application/pdf}
}

@article{sadhanalaAdditiveModelsTrend2019,
	title = {Additive models with trend filtering},
	volume = {47},
	issn = {0090-5364},
	url = {https://projecteuclid.org/euclid.aos/1572487382},
	doi = {10.1214/19-AOS1833},
	abstract = {We study additive models built with trend ﬁltering, i.e., additive models whose components are each regularized by the (discrete) total variation of their kth (discrete) derivative, for a chosen integer k ≥ 0. This results in kth degree piecewise polynomial components, (e.g., k = 0 gives piecewise constant components, k = 1 gives piecewise linear, k = 2 gives piecewise quadratic, etc.). Analogous to its advantages in the univariate case, additive trend ﬁltering has favorable theoretical and computational properties, thanks in large part to the localized nature of the (discrete) total variation regularizer that it uses. On the theory side, we derive fast error rates for additive trend ﬁltering estimates, and show these rates are minimax optimal when the underlying function is additive and has component functions whose derivatives are of bounded variation. We also show that these rates are unattainable by additive smoothing splines (and by additive models built from linear smoothers, in general). On the computational side, we use backﬁtting, to leverage fast univariate trend ﬁltering solvers; we also describe a new backﬁtting algorithm whose iterations can be run in parallel, which (as far as we can tell) is the ﬁrst of its kind. Lastly, we present a number of experiments to examine the empirical performance of trend ﬁltering.},
	language = {en},
	number = {6},
	urldate = {2020-03-29},
	journal = {The Annals of Statistics},
	author = {Sadhanala, Veeranjaneyulu and Tibshirani, Ryan J.},
	month = dec,
	year = {2019},
	pages = {3032--3068},
	file = {Sadhanala and Tibshirani - 2019 - Additive models with trend filtering.pdf:/mnt/data/Google Drive/Zotero/storage/2Y3KRKHQ/Sadhanala and Tibshirani - 2019 - Additive models with trend filtering.pdf:application/pdf}
}

@article{brooksNonmechanisticForecastsSeasonal2018a,
	title = {Nonmechanistic forecasts of seasonal influenza with iterative one-week-ahead distributions},
	volume = {14},
	issn = {1553-7358},
	url = {https://dx.plos.org/10.1371/journal.pcbi.1006134},
	doi = {10.1371/journal.pcbi.1006134},
	abstract = {Accurate and reliable forecasts of seasonal epidemics of infectious disease can assist in the design of countermeasures and increase public awareness and preparedness. This article describes two main contributions we made recently toward this goal: a novel approach to probabilistic modeling of surveillance time series based on “delta densities”, and an optimization scheme for combining output from multiple forecasting methods into an adaptively weighted ensemble. Delta densities describe the probability distribution of the change between one observation and the next, conditioned on available data; chaining together nonparametric estimates of these distributions yields a model for an entire trajectory. Corresponding distributional forecasts cover more observed events than alternatives that treat the whole season as a unit, and improve upon multiple evaluation metrics when extracting key targets of interest to public health officials. Adaptively weighted ensembles integrate the results of multiple forecasting methods, such as delta density, using weights that can change from situation to situation. We treat selection of optimal weightings across forecasting methods as a separate estimation task, and describe an estimation procedure based on optimizing cross-validation performance. We consider some details of the data generation process, including data revisions and holiday effects, both in the construction of these forecasting methods and when performing retrospective evaluation. The delta density method and an adaptively weighted ensemble of other forecasting methods each improve significantly on the next best ensemble component when applied separately, and achieve even better cross-validated performance when used in conjunction. We submitted real-time forecasts based on these contributions as part of CDC’s 2015/2016 FluSight Collaborative Comparison. Among the fourteen submissions that season, this system was ranked by CDC as the most accurate.},
	language = {en},
	number = {6},
	urldate = {2020-03-29},
	journal = {PLOS Computational Biology},
	author = {Brooks, Logan C. and Farrow, David C. and Hyun, Sangwon and Tibshirani, Ryan J. and Rosenfeld, Roni},
	editor = {Viboud, Cecile},
	month = jun,
	year = {2018},
	pages = {e1006134},
	file = {Brooks et al. - 2018 - Nonmechanistic forecasts of seasonal influenza wit.pdf:/mnt/data/Google Drive/Zotero/storage/88JUM5DJ/Brooks et al. - 2018 - Nonmechanistic forecasts of seasonal influenza wit.pdf:application/pdf}
}

@article{hyndmanAutomaticTimeSeries2008,
	title = {Automatic {Time} {Series} {Forecasting}: {The} \textbf{forecast} {Package} for \textit{{R}}},
	volume = {27},
	issn = {1548-7660},
	shorttitle = {Automatic {Time} {Series} {Forecasting}},
	url = {http://www.jstatsoft.org/v27/i03/},
	doi = {10.18637/jss.v027.i03},
	language = {en},
	number = {3},
	urldate = {2020-03-30},
	journal = {Journal of Statistical Software},
	author = {Hyndman, Rob J. and Khandakar, Yeasmin},
	year = {2008},
	file = {Hyndman and Khandakar - 2008 - Automatic Time Series Forecasting The bforecast.pdf:/mnt/data/Google Drive/Zotero/storage/J8VLZS83/Hyndman and Khandakar - 2008 - Automatic Time Series Forecasting The bforecast.pdf:application/pdf}
}

@book{10GroupedTime,
	title = {10.2 {Grouped} time series {\textbar} {Forecasting}: {Principles} and {Practice}},
	shorttitle = {10.2 {Grouped} time series {\textbar} {Forecasting}},
	url = {https://Otexts.com/fpp2/},
	abstract = {2nd edition},
	urldate = {2020-03-31},
	file = {Snapshot:/mnt/data/Google Drive/Zotero/storage/6QI93F5W/gts.html:text/html}
}

@misc{EstimationContinuousRanked,
	title = {Estimation of the {Continuous} {Ranked} {Probability} {Score} with {Limited} {Information} and {Applications} to {Ensemble} {Weather} {Forecasts} {\textbar} {SpringerLink}},
	url = {https://link.springer.com/article/10.1007/s11004-017-9709-7},
	urldate = {2020-04-02},
	file = {Estimation of the Continuous Ranked Probability Score with Limited Information and Applications to Ensemble Weather Forecasts | SpringerLink:/mnt/data/Google Drive/Zotero/storage/4XESHFER/s11004-017-9709-7.html:text/html}
}

@misc{OptimisingRenewalModels,
	title = {Optimising {Renewal} {Models} for {Real}-{Time} {Epidemic} {Prediction} and {Estimation} {\textbar} {bioRxiv}},
	url = {https://www.biorxiv.org/content/10.1101/835181v2.full},
	urldate = {2020-04-02},
	file = {Optimising Renewal Models for Real-Time Epidemic Prediction and Estimation | bioRxiv:/mnt/data/Google Drive/Zotero/storage/8ZTZEYVU/835181v2.html:text/html}
}

@article{macheteEarlyWarningCalibrated2013a,
	title = {Early {Warning} with {Calibrated} and {Sharper} {Probabilistic} {Forecasts}},
	volume = {32},
	issn = {1099-131X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/for.2242},
	doi = {10.1002/for.2242},
	abstract = {ABSTRACTGiven a nonlinear model, a probabilistic forecast may be obtained by Monte Carlo simulations. At a given forecast horizon, Monte Carlo simulations yield sets of discrete forecasts, which can be converted to density forecasts. The resulting density forecasts will inevitably be downgraded by model misspecification. In order to enhance the quality of the density forecasts, one can mix them with the unconditional density. This paper examines the value of combining conditional density forecasts with the unconditional density. The findings have positive implications for issuing early warnings in different disciplines including economics and meteorology, but UK inflation forecasts are considered as an example. Copyright © 2012 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {5},
	urldate = {2020-04-03},
	journal = {Journal of Forecasting},
	author = {Machete, Reason L.},
	year = {2013},
	keywords = {calibration, combining forecasts, density forecasts, scoring rule},
	pages = {452--468},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/PKZT8IGY/Machete - 2013 - Early Warning with Calibrated and Sharper Probabil.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/7LJPT7DX/for.html:text/html}
}

@article{aminikhanghahiSurveyMethodsTime2017,
	title = {A {Survey} of {Methods} for {Time} {Series} {Change} {Point} {Detection}},
	volume = {51},
	issn = {0219-1377},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5464762/},
	doi = {10.1007/s10115-016-0987-z},
	abstract = {Change points are abrupt variations in time series data. Such abrupt changes may represent transitions that occur between states. Detection of change points is useful in modelling and prediction of time series and is found in application areas such as medical condition monitoring, climate change detection, speech and image analysis, and human activity analysis. This survey article enumerates, categorizes, and compares many of the methods that have been proposed to detect change points in time series. The methods examined include both supervised and unsupervised algorithms that have been introduced and evaluated. We introduce several criteria to compare the algorithms. Finally, we present some grand challenges for the community to consider.},
	number = {2},
	urldate = {2020-04-03},
	journal = {Knowledge and information systems},
	author = {Aminikhanghahi, Samaneh and Cook, Diane J.},
	month = may,
	year = {2017},
	pmid = {28603327},
	pmcid = {PMC5464762},
	pages = {339--367},
	file = {PubMed Central Full Text PDF:/mnt/data/Google Drive/Zotero/storage/LWWTKKN3/Aminikhanghahi and Cook - 2017 - A Survey of Methods for Time Series Change Point D.pdf:application/pdf}
}

@article{killickOptimalDetectionChangepoints2012,
	title = {Optimal detection of changepoints with a linear computational cost},
	volume = {107},
	issn = {0162-1459, 1537-274X},
	url = {http://arxiv.org/abs/1101.1438},
	doi = {10.1080/01621459.2012.737745},
	abstract = {We consider the problem of detecting multiple changepoints in large data sets. Our focus is on applications where the number of changepoints will increase as we collect more data: for example in genetics as we analyse larger regions of the genome, or in ﬁnance as we observe time-series over longer periods. We consider the common approach of detecting changepoints through minimising a cost function over possible numbers and locations of changepoints. This includes several established procedures for detecting changing points, such as penalised likelihood and minimum description length. We introduce a new ∗R. Killick is Senior Research Associate, Department of Mathematics \& Statistics, Lancaster University, Lancaster, UK (E-mail: r.killick@lancs.ac.uk). P. Fearnhead is Professor, Department of Mathematics \& Statistics, Lancaster University, Lancaster, UK (E-mail: p.fearnhead@lancs.ac.uk). I.A. Eckley is Senior Lecturer, Department of Mathematics \& Statistics, Lancaster University, Lancaster, UK (E-mail: i.eckley@lancs.ac.uk). The authors are grateful to Richard Davis and Alice Cleynen for providing the Auto-PARM and PDPA software respectively. Part of this research was conducted whilst R. Killick was a jointly funded Engineering and Physical Sciences Research Council (EPSRC) / Shell Research Ltd graduate student at Lancaster University. Both I.A. Eckley and R. Killick also gratefully acknowledge the ﬁnancial support of the EPSRC grant number EP/I016368/1.},
	language = {en},
	number = {500},
	urldate = {2020-04-03},
	journal = {Journal of the American Statistical Association},
	author = {Killick, R. and Fearnhead, P. and Eckley, I. A.},
	month = dec,
	year = {2012},
	note = {arXiv: 1101.1438},
	keywords = {Statistics - Methodology, Quantitative Biology - Quantitative Methods, Quantitative Biology - Genomics},
	pages = {1590--1598},
	annote = {Comment: 25 pages, 4 figures, To appear in Journal of the American Statistical Association},
	file = {Killick et al. - 2012 - Optimal detection of changepoints with a linear co.pdf:/mnt/data/Google Drive/Zotero/storage/NUKTI622/Killick et al. - 2012 - Optimal detection of changepoints with a linear co.pdf:application/pdf}
}

@techreport{paragOptimisingRenewalModels2019,
	type = {preprint},
	title = {Optimising {Renewal} {Models} for {Real}-{Time} {Epidemic} {Prediction} and {Estimation}},
	url = {http://biorxiv.org/lookup/doi/10.1101/835181},
	abstract = {Abstract
          
            The effective reproduction number,
            R
            
              t
            
            , is an important prognostic for infectious disease epidemics. Significant changes in
            R
            
              t
            
            can forewarn about new transmissions or predict the efficacy of interventions. The renewal model infers
            R
            
              t
            
            from incidence data and has been applied to Ebola virus disease and pandemic influenza outbreaks, among others. This model estimates
            R
            
              t
            
            using a sliding window of length
            k
            . While this facilitates real-time detection of statistically significant
            R
            
              t
            
            fluctuations, inference is highly
            k
            -sensitive. Models with too large or small
            k
            might ignore meaningful changes or over-interpret noise-induced ones. No principled
            k
            -selection scheme exists. We develop a practical yet rigorous scheme using the accumulated prediction error (APE) metric from information theory. We derive exact incidence prediction distributions and integrate these within an APE framework to identify the
            k
            best supported by available data. We find that this
            k
            optimises short-term prediction accuracy and expose how common, heuristic
            k
            -choices, which seem sensible, could be misleading.},
	language = {en},
	urldate = {2020-04-03},
	institution = {Bioinformatics},
	author = {Parag, Kv and Donnelly, Ca},
	month = nov,
	year = {2019},
	doi = {10.1101/835181},
	file = {Parag and Donnelly - 2019 - Optimising Renewal Models for Real-Time Epidemic P.pdf:/mnt/data/Google Drive/Zotero/storage/3VCUX3E7/Parag and Donnelly - 2019 - Optimising Renewal Models for Real-Time Epidemic P.pdf:application/pdf}
}

@article{rissanenOrderEstimationAccumulated1986,
	title = {Order {Estimation} by {Accumulated} {Prediction} {Errors}},
	volume = {23},
	issn = {0021-9002},
	url = {https://www.jstor.org/stable/3214342},
	doi = {10.2307/3214342},
	abstract = {This paper presents a new criterion based on prediction error which allows the estimation of the number of parameters as well as structures in statistical models. The criterion is valid for short and long samples alike. Unlike Akaike's earlier criterion, also based on prediction error, the criterion proposed here appears to produce consistent error estimates in ARMA processes.},
	urldate = {2020-04-03},
	journal = {Journal of Applied Probability},
	author = {Rissanen, Jorma},
	year = {1986},
	pages = {55--61}
}

@article{OrderEstimationAccumulated,
	title = {Order {Estimation} by {Accumulated} {Prediction} {Errors}},
	abstract = {This paper presents a new criterion based on prediction error which allows the estimation of the number of parameters as well as structures in statistical models. The criterion is valid for short and long samples alike. Unlike Akaike's earlier criterion, also based on prediction error, the criterion proposed here appears to produce consistent error estimates in ARMA processes.},
	language = {en},
	pages = {8},
	file = {Order Estimation by Accumulated Prediction Errors.pdf:/mnt/data/Google Drive/Zotero/storage/JTIXZW74/Order Estimation by Accumulated Prediction Errors.pdf:application/pdf}
}

@article{holmesAnalysisMultivariateTime,
	title = {Analysis of multivariate time- series using the {MARSS} package},
	language = {en},
	author = {Holmes, E E and Ward, E J and Scheuerell, M D},
	pages = {284},
	file = {Holmes et al. - Analysis of multivariate time- series using the MA.pdf:/mnt/data/Google Drive/Zotero/storage/DLEKJXGW/Holmes et al. - Analysis of multivariate time- series using the MA.pdf:application/pdf}
}

@article{montero-mansoFFORMAFeaturebasedForecast2020,
	title = {{FFORMA}: {Feature}-based forecast model averaging},
	volume = {36},
	issn = {01692070},
	shorttitle = {{FFORMA}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0169207019300895},
	doi = {10.1016/j.ijforecast.2019.02.011},
	abstract = {We propose an automated method for obtaining weighted forecast combinations using time series features. The proposed approach involves two phases. First, we use a collection of time series to train a meta-model to assign weights to various possible forecasting methods with the goal of minimizing the average forecasting loss obtained from a weighted forecast combination. The inputs to the meta-model are features extracted from each series. In the second phase, we forecast new series using a weighted forecast combination where the weights are obtained from our previously trained meta-model. Our method outperforms a simple forecast combination, and outperforms all of the most popular individual methods in the time series forecasting literature. The approach achieved second position in the M4 competition.},
	language = {en},
	number = {1},
	urldate = {2020-04-06},
	journal = {International Journal of Forecasting},
	author = {Montero-Manso, Pablo and Athanasopoulos, George and Hyndman, Rob J. and Talagala, Thiyanga S.},
	month = jan,
	year = {2020},
	pages = {86--92},
	file = {Montero-Manso et al. - 2020 - FFORMA Feature-based forecast model averaging.pdf:/mnt/data/Google Drive/Zotero/storage/MNTZFESR/Montero-Manso et al. - 2020 - FFORMA Feature-based forecast model averaging.pdf:application/pdf}
}

@article{wilksEnforcingCalibrationEnsemble2018,
	title = {Enforcing calibration in ensemble postprocessing: {Enforcing} {Calibration} in {Ensemble} {Postprocessing}},
	volume = {144},
	issn = {00359009},
	shorttitle = {Enforcing calibration in ensemble postprocessing},
	url = {http://doi.wiley.com/10.1002/qj.3185},
	doi = {10.1002/qj.3185},
	language = {en},
	number = {710},
	urldate = {2020-04-06},
	journal = {Quarterly Journal of the Royal Meteorological Society},
	author = {Wilks, Daniel S.},
	month = jan,
	year = {2018},
	pages = {76--84},
	file = {Wilks - 2018 - Enforcing calibration in ensemble postprocessing .pdf:/mnt/data/Google Drive/Zotero/storage/KDGXTZRJ/Wilks - 2018 - Enforcing calibration in ensemble postprocessing .pdf:application/pdf}
}

@techreport{gibsonImprovingProbabilisticInfectious2019,
	type = {preprint},
	title = {Improving {Probabilistic} {Infectious} {Disease} {Forecasting} {Through} {Coherence}},
	url = {http://biorxiv.org/lookup/doi/10.1101/2019.12.27.889212},
	abstract = {With an estimated \$10.4 billion in medical costs and 31.4 million outpatient visits each year, inﬂuenza poses a serious burden of disease in the United States. To provide insights and advance warning into the spread of inﬂuenza, the U.S. Centers for Disease Control and Prevention (CDC) runs a challenge for forecasting weighted inﬂuenza-like illness (wILI) at the national and regional level. Many models produce independent forecasts for each geographical unit, ignoring the constraint that the national wILI is a weighted sum of regional wILI, where the weights correspond to the population size of the region. We propose a novel algorithm that transforms a set of independent forecast distributions to obey this constraint, which we refer to as probabilistically coherent. Enforcing probabilistic coherence led to an increase in forecast skill for 90\% of the models we tested over multiple ﬂu seasons, highlighting the importance of respecting the forecasting system’s geographical hierarchy.},
	language = {en},
	urldate = {2020-04-06},
	institution = {Bioinformatics},
	author = {Gibson, Graham Casey and Moran, Kelly R. and Reich, Nicholas G. and Osthus, Dave},
	month = dec,
	year = {2019},
	doi = {10.1101/2019.12.27.889212},
	file = {Gibson et al. - 2019 - Improving Probabilistic Infectious Disease Forecas.pdf:/mnt/data/Google Drive/Zotero/storage/J22WMETR/Gibson et al. - 2019 - Improving Probabilistic Infectious Disease Forecas.pdf:application/pdf}
}

@article{schaeybroeckEnsemblePostprocessingUsing2015,
	title = {Ensemble post-processing using member-by-member approaches: theoretical aspects},
	volume = {141},
	copyright = {© 2014 Royal Meteorological Society},
	issn = {1477-870X},
	shorttitle = {Ensemble post-processing using member-by-member approaches},
	url = {https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/qj.2397},
	doi = {10.1002/qj.2397},
	abstract = {Linear post-processing approaches are proposed and fundamental mechanisms are analyzed by which the probabilistic skill of an ensemble forecast can be improved. The ensemble mean of the corrected forecast is a linear function of the ensemble mean(s) of the predictor(s). Likewise, the ensemble spread of the corrected forecast depends linearly on that of the uncorrected forecast. The regression coefficients are obtained by maximizing the likelihood function for the error distribution. Comparing different calibration approaches on simple systems that exhibit chaotic features (the Kuramoto–Sivashinsky equation, the spatially extended Lorenz system), four correction mechanisms are identified: the ensemble-mean scaling and nudging using the predictor(s), and the ensemble-spread scaling and nudging. Ensemble-spread corrections turn out to yield improvement only when ‘reliability’ constraints are imposed on the corrected forecast. First of all climatological reliability is enforced and is satisfied when the total variability of the forecast is equal to the variability of the observations. Second, ensemble reliability or calibration of the ensembles is enforced such that the squared error of the ensemble mean coincides with the ensemble variance. In terms of continuous ranked probability skill score, spread calibration provides much more gain in skill than the traditional ensemble-mean calibration and extends for lead times far beyond the error-doubling time. The skill performance is better than or as good as the benchmark calibration method which derives from statistical assumptions –non-homogeneous Gaussian regression. In addition to the member-by-member nature of the approach, benefits compared with the benchmark method can be pinpointed. In particular, although the post-processing methods are performed for each lead time, location and variable independently, they preserve the rank correlations and thus take dependencies across space, time, and different variables into account. In addition, higher-order ensemble moments like kurtosis and skewness correspond to those of the uncorrected forecasts.},
	language = {en},
	number = {688},
	urldate = {2020-04-06},
	journal = {Quarterly Journal of the Royal Meteorological Society},
	author = {Schaeybroeck, Bert Van and Vannitsem, Stéphane},
	year = {2015},
	keywords = {member-by-member approach, model output statistics, statistical post-processing},
	pages = {807--818},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/6YNK3RMJ/Schaeybroeck and Vannitsem - 2015 - Ensemble post-processing using member-by-member ap.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/C5IM6W5S/qj.html:text/html}
}

@article{fraleyEnsembleBMAPackageProbabilistic,
	title = {{ensembleBMA}: {An} {R} {Package} for {Probabilistic} {Forecasting} using {Ensembles} and {Bayesian} {Model} {Averaging}},
	language = {en},
	author = {Fraley, Chris and Raftery, Adrian E and Gneiting, Tilmann and Sloughter, J McLean},
	pages = {19},
	file = {Fraley et al. - ensembleBMA An R Package for Probabilistic Foreca.pdf:/mnt/data/Google Drive/Zotero/storage/396E78V2/Fraley et al. - ensembleBMA An R Package for Probabilistic Foreca.pdf:application/pdf}
}

@article{monacheProbabilisticAspectsMeteorological2006,
	title = {Probabilistic aspects of meteorological and ozone regional ensemble forecasts},
	volume = {111},
	copyright = {Copyright 2006 by the American Geophysical Union.},
	issn = {2156-2202},
	url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2005JD006917},
	doi = {10.1029/2005JD006917},
	abstract = {This study investigates whether probabilistic ozone forecasts from an ensemble can be made with skill: i.e., high verification resolution and reliability. Twenty-eight ozone forecasts were generated over the Lower Fraser Valley, British Columbia, Canada, for the 5-day period 11–15 August 2004 and compared with 1-hour averaged measurements of ozone concentrations at five stations. The forecasts were obtained by driving the Community Multiscale Air Quality Model (CMAQ) model with four meteorological forecasts and seven emission scenarios: a control run, ±50\% NOx, ±50\% volatile organic compounds (VOC), and ±50\% NOx combined with VOC. Probabilistic forecast quality is verified using relative operating characteristic curves, Talagrand diagrams, and a new reliability index. Results show that both meteorology and emission perturbations are needed to have a skillful probabilistic forecast system: the meteorology perturbation is important to capture the ozone temporal and spatial distribution and the emission perturbation is needed to span the range of ozone concentration magnitudes. Emission perturbations are more important than meteorology perturbations for capturing the likelihood of high ozone concentrations. Perturbations involving NOx resulted in a more skillful probabilistic forecast for the episode analyzed, and therefore the 50\% perturbation values appear to span much of the emission uncertainty for this case. All of the ensembles analyzed show a high ozone concentration bias in the Talagrand diagrams, even when the biases from the unperturbed emissions forecasts are removed from all ensemble members. This result indicates nonlinearity in the ensemble, which arises from both ozone chemistry and its interaction with input from particular meteorological models.},
	language = {en},
	number = {D24},
	urldate = {2020-04-07},
	journal = {Journal of Geophysical Research: Atmospheres},
	author = {Monache, Luca Delle and Hacker, Joshua P. and Zhou, Yongmei and Deng, Xingxiu and Stull, Roland B.},
	year = {2006},
	keywords = {ensemble, ozone, probabilistic forecasts},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/VJAUP5ZB/Monache et al. - 2006 - Probabilistic aspects of meteorological and ozone .pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/GT337KX3/2005JD006917.html:text/html}
}

@article{liReviewStatisticalPostprocessing2017,
	title = {A review on statistical postprocessing methods for hydrometeorological ensemble forecasting},
	volume = {4},
	copyright = {© 2017 Wiley Periodicals, Inc.},
	issn = {2049-1948},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wat2.1246},
	doi = {10.1002/wat2.1246},
	abstract = {Computer simulation models have been widely used to generate hydrometeorological forecasts. As the raw forecasts contain uncertainties arising from various sources, including model inputs and outputs, model initial and boundary conditions, model structure, and model parameters, it is necessary to apply statistical postprocessing methods to quantify and reduce those uncertainties. Different postprocessing methods have been developed for meteorological forecasts (e.g., precipitation) and for hydrological forecasts (e.g., streamflow) due to their different statistical properties. In this paper, we conduct a comprehensive review of the commonly used statistical postprocessing methods for both meteorological and hydrological forecasts. Moreover, methods to generate ensemble members that maintain the observed spatiotemporal and intervariable dependency are reviewed. Finally, some perspectives on the further development of statistical postprocessing methods for hydrometeorological ensemble forecasting are provided. WIREs Water 2017, 4:e1246. doi: 10.1002/wat2.1246 This article is categorized under: Science of Water {\textgreater} Methods Science of Water {\textgreater} Water Extremes},
	language = {en},
	number = {6},
	urldate = {2020-04-07},
	journal = {WIREs Water},
	author = {Li, Wentao and Duan, Qingyun and Miao, Chiyuan and Ye, Aizhong and Gong, Wei and Di, Zhenhua},
	year = {2017},
	pages = {e1246},
	file = {Snapshot:/mnt/data/Google Drive/Zotero/storage/YQBGAS3E/wat2.html:text/html}
}

@article{liReviewStatisticalPostprocessing2017a,
	title = {A review on statistical postprocessing methods for hydrometeorological ensemble forecasting},
	volume = {4},
	issn = {2049-1948},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wat2.1246},
	doi = {10.1002/wat2.1246},
	abstract = {Computer simulation models have been widely used to generate hydrometeorological forecasts. As the raw forecasts contain uncertainties arising from various sources, including model inputs and outputs, model initial and boundary conditions, model structure, and model parameters, it is necessary to apply statistical postprocessing methods to quantify and reduce those uncertainties. Different postprocessing methods have been developed for meteorological forecasts (e.g., precipitation) and for hydrological forecasts (e.g., streamflow) due to their different statistical properties. In this paper, we conduct a comprehensive review of the commonly used statistical postprocessing methods for both meteorological and hydrological forecasts. Moreover, methods to generate ensemble members that maintain the observed spatiotemporal and intervariable dependency are reviewed. Finally, some perspectives on the further development of statistical postprocessing methods for hydrometeorological ensemble forecasting are provided. WIREs Water 2017, 4:e1246. doi: 10.1002/wat2.1246 This article is categorized under: Science of Water {\textgreater} Methods Science of Water {\textgreater} Water Extremes},
	language = {en},
	number = {6},
	urldate = {2020-04-07},
	journal = {WIREs Water},
	author = {Li, Wentao and Duan, Qingyun and Miao, Chiyuan and Ye, Aizhong and Gong, Wei and Di, Zhenhua},
	year = {2017},
	pages = {e1246},
	file = {Snapshot:/mnt/data/Google Drive/Zotero/storage/ECCB4UFB/wat2.html:text/html}
}

@article{liReviewStatisticalPostprocessing2017b,
	title = {A review on statistical postprocessing methods for hydrometeorological ensemble forecasting},
	volume = {4},
	issn = {2049-1948},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wat2.1246},
	doi = {10.1002/wat2.1246},
	abstract = {Computer simulation models have been widely used to generate hydrometeorological forecasts. As the raw forecasts contain uncertainties arising from various sources, including model inputs and outputs, model initial and boundary conditions, model structure, and model parameters, it is necessary to apply statistical postprocessing methods to quantify and reduce those uncertainties. Different postprocessing methods have been developed for meteorological forecasts (e.g., precipitation) and for hydrological forecasts (e.g., streamflow) due to their different statistical properties. In this paper, we conduct a comprehensive review of the commonly used statistical postprocessing methods for both meteorological and hydrological forecasts. Moreover, methods to generate ensemble members that maintain the observed spatiotemporal and intervariable dependency are reviewed. Finally, some perspectives on the further development of statistical postprocessing methods for hydrometeorological ensemble forecasting are provided. WIREs Water 2017, 4:e1246. doi: 10.1002/wat2.1246 This article is categorized under: Science of Water {\textgreater} Methods Science of Water {\textgreater} Water Extremes},
	language = {en},
	number = {6},
	urldate = {2020-04-07},
	journal = {WIREs Water},
	author = {Li, Wentao and Duan, Qingyun and Miao, Chiyuan and Ye, Aizhong and Gong, Wei and Di, Zhenhua},
	year = {2017},
	pages = {e1246},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/6WQKZ5C5/Li et al. - 2017 - A review on statistical postprocessing methods for.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/DE2Z8TLS/wat2.html:text/html}
}

@misc{WhyOptimOut2016,
	title = {Why optim() is out of date},
	url = {https://www.r-bloggers.com/why-optim-is-out-of-date/},
	abstract = {Why optim() is out of date And perhaps you should be careful using it Once upon a time Once upon a time, there was a young Oxford D.Phil. graduate with a thesis on quantum mechanics who — by virtue of a mixup in identities — got hired as an Agricultural Economist. He was actually better…},
	language = {en-US},
	urldate = {2020-04-07},
	journal = {R-bloggers},
	month = nov,
	year = {2016},
	file = {Snapshot:/mnt/data/Google Drive/Zotero/storage/ANNBD4ZN/why-optim-is-out-of-date.html:text/html}
}

@article{nousuStatisticalPostprocessingEnsemble2019,
	title = {Statistical post-processing of ensemble forecasts of the height of new snow},
	volume = {26},
	issn = {1023-5809},
	url = {https://www.nonlin-processes-geophys.net/26/339/2019/},
	doi = {https://doi.org/10.5194/npg-26-339-2019},
	abstract = {{\textless}p{\textgreater}{\textless}strong{\textgreater}Abstract.{\textless}/strong{\textgreater} Forecasting the height of new snow (HN) is crucial for avalanche hazard forecasting, road viability, ski resort management and tourism attractiveness. Météo-France operates the PEARP-S2M probabilistic forecasting system, including 35 members of the PEARP Numerical Weather Prediction system, where the SAFRAN downscaling tool refines the elevation resolution and the Crocus snowpack model represents the main physical processes in the snowpack. It provides better HN forecasts than direct NWP diagnostics but exhibits significant biases and underdispersion. We applied a statistical post-processing to these ensemble forecasts, based on non-homogeneous regression with a censored shifted Gamma distribution. Observations come from manual measurements of 24\&thinsp;h HN in the French Alps and Pyrenees. The calibration is tested at the station scale and the massif scale (i.e. aggregating different stations over areas of 1000\&thinsp;km{\textless}span class="inline-formula"{\textgreater}$^{\textrm{2}}${\textless}/span{\textgreater}). Compared to the raw forecasts, similar improvements are obtained for both spatial scales. Therefore, the post-processing can be applied at any point of the massifs. Two training datasets are tested: (1) a 22-year homogeneous reforecast for which the NWP model resolution and physical options are identical to the operational system but without the same initial perturbations; (2) 3-year real-time forecasts with a heterogeneous model configuration but the same perturbation methods. The impact of the training dataset depends on lead time and on the evaluation criteria. The long-term reforecast improves the reliability of severe snowfall but leads to overdispersion due to the discrepancy in real-time perturbations. Thus, the development of reliable automatic forecasting products of HN needs long reforecasts as homogeneous as possible with the operational systems.{\textless}/p{\textgreater}},
	language = {English},
	number = {3},
	urldate = {2020-04-08},
	journal = {Nonlinear Processes in Geophysics},
	author = {Nousu, Jari-Pekka and Lafaysse, Matthieu and Vernay, Matthieu and Bellier, Joseph and Evin, Guillaume and Joly, Bruno},
	month = sep,
	year = {2019},
	pages = {339--357},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/PCPFWPVD/Nousu et al. - 2019 - Statistical post-processing of ensemble forecasts .pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/X26LRXTJ/2019.html:text/html}
}

@article{diazStatisticalPostprocessingEnsemble2020,
	title = {Statistical post-processing of ensemble forecasts of temperature in {Santiago} de {Chile}},
	volume = {27},
	copyright = {© 2019 Royal Meteorological Society},
	issn = {1469-8080},
	url = {https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/met.1818},
	doi = {10.1002/met.1818},
	abstract = {Modelling forecast uncertainty is a difficult task in any forecasting problem. In weather forecasting a possible solution is the use of forecast ensembles, which are obtained from multiple runs of numerical weather prediction models with various initial conditions and model parametrizations to provide information about the expected uncertainty. Currently all major meteorological centres issue forecasts using their operational ensemble prediction systems. However, it is a general problem that the spread of the ensemble is too small compared to observations at specific sites resulting in under-dispersive forecasts, leading to a lack of calibration. In order to correct this problem, various statistical calibration techniques have been developed in the last two decades. In the present work different post-processing techniques were tested for calibrating nine member ensemble forecasts of temperature for Santiago de Chile, obtained by the Weather Research and Forecasting model using different planetary boundary layer and land surface model parametrizations. In particular, the ensemble model output statistics and Bayesian model averaging techniques were implemented and, since the observations are characterized by large altitude differences, the estimation of model parameters was adapted to the actual conditions at hand. Compared to the raw ensemble, all tested post-processing approaches significantly improve the calibration of probabilistic forecasts and the accuracy of point forecasts. The ensemble model output statistics method using parameter estimation based on expert clustering of stations (according to their altitudes) shows the best forecast skill.},
	language = {en},
	number = {1},
	urldate = {2020-04-08},
	journal = {Meteorological Applications},
	author = {Díaz, Mailiu and Nicolis, Orietta and Marín, Julio César and Baran, Sándor},
	year = {2020},
	keywords = {Bayesian model averaging, ensemble model output statistics, ensemble post-processing, probabilistic forecasting, temperature forecast},
	pages = {e1818},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/U8R842AZ/Díaz et al. - 2020 - Statistical post-processing of ensemble forecasts .pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/W8BARAGL/met.html:text/html}
}

@article{mcandrewAdaptivelyStackingEnsembles2019,
	title = {Adaptively stacking ensembles for influenza forecasting with incomplete data},
	url = {http://arxiv.org/abs/1908.01675},
	abstract = {Seasonal influenza infects between 10 and 50 million people in the United States every year, overburdening hospitals during weeks of peak incidence. Named by the CDC as an important tool to fight the damaging effects of these epidemics, accurate forecasts of influenza and influenza-like illness (ILI) forewarn public health officials about when, and where, seasonal influenza outbreaks will hit hardest. Multi-model ensemble forecasts---weighted combinations of component models---have shown positive results in forecasting. Ensemble forecasts of influenza outbreaks have been static, training on all past ILI data at the beginning of a season, generating a set of optimal weights for each model in the ensemble, and keeping the weights constant. We propose an adaptive ensemble forecast that (i) changes model weights week-by-week throughout the influenza season, (ii) only needs the current influenza season's data to make predictions, and (iii) by introducing a prior distribution, shrinks weights toward the reference equal weighting approach and adjusts for observed ILI percentages that are subject to future revisions. We investigate the prior's ability to impact adaptive ensemble performance and, after finding an optimal prior via a cross-validation approach, compare our adaptive ensemble's performance to equal-weighted and static ensembles. Applied to forecasts of short-term ILI incidence at the regional and national level in the US, our adaptive model outperforms a naive equal-weighted ensemble, and has similar or better performance to the static ensemble, which requires multiple years of training data. Adaptive ensembles are able to quickly train and forecast during epidemics, and provide a practical tool to public health officials looking for forecasts that can conform to unique features of a specific season.},
	urldate = {2020-04-12},
	journal = {arXiv:1908.01675 [cs, stat]},
	author = {McAndrew, Thomas and Reich, Nicholas G.},
	month = jul,
	year = {2019},
	note = {arXiv: 1908.01675},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Applications},
	file = {arXiv Fulltext PDF:/mnt/data/Google Drive/Zotero/storage/7SC4EU2N/McAndrew and Reich - 2019 - Adaptively stacking ensembles for influenza foreca.pdf:application/pdf;arXiv.org Snapshot:/mnt/data/Google Drive/Zotero/storage/KKANAKWG/1908.html:text/html}
}

@misc{timothywrusselljoelhellewell1samabbott1nickgoldinghamishgibbschristopherijarviskevinvanzandvoortcmmidncovworkinggroupstefanflascherosalindeggowjohnedmundsadamjkucharski.UsingDelayadjustedCase2020,
	title = {Using a delay-adjusted case fatality ratio to estimate under-reporting},
	url = {https://cmmid.github.io/topics/covid19/severity/global_cfr_estimates.html},
	abstract = {Using a corrected case fatality ratio, we calculate estimates of the level of under-reporting for any country with greater than ten deaths},
	language = {en},
	urldate = {2020-04-12},
	journal = {CMMID Repository},
	author = {Timothy W Russell, Joel Hellewell1, Sam Abbott1, Nick Golding, Hamish Gibbs, Christopher I Jarvis, Kevin van Zandvoort, CMMID nCov working group, Stefan Flasche, Rosalind Eggo, W John Edmunds \& Adam J Kucharski.},
	month = mar,
	year = {2020},
	file = {Snapshot:/mnt/data/Google Drive/Zotero/storage/BK7WAVI3/global_cfr_estimates.html:text/html}
}

@techreport{jarvisQuantifyingImpactPhysical2020,
	type = {preprint},
	title = {Quantifying the impact of physical distance measures on the transmission of {COVID}-19 in the {UK}},
	url = {http://medrxiv.org/lookup/doi/10.1101/2020.03.31.20049023},
	abstract = {Background: 
To mitigate and slow the spread of COVID-19, many countries have adopted unprecedented physical distancing policies, including the UK. We evaluate whether these measures might be sufficient to control the epidemic by estimating their impact on the reproduction number (R0, the average number of secondary cases generated per case).

Methods: 
We asked a representative sample of UK adults about their contact patterns on the previous day. The questionnaire documents the age and location of contacts and as well as a measure of their intimacy (whether physical contact was made or not). In addition, we asked about adherence to different physical distancing measures. The first surveys were sent on Tuesday 24th March, one day after a “lockdown” was implemented across the UK. We compared measured contact patterns during the lockdown to patterns of social contact made during a non-epidemic period. By comparing these, we estimated the change in reproduction number as a consequence of the physical distancing measures imposed. We used a meta-analysis of published estimates to inform our estimates of the reproduction number before interventions were put in place.

Findings: 
We found a 73\% reduction in the average daily number of contacts observed per participant (from 10.2 to 2.9). This would be sufficient to reduce R0 from 2.6 prior to lockdown to 0.62 (95\% confidence interval [CI] 0.37 - 0.89) after the lockdown, based on all types of contact and 0.37 (95\% CI = 0.22 - 0.53) for physical contacts only.


Interpretation:

The physical distancing measures adopted by the UK public have substantially reduced contact levels and will likely lead to a substantial impact and a decline in cases in the coming weeks. However, this projected decline in incidence will not occur immediately as there are significant delays between infection, the onset of symptomatic disease and hospitalisation, as well as further delays to these events being reported. Tracking behavioural change can give a more rapid assessment of the impact of physical distancing measures than routine epidemiological surveillance.},
	language = {en},
	urldate = {2020-04-12},
	institution = {Epidemiology},
	author = {Jarvis, Christopher I and Van Zandvoort, Kevin and Gimma, Amy and Prem, Kiesha and {CMMID COVID-19 working group} and Klepac, Petra and Rubin, G James and Edmunds, W John},
	month = apr,
	year = {2020},
	doi = {10.1101/2020.03.31.20049023},
	file = {Jarvis et al. - 2020 - Quantifying the impact of physical distance measur.pdf:/mnt/data/Google Drive/Zotero/storage/LSE79PJL/Jarvis et al. - 2020 - Quantifying the impact of physical distance measur.pdf:application/pdf}
}

@techreport{daviesEffectNonpharmaceuticalInterventions2020,
	type = {preprint},
	title = {The effect of non-pharmaceutical interventions on {COVID}-19 cases, deaths and demand for hospital services in the {UK}: a modelling study},
	shorttitle = {The effect of non-pharmaceutical interventions on {COVID}-19 cases, deaths and demand for hospital services in the {UK}},
	url = {http://medrxiv.org/lookup/doi/10.1101/2020.04.01.20049908},
	abstract = {Background 
Non-pharmaceutical interventions have been implemented to reduce transmission of SARS-CoV-2 in the UK. Projecting the size of an unmitigated epidemic and the potential effect of different control measures has been critical to support evidence-based policymaking during the early stages of the epidemic. 

Methods We used a stochastic age-structured transmission model to explore a range of intervention scenarios, including the introduction of school closures, social distancing, shielding of elderly groups, self-isolation of symptomatic cases, and extreme "lockdown"-type restrictions. We simulated different durations of interventions and triggers for introduction, as well as combinations of interventions. For each scenario, we projected estimated new cases over time, patients requiring inpatient and critical care (intensive care unit, ICU) treatment, and deaths.

Findings 
We found that mitigation measures aimed at reducing transmission would likely have decreased the reproduction number, but not sufficiently to prevent ICU demand from exceeding NHS availability. To keep ICU bed demand below capacity in the model, more extreme restrictions were necessary. In a scenario where "lockdown"-type interventions were put in place to reduce transmission, these interventions would need to be in place for a large proportion of the coming year in order to prevent healthcare demand exceeding availability.

Interpretation
The characteristics of SARS-CoV-2 mean that extreme measures are likely required to bring the epidemic under control and to prevent very large numbers of deaths and an excess of demand on hospital beds, especially those in ICUs.},
	language = {en},
	urldate = {2020-04-12},
	institution = {Infectious Diseases (except HIV/AIDS)},
	author = {Davies, Nicholas G and Kucharski, Adam J and Eggo, Rosalind M and Gimma, Amy and {CMMID COVID-19 Working Group} and Edmunds, W. John},
	month = apr,
	year = {2020},
	doi = {10.1101/2020.04.01.20049908},
	file = {Davies et al. - 2020 - The effect of non-pharmaceutical interventions on .pdf:/mnt/data/Google Drive/Zotero/storage/7SMLCJY4/Davies et al. - 2020 - The effect of non-pharmaceutical interventions on .pdf:application/pdf}
}

@article{hellewellFeasibilityControllingCOVID192020,
	title = {Feasibility of controlling {COVID}-19 outbreaks by isolation of cases and contacts},
	volume = {8},
	issn = {2214-109X},
	url = {https://www.thelancet.com/journals/langlo/article/PIIS2214-109X(20)30074-7/abstract},
	doi = {10.1016/S2214-109X(20)30074-7},
	abstract = {{\textless}h2{\textgreater}Summary{\textless}/h2{\textgreater}{\textless}h3{\textgreater}Background{\textless}/h3{\textgreater}{\textless}p{\textgreater}Isolation of cases and contact tracing is used to control outbreaks of infectious diseases, and has been used for coronavirus disease 2019 (COVID-19). Whether this strategy will achieve control depends on characteristics of both the pathogen and the response. Here we use a mathematical model to assess if isolation and contact tracing are able to control onwards transmission from imported cases of COVID-19.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Methods{\textless}/h3{\textgreater}{\textless}p{\textgreater}We developed a stochastic transmission model, parameterised to the COVID-19 outbreak. We used the model to quantify the potential effectiveness of contact tracing and isolation of cases at controlling a severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)-like pathogen. We considered scenarios that varied in the number of initial cases, the basic reproduction number (\textit{R}$_{\textrm{0}}$), the delay from symptom onset to isolation, the probability that contacts were traced, the proportion of transmission that occurred before symptom onset, and the proportion of subclinical infections. We assumed isolation prevented all further transmission in the model. Outbreaks were deemed controlled if transmission ended within 12 weeks or before 5000 cases in total. We measured the success of controlling outbreaks using isolation and contact tracing, and quantified the weekly maximum number of cases traced to measure feasibility of public health effort.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Findings{\textless}/h3{\textgreater}{\textless}p{\textgreater}Simulated outbreaks starting with five initial cases, an \textit{R}$_{\textrm{0}}$ of 1·5, and 0\% transmission before symptom onset could be controlled even with low contact tracing probability; however, the probability of controlling an outbreak decreased with the number of initial cases, when \textit{R}$_{\textrm{0}}$ was 2·5 or 3·5 and with more transmission before symptom onset. Across different initial numbers of cases, the majority of scenarios with an \textit{R}$_{\textrm{0}}$ of 1·5 were controllable with less than 50\% of contacts successfully traced. To control the majority of outbreaks, for \textit{R}$_{\textrm{0}}$ of 2·5 more than 70\% of contacts had to be traced, and for an \textit{R}$_{\textrm{0}}$ of 3·5 more than 90\% of contacts had to be traced. The delay between symptom onset and isolation had the largest role in determining whether an outbreak was controllable when \textit{R}$_{\textrm{0}}$ was 1·5. For \textit{R}$_{\textrm{0}}$ values of 2·5 or 3·5, if there were 40 initial cases, contact tracing and isolation were only potentially feasible when less than 1\% of transmission occurred before symptom onset.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Interpretation{\textless}/h3{\textgreater}{\textless}p{\textgreater}In most scenarios, highly effective contact tracing and case isolation is enough to control a new outbreak of COVID-19 within 3 months. The probability of control decreases with long delays from symptom onset to isolation, fewer cases ascertained by contact tracing, and increasing transmission before symptoms. This model can be modified to reflect updated transmission characteristics and more specific definitions of outbreak control to assess the potential success of local response efforts.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Funding{\textless}/h3{\textgreater}{\textless}p{\textgreater}Wellcome Trust, Global Challenges Research Fund, and Health Data Research UK.{\textless}/p{\textgreater}},
	language = {English},
	number = {4},
	urldate = {2020-04-12},
	journal = {The Lancet Global Health},
	author = {Hellewell, Joel and Abbott, Sam and Gimma, Amy and Bosse, Nikos I. and Jarvis, Christopher I. and Russell, Timothy W. and Munday, James D. and Kucharski, Adam J. and Edmunds, W. John and Sun, Fiona and Flasche, Stefan and Quilty, Billy J. and Davies, Nicholas and Liu, Yang and Clifford, Samuel and Klepac, Petra and Jit, Mark and Diamond, Charlie and Gibbs, Hamish and Zandvoort, Kevin van and Funk, Sebastian and Eggo, Rosalind M.},
	month = apr,
	year = {2020},
	pmid = {32119825},
	pages = {e488--e496},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/NNCP658M/Hellewell et al. - 2020 - Feasibility of controlling COVID-19 outbreaks by i.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/YDUTTSX3/fulltext.html:text/html}
}

@misc{Dash,
	title = {Dash},
	url = {http://covid19icu.cl.cam.ac.uk/},
	urldate = {2020-04-12},
	file = {Dash:/mnt/data/Google Drive/Zotero/storage/463VXN77/covid19icu.cl.cam.ac.uk.html:text/html}
}

@article{deasyForecastingUltraearlyIntensive2020,
	title = {Forecasting ultra-early intensive care strain from {COVID}-19 in {England}},
	copyright = {© 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.medrxiv.org/content/10.1101/2020.03.19.20039057v3},
	doi = {10.1101/2020.03.19.20039057},
	abstract = {{\textless}p{\textgreater}The COVID-19 pandemic has led to unprecedented strain on intensive care unit (ICU) admission in parts of the world. Strategies to create surge ICU capacity requires complex local and national service reconfiguration and reduction or cancellation of elective activity. Theses measures require time to implement and have an inevitable lag before additional capacity comes on-line. An accurate short-range forecast would be helpful in guiding such difficult, costly and ethically challenging decisions. At the time this work began, cases in England were starting to increase. Here we present an attempt at an agile short-range forecast based on published real-time COVID-19 case data from the seven National Health Service commissioning regions in England (East of England, London, Midlands, North East and Yorkshire, North West, South East and South West). We use a Monte Carlo approach to model the likely impact of current diagnoses on regional ICU capacity over a 14 day horizon. Our model is designed to be parsimonious and based on plausible epidemiological data from the literature available. On the basis of the modelling assumptions made, ICU occupancy is likely to increase dramatically in the the days following the time of modelling. If the current exponential growth continues, 5 out of 7 commissioning regions will have more critically ill COVID-19 patients than there are ICU beds within two weeks{\textbackslash}todo\{last thing to do\}. Despite variable growth in absolute patients, all commissioning regions are forecast to be heavily burdened under the assumptions used. Whilst, like any forecast model, there remain uncertainties both in terms of model specification and robust epidemiological data in this early prospective phase, it would seem that surge capacity will be required in the very near future. We hope that our model will help policy decision makers with their preparations. The uncertainties in the data highlight the urgent need for ongoing real-time surveillance to allow forecasts to be constantly updated using high quality local patient-facing data as it emerges.{\textless}/p{\textgreater}},
	language = {en},
	urldate = {2020-04-12},
	journal = {medRxiv},
	author = {Deasy, Jacob and Rocheteau, Emma and Kohler, Katharina and Stubbs, Daniel J. and Barbiero, Pietro and Liò, Pietro and Ercole, Ari},
	month = apr,
	year = {2020},
	pages = {2020.03.19.20039057},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/REVI8V8L/Deasy et al. - 2020 - Forecasting ultra-early intensive care strain from.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/R3EEKC86/2020.03.19.20039057v3.html:text/html}
}

@article{viboudFutureInfluenzaForecasts2019,
	title = {The future of influenza forecasts},
	volume = {116},
	copyright = {© 2019 . https://www.pnas.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/116/8/2802},
	doi = {10.1073/pnas.1822167116},
	abstract = {Recent years have seen a growing interest in generating real-time epidemic forecasts to help control infectious diseases, prompted by a succession of global and regional outbreaks. Increased availability of epidemiological data and novel digital data streams such as search engine queries and social media (1, 2), together with the rise of machine learning and sophisticated statistical approaches, have injected new blood into the science of outbreak forecasts (3, 4). In parallel, mechanistic transmission models have benefited from computational advances and extensive data on the mobility and sociodemographic structure of human populations (5, 6). In this rapidly advancing research landscape, modeling consortiums have generated systematic model comparisons of the impact of new interventions and ensemble predictions of outbreak trajectory, for use by decision makers (7⇓⇓⇓⇓–12). Despite the rapid development of disease forecasting as a discipline, however, and the interest of public health policy makers in making better use of analytics tools to control outbreaks, forecasts are rarely operational in the same way that weather forecasts, extreme events, and climate predictions are. The influenza study by Reich et al. (13) in PNAS is a unique example of multiyear infectious disease forecasts featuring a variety of modeling approaches, with consistent model formulations and forecasting targets throughout the 7-y study period (13). This is a major improvement over previous model comparison studies that used different targets and time horizons and sometimes different epidemiological datasets.

While there is considerable interest among modelers in advancing the science of disease forecasts, the level of confidence of the public health community in exploiting these predictions in real-world situations remains unclear. The disconnect is in part due to poor understanding of modeling concepts by policy experts, which is compounded by a lack of a well-established operational framework for using and … 

[↵][1]1To whom correspondence should be addressed. Email: viboudc\{at\}mail.nih.gov.

 [1]: \#xref-corresp-1-1},
	language = {en},
	number = {8},
	urldate = {2020-04-12},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Viboud, Cécile and Vespignani, Alessandro},
	month = feb,
	year = {2019},
	pmid = {30737293},
	pages = {2802--2804},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/JFE2J84D/Viboud and Vespignani - 2019 - The future of influenza forecasts.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/67WHC8X4/2802.html:text/html}
}

@article{fergusonImpactNonpharmaceuticalInterventions2020,
	title = {Impact of non-pharmaceutical interventions ({NPIs}) to reduce {COVID}- 19 mortality and healthcare demand},
	abstract = {The global impact of COVID-19 has been profound, and the public health threat it represents is the most serious seen in a respiratory virus since the 1918 H1N1 influenza pandemic. Here we present the results of epidemiological modelling which has informed policymaking in the UK and other countries in recent weeks. In the absence of a COVID-19 vaccine, we assess the potential role of a number of public health measures – so-called non-pharmaceutical interventions (NPIs) – aimed at reducing contact rates in the population and thereby reducing transmission of the virus. In the results presented here, we apply a previously published microsimulation model to two countries: the UK (Great Britain specifically) and the US. We conclude that the effectiveness of any one intervention in isolation is likely to be limited, requiring multiple interventions to be combined to have a substantial impact on transmission.},
	language = {en},
	author = {Ferguson, Neil M and Laydon, Daniel and Nedjati-Gilani, Gemma and Imai, Natsuko and Ainslie, Kylie and Baguelin, Marc and Bhatia, Sangeeta and Boonyasiri, Adhiratha and Cucunubá, Zulma and Cuomo-Dannenburg, Gina and Dighe, Amy and Fu, Han and Gaythorpe, Katy and Thompson, Hayley and Verity, Robert and Volz, Erik and Wang, Haowei and Wang, Yuanrong and Walker, Patrick GT and Walters, Caroline and Winskill, Peter and Whittaker, Charles and Donnelly, Christl A and Riley, Steven and Ghani, Azra C},
	year = {2020},
	pages = {20},
	file = {Ferguson et al. - 2020 - Impact of non-pharmaceutical interventions (NPIs) .pdf:/mnt/data/Google Drive/Zotero/storage/794A9ZNP/Ferguson et al. - 2020 - Impact of non-pharmaceutical interventions (NPIs) .pdf:application/pdf}
}

@misc{thibautjombartemilysnightingalemarkjitolivierlepolaindewarouxgwenknightstefanflascherosalindeggoadamjkucharskicarla.b.pearsonsimonrproctercmmidncovworkinggroupwjohnedmunds.ForecastingCriticalCare2020,
	title = {Forecasting critical care bed requirements for {COVID}-19 patients in {England}},
	url = {https://cmmid.github.io/topics/covid19/current-patterns-transmission/ICU-projections.html},
	abstract = {We estimate critical care bed demand for COVID-19 cases in England for the next two weeks. Results suggest that current capacity might be reached or exceeded by the end of March 2020.},
	language = {en},
	urldate = {2020-04-12},
	journal = {CMMID Repository},
	author = {Thibaut Jombart, Emily S Nightingale, Mark Jit, Olivier le Polain de Waroux, Gwen Knight, Stefan Flasche, Rosalind Eggo, Adam J Kucharski, Carl A.B. Pearson, Simon R Procter, CMMID nCov working group \& W John Edmunds.},
	month = mar,
	year = {2020},
	file = {Snapshot:/mnt/data/Google Drive/Zotero/storage/R38ZM99F/ICU-projections.html:text/html}
}

@misc{CollaborativeMultiyearMultimodel,
	title = {A collaborative multiyear, multimodel assessment of seasonal influenza forecasting in the {United} {States} {\textbar} {PNAS}},
	url = {https://www.pnas.org/content/116/8/3146},
	urldate = {2020-04-12},
	file = {A collaborative multiyear, multimodel assessment of seasonal influenza forecasting in the United States | PNAS:/mnt/data/Google Drive/Zotero/storage/Z66CQLI6/3146.html:text/html}
}

@misc{samabbottCRANPackageIdmodelr,
	title = {{CRAN} - {Package} idmodelr},
	url = {https://cran.r-project.org/web/packages/idmodelr/index.html},
	urldate = {2020-04-12},
	author = {{Sam Abbott}},
	file = {CRAN - Package idmodelr:/mnt/data/Google Drive/Zotero/storage/PCKAL6ZT/index.html:text/html}
}

@article{fingerRealtimeAnalysisDiphtheria2019,
	title = {Real-time analysis of the diphtheria outbreak in forcibly displaced {Myanmar} nationals in {Bangladesh}},
	volume = {17},
	issn = {1741-7015},
	url = {https://doi.org/10.1186/s12916-019-1288-7},
	doi = {10.1186/s12916-019-1288-7},
	abstract = {Between August and December 2017, more than 625,000 Rohingya from Myanmar fled into Bangladesh, settling in informal makeshift camps in Cox’s Bazar district and joining 212,000 Rohingya already present. In early November, a diphtheria outbreak hit the camps, with 440 reported cases during the first month. A rise in cases during early December led to a collaboration between teams from Médecins sans Frontières—who were running a provisional diphtheria treatment centre—and the London School of Hygiene and Tropical Medicine with the goal to use transmission dynamic models to forecast the potential scale of the outbreak and the resulting resource needs.},
	number = {1},
	urldate = {2020-04-12},
	journal = {BMC Medicine},
	author = {Finger, Flavio and Funk, Sebastian and White, Kate and Siddiqui, M. Ruby and Edmunds, W. John and Kucharski, Adam J.},
	month = mar,
	year = {2019},
	pages = {58},
	file = {Full Text:/mnt/data/Google Drive/Zotero/storage/RZM5I6TE/Finger et al. - 2019 - Real-time analysis of the diphtheria outbreak in f.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/UPFFPNMC/s12916-019-1288-7.html:text/html}
}

@article{camachoTemporalChangesEbola2015,
	title = {Temporal {Changes} in {Ebola} {Transmission} in {Sierra} {Leone} and {Implications} for {Control} {Requirements}: a {Real}-time {Modelling} {Study}},
	issn = {2157-3999},
	shorttitle = {Temporal {Changes} in {Ebola} {Transmission} in {Sierra} {Leone} and {Implications} for {Control} {Requirements}},
	url = {index.html%3Fp=55052.html},
	doi = {10.1371/currents.outbreaks.406ae55e83ec0b5193e30856b9235ed2},
	language = {English},
	urldate = {2020-04-12},
	journal = {PLOS Currents Outbreaks},
	author = {Camacho, Anton and Kucharski, Adam and Aki-Sawyerr, Yvonne and White, Mark A. and Flasche, Stefan and Baguelin, Marc and Pollington, Timothy and Carney, Julia R. and Glover, Rebecca and Smout, Elizabeth and Tiffany, Amanda and Edmunds, W. John and Funk, Sebastian},
	month = feb,
	year = {2015},
	file = {Accepted Version:/mnt/data/Google Drive/Zotero/storage/Q2PBGAHY/Camacho et al. - 2015 - Temporal Changes in Ebola Transmission in Sierra L.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/3HPFHCTN/index.htmlp=55052.html:text/html}
}

@article{burknerApproximateLeavefutureoutCrossvalidation2019,
	title = {Approximate leave-future-out cross-validation for {Bayesian} time series models},
	url = {http://arxiv.org/abs/1902.06281},
	abstract = {One of the common goals of time series analysis is to use the observed series to inform predictions for future observations. In the absence of any actual new data to predict, cross-validation can be used to estimate a model's future predictive accuracy, for instance, for the purpose of model comparison or selection. Exact cross-validation for Bayesian models is often computationally expensive, but approximate cross-validation methods have been developed, most notably methods for leave-one-out cross-validation (LOO-CV). If the actual prediction task is to predict the future given the past, LOO-CV provides an overly optimistic estimate because the information from future observations is available to influence predictions of the past. To properly account for the time series structure, we can use leave-future-out cross-validation (LFO-CV). Like exact LOO-CV, exact LFO-CV requires refitting the model many times to different subsets of the data. Using Pareto smoothed importance sampling, we propose a method for approximating exact LFO-CV that drastically reduces the computational costs while also providing informative diagnostics about the quality of the approximation.},
	urldate = {2020-04-13},
	journal = {arXiv:1902.06281 [stat]},
	author = {Bürkner, Paul-Christian and Gabry, Jonah and Vehtari, Aki},
	month = oct,
	year = {2019},
	note = {arXiv: 1902.06281},
	keywords = {Statistics - Methodology},
	annote = {Comment: 26 pages, 15 figures, 2 tables},
	file = {arXiv Fulltext PDF:/mnt/data/Google Drive/Zotero/storage/VYSN2ZSQ/Bürkner et al. - 2019 - Approximate leave-future-out cross-validation for .pdf:application/pdf;arXiv.org Snapshot:/mnt/data/Google Drive/Zotero/storage/62CNNBDG/1902.html:text/html}
}

@article{zamoEstimationContinuousRanked2018,
	title = {Estimation of the {Continuous} {Ranked} {Probability} {Score} with {Limited} {Information} and {Applications} to {Ensemble} {Weather} {Forecasts}},
	volume = {50},
	issn = {1874-8953},
	url = {https://doi.org/10.1007/s11004-017-9709-7},
	doi = {10.1007/s11004-017-9709-7},
	abstract = {The continuous ranked probability score (CRPS) is a much used measure of performance for probabilistic forecasts of a scalar observation. It is a quadratic measure of the difference between the forecast cumulative distribution function (CDF) and the empirical CDF of the observation. Analytic formulations of the CRPS can be derived for most classical parametric distributions, and be used to assess the efficiency of different CRPS estimators. When the true forecast CDF is not fully known, but represented as an ensemble of values, the CRPS is estimated with some error. Thus, using the CRPS to compare parametric probabilistic forecasts with ensemble forecasts may be misleading due to the unknown error of the estimated CRPS for the ensemble. With simulated data, the impact of the type of the verified ensemble (a random sample or a set of quantiles) on the CRPS estimation is studied. Based on these simulations, recommendations are issued to choose the most accurate CRPS estimator according to the type of ensemble. The interest of these recommendations is illustrated with real ensemble weather forecasts. Also, relationships between several estimators of the CRPS are demonstrated and used to explain the differences of accuracy between the estimators.},
	language = {en},
	number = {2},
	urldate = {2020-04-13},
	journal = {Mathematical Geosciences},
	author = {Zamo, Michaël and Naveau, Philippe},
	month = feb,
	year = {2018},
	pages = {209--234},
	file = {Springer Full Text PDF:/mnt/data/Google Drive/Zotero/storage/IRN58QLH/Zamo and Naveau - 2018 - Estimation of the Continuous Ranked Probability Sc.pdf:application/pdf}
}

@misc{FK83Bvarsv,
	title = {{FK83}/bvarsv},
	url = {https://github.com/FK83/bvarsv},
	abstract = {Analysis of the Primiceri (REStud, 2005) model. Contribute to FK83/bvarsv development by creating an account on GitHub.},
	language = {en},
	urldate = {2020-04-13},
	journal = {GitHub},
	file = {Snapshot:/mnt/data/Google Drive/Zotero/storage/34QH4CE5/bvarsv_Nov2015_website.html:text/html}
}

@article{haiderPredictionMixtureModels,
	title = {Prediction with {Mixture} {Models}},
	language = {de},
	author = {Haider, Peter},
	pages = {58},
	file = {Haider - Prediction with Mixture Models.pdf:/mnt/data/Google Drive/Zotero/storage/UXIZ5K8I/Haider - Prediction with Mixture Models.pdf:application/pdf}
}

@article{qiSampleexpandMethodPredicting2014,
	title = {Sample-expand method for predicting the specified structure of microporous aluminophosphate},
	volume = {185},
	issn = {1387-1811},
	url = {http://www.sciencedirect.com/science/article/pii/S1387181113005167},
	doi = {10.1016/j.micromeso.2013.10.009},
	abstract = {Imbalanced data sets often exist in many real-world fields and this problem has got more and more attention in recent years. In this paper, a sample-expand method is proposed as data pre-processing procedure to improve the predictive performance of the zeolite synthesis on imbalance data set. First, the data pre-processing is implemented for expanding samples by exploring the marginal structure of the given data set using k-nearest neighbor algorithm (KNN). Then, the expanded data set is input to support vector machines (SVM) for classification. Finally, Q times n-fold cross-validations procedure (CVs) is adopted to assess the prediction performance. The advantage of the data pre-processing is that it can obtain stable data set for establishing the training model and abide by the classification criteria of SVM, such that the improved predictive performance is achievable. Moreover, other classical machine learning methods are also presented to accomplish the prediction task. Compared experimental results demonstrate that SVM method can reach very satisfactory predictive accuracy on the pre-processing data set. Specially, the phase diagram of gel composition is provided as a guiding role for subsequent rational synthesis experiments.},
	language = {en},
	urldate = {2020-04-15},
	journal = {Microporous and Mesoporous Materials},
	author = {Qi, Miao and Qin, Zhanmin and Gao, Na and Kong, Jun and Guo, Yuting and Lu, Yinghua},
	month = feb,
	year = {2014},
	keywords = {Prediction, -nearest neighbor, Sample-expand, Support vector machines, Zeolite synthesis},
	pages = {1--6},
	file = {ScienceDirect Snapshot:/mnt/data/Google Drive/Zotero/storage/XIGBX538/S1387181113005167.html:text/html}
}

@article{zamoEstimationContinuousRanked2018a,
	title = {Estimation of the {Continuous} {Ranked} {Probability} {Score} with {Limited} {Information} and {Applications} to {Ensemble} {Weather} {Forecasts}},
	volume = {50},
	issn = {1874-8953},
	url = {https://doi.org/10.1007/s11004-017-9709-7},
	doi = {10.1007/s11004-017-9709-7},
	abstract = {The continuous ranked probability score (CRPS) is a much used measure of performance for probabilistic forecasts of a scalar observation. It is a quadratic measure of the difference between the forecast cumulative distribution function (CDF) and the empirical CDF of the observation. Analytic formulations of the CRPS can be derived for most classical parametric distributions, and be used to assess the efficiency of different CRPS estimators. When the true forecast CDF is not fully known, but represented as an ensemble of values, the CRPS is estimated with some error. Thus, using the CRPS to compare parametric probabilistic forecasts with ensemble forecasts may be misleading due to the unknown error of the estimated CRPS for the ensemble. With simulated data, the impact of the type of the verified ensemble (a random sample or a set of quantiles) on the CRPS estimation is studied. Based on these simulations, recommendations are issued to choose the most accurate CRPS estimator according to the type of ensemble. The interest of these recommendations is illustrated with real ensemble weather forecasts. Also, relationships between several estimators of the CRPS are demonstrated and used to explain the differences of accuracy between the estimators.},
	language = {en},
	number = {2},
	urldate = {2020-05-12},
	journal = {Mathematical Geosciences},
	author = {Zamo, Michaël and Naveau, Philippe},
	month = feb,
	year = {2018},
	pages = {209--234},
	file = {Springer Full Text PDF:/mnt/data/Google Drive/Zotero/storage/JKHD2EQY/Zamo and Naveau - 2018 - Estimation of the Continuous Ranked Probability Sc.pdf:application/pdf}
}

@article{hersbachDecompositionContinuousRanked2000,
	title = {Decomposition of the {Continuous} {Ranked} {Probability} {Score} for {Ensemble} {Prediction} {Systems}},
	volume = {15},
	issn = {0882-8156},
	url = {https://journals.ametsoc.org/doi/10.1175/1520-0434%282000%29015%3C0559%3ADOTCRP%3E2.0.CO%3B2},
	doi = {10.1175/1520-0434(2000)015<0559:DOTCRP>2.0.CO;2},
	abstract = {Some time ago, the continuous ranked probability score (CRPS) was proposed as a new verification tool for (probabilistic) forecast systems. Its focus is on the entire permissible range of a certain (weather) parameter. The CRPS can be seen as a ranked probability score with an infinite number of classes, each of zero width. Alternatively, it can be interpreted as the integral of the Brier score over all possible threshold values for the parameter under consideration. For a deterministic forecast system the CRPS reduces to the mean absolute error. In this paper it is shown that for an ensemble prediction system the CRPS can be decomposed into a reliability part and a resolution/uncertainty part, in a way that is similar to the decomposition of the Brier score. The reliability part of the CRPS is closely connected to the rank histogram of the ensemble, while the resolution/uncertainty part can be related to the average spread within the ensemble and the behavior of its outliers. The usefulness of such a decomposition is illustrated for the ensemble prediction system running at the European Centre for Medium-Range Weather Forecasts. The evaluation of the CRPS and its decomposition proposed in this paper can be extended to systems issuing continuous probability forecasts, by realizing that these can be interpreted as the limit of ensemble forecasts with an infinite number of members.},
	number = {5},
	urldate = {2020-05-12},
	journal = {Weather and Forecasting},
	author = {Hersbach, Hans},
	month = oct,
	year = {2000},
	note = {Publisher: American Meteorological Society},
	pages = {559--570},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/HB3ZPJY3/Hersbach - 2000 - Decomposition of the Continuous Ranked Probability.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/R4LMBJDM/1520-0434(2000)0150559DOTCRP2.0.html:text/html}
}

@techreport{lemaitreAssessingImpactNonpharmaceutical2020,
	type = {preprint},
	title = {Assessing the impact of non-pharmaceutical interventions on {SARS}-{CoV}-2 transmission in {Switzerland}},
	url = {http://medrxiv.org/lookup/doi/10.1101/2020.05.04.20090639},
	abstract = {Following the rapid dissemination of COVID-19 cases in Switzerland, large-scale non-pharmaceutical interventions (NPIs) were implemented by the cantons and the federal government between February 28 and March 20. Estimates of the impact of these interventions on SARS-CoV-2 transmission are critical for decision making in this and future outbreaks. We here aim to assess the impact of these NPIs on disease transmission by estimating changes in the basic reproduction number (R0) at national and cantonal levels in relation to the timing of these NPIs. We estimate the time-varying R0 nationally and in twelve cantons by fitting a stochastic transmission model explicitly simulating within hospital dynamics. We use individual-level data of {\textgreater}1,000 hospitalized patients in Switzerland and public daily reports of hospitalizations and deaths. We estimate the national R0 was 3.15 (95\% CI: 2.13-3.76) at the start of the epidemic. Starting from around March 6, we find a strong reduction in R0 with a 85\% median decrease (95\% quantile range, QR: 83\%-90\%) to a value of 0.44 (95\% QR: 0.27-0.65) in the period of March 29-April 5. At the cantonal-level R0 decreased over the course of the epidemic between 71\% and 94\%. We found that reductions in R0 were synchronous with changes in mobility patterns as estimated through smartphone activity, which started before the official implementation of NPIs. We found that most of the reduction of transmission is due to behavioural changes as opposed to natural immunity, the latter accounting for only about 3\% of the total reduction in effective transmission. As Switzerland considers relaxing some of the restrictions of social mixing, current estimates of R0 well below one are promising. However most of inferred transmission reduction was due to behaviour change ({\textless}3\% due to natural immunity buildup), with an estimated 97\% (95\% QR: 96.6\%-97.2\%) of the Swiss population still susceptible to SARS-CoV-2 as of April 24. These results warrant a cautious relaxation of social distance practices and close monitoring of changes in both the basic and effective reproduction numbers.},
	language = {en},
	urldate = {2020-05-13},
	institution = {Epidemiology},
	author = {Lemaitre, Joseph Chadi and Perez-Saez, Javier and Azman, Andrew and Rinaldo, Andrea and Fellay, Jacques},
	month = may,
	year = {2020},
	doi = {10.1101/2020.05.04.20090639},
	file = {Lemaitre et al. - 2020 - Assessing the impact of non-pharmaceutical interve.pdf:/mnt/data/Google Drive/Zotero/storage/63CW59X2/Lemaitre et al. - 2020 - Assessing the impact of non-pharmaceutical interve.pdf:application/pdf}
}

@incollection{wilksUnivariateEnsemblePostprocessing2018,
	title = {Univariate {Ensemble} {Postprocessing}},
	isbn = {978-0-12-812372-0},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780128123720000030},
	language = {en},
	urldate = {2020-05-14},
	booktitle = {Statistical {Postprocessing} of {Ensemble} {Forecasts}},
	publisher = {Elsevier},
	author = {Wilks, Daniel S.},
	year = {2018},
	doi = {10.1016/B978-0-12-812372-0.00003-0},
	pages = {49--89},
	file = {Wilks - 2018 - Univariate Ensemble Postprocessing.pdf:/mnt/data/Google Drive/Zotero/storage/T9LR4PUI/Wilks - 2018 - Univariate Ensemble Postprocessing.pdf:application/pdf}
}

@article{thoreyOnlineLearningContinuous2017,
	title = {Online learning with the {Continuous} {Ranked} {Probability} {Score} for ensemble forecasting: {Ensemble} {Online} {Learning}},
	volume = {143},
	issn = {00359009},
	shorttitle = {Online learning with the {Continuous} {Ranked} {Probability} {Score} for ensemble forecasting},
	url = {http://doi.wiley.com/10.1002/qj.2940},
	doi = {10.1002/qj.2940},
	abstract = {Ensemble forecasting resorts to multiple individual forecasts to produce a discrete probability distribution which accurately represents the uncertainties. Before every forecast, a weighted empirical distribution function is derived from the ensemble, so as to minimize the Continuous Ranked Probability Score (CRPS). We apply online learning techniques, which have previously been used for deterministic forecasting, and we adapt them for the minimization of the CRPS. The proposed method theoretically guarantees that the aggregated forecast competes, in terms of CRPS, against the best weighted empirical distribution function with weights constant in time. This is illustrated on synthetic data. Besides, our study improves the knowledge of the CRPS expectation for model mixtures. We generalize results on the bias of the CRPS computed with ensemble forecasts, and propose a new scheme to achieve fair CRPS minimization, without any assumption on the distributions.},
	language = {en},
	number = {702},
	urldate = {2020-05-14},
	journal = {Quarterly Journal of the Royal Meteorological Society},
	author = {Thorey, J. and Mallet, V. and Baudin, P.},
	month = jan,
	year = {2017},
	pages = {521--529},
	file = {Thorey et al. - 2017 - Online learning with the Continuous Ranked Probabi.pdf:/mnt/data/Google Drive/Zotero/storage/CR9EWRQ8/Thorey et al. - 2017 - Online learning with the Continuous Ranked Probabi.pdf:application/pdf}
}

@article{wickramasuriyaOptimalForecastReconciliation2019,
	title = {Optimal {Forecast} {Reconciliation} for {Hierarchical} and {Grouped} {Time} {Series} {Through} {Trace} {Minimization}},
	volume = {114},
	issn = {0162-1459, 1537-274X},
	url = {https://www.tandfonline.com/doi/full/10.1080/01621459.2018.1448825},
	doi = {10.1080/01621459.2018.1448825},
	abstract = {Large collections of time series often have aggregation constraints due to product or geographical groupings. The forecasts for the most disaggregated series are usually required to add-up exactly to the forecasts of the aggregated series, a constraint we refer to as “coherence”. Forecast reconciliation is the process of adjusting forecasts to make them coherent.},
	language = {en},
	number = {526},
	urldate = {2020-05-15},
	journal = {Journal of the American Statistical Association},
	author = {Wickramasuriya, Shanika L. and Athanasopoulos, George and Hyndman, Rob J.},
	month = apr,
	year = {2019},
	pages = {804--819},
	file = {Wickramasuriya et al. - 2019 - Optimal Forecast Reconciliation for Hierarchical a.pdf:/mnt/data/Google Drive/Zotero/storage/TIDP2UWF/Wickramasuriya et al. - 2019 - Optimal Forecast Reconciliation for Hierarchical a.pdf:application/pdf}
}

@article{makridakisStatisticalMachineLearning2018,
	title = {Statistical and {Machine} {Learning} forecasting methods: {Concerns} and ways forward},
	volume = {13},
	issn = {1932-6203},
	shorttitle = {Statistical and {Machine} {Learning} forecasting methods},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0194889},
	doi = {10.1371/journal.pone.0194889},
	abstract = {Machine Learning (ML) methods have been proposed in the academic literature as alternatives to statistical ones for time series forecasting. Yet, scant evidence is available about their relative performance in terms of accuracy and computational requirements. The purpose of this paper is to evaluate such performance across multiple forecasting horizons using a large subset of 1045 monthly time series used in the M3 Competition. After comparing the post-sample accuracy of popular ML methods with that of eight traditional statistical ones, we found that the former are dominated across both accuracy measures used and for all forecasting horizons examined. Moreover, we observed that their computational requirements are considerably greater than those of statistical methods. The paper discusses the results, explains why the accuracy of ML models is below that of statistical ones and proposes some possible ways forward. The empirical results found in our research stress the need for objective and unbiased ways to test the performance of forecasting methods that can be achieved through sizable and open competitions allowing meaningful comparisons and definite conclusions.},
	language = {en},
	number = {3},
	urldate = {2020-05-15},
	journal = {PLOS ONE},
	author = {Makridakis, Spyros and Spiliotis, Evangelos and Assimakopoulos, Vassilios},
	month = mar,
	year = {2018},
	note = {Publisher: Public Library of Science},
	keywords = {Algorithms, Forecasting, Support vector machines, Computing methods, Neural networks, Optimization, Preprocessing, Statistical methods},
	pages = {e0194889},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/GV5QJGJ8/Makridakis et al. - 2018 - Statistical and Machine Learning forecasting metho.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/NSJH4N43/article.html:text/html}
}

@article{burknerBrmsPackageBayesian2017,
	title = {\textbf{brms} : {An} \textit{{R}} {Package} for {Bayesian} {Multilevel} {Models} {Using} \textit{{Stan}}},
	volume = {80},
	issn = {1548-7660},
	shorttitle = {\textbf{brms}},
	url = {http://www.jstatsoft.org/v80/i01/},
	doi = {10.18637/jss.v080.i01},
	abstract = {The brms package implements Bayesian multilevel models in R using the probabilistic programming language Stan. A wide range of distributions and link functions are supported, allowing users to ﬁt – among others – linear, robust linear, binomial, Poisson, survival, response times, ordinal, quantile, zero-inﬂated, hurdle, and even non-linear models all in a multilevel context. Further modeling options include autocorrelation of the response variable, user deﬁned covariance structures, censored data, as well as metaanalytic standard errors. Prior speciﬁcations are ﬂexible and explicitly encourage users to apply prior distributions that actually reﬂect their beliefs. In addition, model ﬁt can easily be assessed and compared using posterior-predictive checks and leave-one-out crossvalidation. If you use brms, please cite this article as published in the Journal of Statistical Software (Bu¨rkner 2017).},
	language = {en},
	number = {1},
	urldate = {2020-05-15},
	journal = {Journal of Statistical Software},
	author = {Bürkner, Paul-Christian},
	year = {2017},
	file = {Bürkner - 2017 - bbrmsb  An iRi Package for Bayesian Mul.pdf:/mnt/data/Google Drive/Zotero/storage/LX9P9SKT/Bürkner - 2017 - bbrmsb  An iRi Package for Bayesian Mul.pdf:application/pdf}
}

@techreport{gibsonImprovingProbabilisticInfectious2019a,
	type = {preprint},
	title = {Improving {Probabilistic} {Infectious} {Disease} {Forecasting} {Through} {Coherence}},
	url = {http://biorxiv.org/lookup/doi/10.1101/2019.12.27.889212},
	abstract = {With an estimated \$10.4 billion in medical costs and 31.4 million outpatient visits each year, inﬂuenza poses a serious burden of disease in the United States. To provide insights and advance warning into the spread of inﬂuenza, the U.S. Centers for Disease Control and Prevention (CDC) runs a challenge for forecasting weighted inﬂuenza-like illness (wILI) at the national and regional level. Many models produce independent forecasts for each geographical unit, ignoring the constraint that the national wILI is a weighted sum of regional wILI, where the weights correspond to the population size of the region. We propose a novel algorithm that transforms a set of independent forecast distributions to obey this constraint, which we refer to as probabilistically coherent. Enforcing probabilistic coherence led to an increase in forecast skill for 90\% of the models we tested over multiple ﬂu seasons, highlighting the importance of respecting the forecasting system’s geographical hierarchy.},
	language = {en},
	urldate = {2020-05-16},
	institution = {Bioinformatics},
	author = {Gibson, Graham Casey and Moran, Kelly R. and Reich, Nicholas G. and Osthus, Dave},
	month = dec,
	year = {2019},
	doi = {10.1101/2019.12.27.889212},
	file = {Gibson et al. - 2019 - Improving Probabilistic Infectious Disease Forecas.pdf:/mnt/data/Google Drive/Zotero/storage/LRMDHF65/Gibson et al. - 2019 - Improving Probabilistic Infectious Disease Forecas.pdf:application/pdf}
}

@article{yaoUsingStackingAverage2018a,
	title = {Using {Stacking} to {Average} {Bayesian} {Predictive} {Distributions} (with {Discussion})},
	volume = {13},
	issn = {1936-0975},
	url = {https://projecteuclid.org/euclid.ba/1516093227},
	doi = {10.1214/17-BA1091},
	abstract = {Bayesian model averaging is ﬂawed in the M-open setting in which the true data-generating process is not one of the candidate models being ﬁt. We take the idea of stacking from the point estimation literature and generalize to the combination of predictive distributions. We extend the utility function to any proper scoring rule and use Pareto smoothed importance sampling to eﬃciently compute the required leave-one-out posterior distributions. We compare stacking of predictive distributions to several alternatives: stacking of means, Bayesian model averaging (BMA), Pseudo-BMA, and a variant of Pseudo-BMA that is stabilized using the Bayesian bootstrap. Based on simulations and real-data applications, we recommend stacking of predictive distributions, with bootstrapped-Pseudo-BMA as an approximate alternative when computation cost is an issue.},
	language = {en},
	number = {3},
	urldate = {2020-05-16},
	journal = {Bayesian Analysis},
	author = {Yao, Yuling and Vehtari, Aki and Simpson, Daniel and Gelman, Andrew},
	month = sep,
	year = {2018},
	pages = {917--1007},
	file = {Yao et al. - 2018 - Using Stacking to Average Bayesian Predictive Dist.pdf:/mnt/data/Google Drive/Zotero/storage/CAU9DYUN/Yao et al. - 2018 - Using Stacking to Average Bayesian Predictive Dist.pdf:application/pdf}
}

@article{chatzilenaContemporaryStatisticalInference2019a,
	title = {Contemporary statistical inference for infectious disease models using {Stan}},
	volume = {29},
	issn = {17554365},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1755436519300325},
	doi = {10.1016/j.epidem.2019.100367},
	language = {en},
	urldate = {2020-05-17},
	journal = {Epidemics},
	author = {Chatzilena, Anastasia and van Leeuwen, Edwin and Ratmann, Oliver and Baguelin, Marc and Demiris, Nikolaos},
	month = dec,
	year = {2019},
	pages = {100367},
	file = {Full Text:/mnt/data/Google Drive/Zotero/storage/TM6MVY4E/Chatzilena et al. - 2019 - Contemporary statistical inference for infectious .pdf:application/pdf}
}

@article{donnatModelingHeterogeneityCOVID192020,
	title = {Modeling the {Heterogeneity} in {COVID}-19's {Reproductive} {Number} and its {Impact} on {Predictive} {Scenarios}},
	url = {http://arxiv.org/abs/2004.05272},
	abstract = {The current COVID-19 pandemic is leading experts to assess the risks posed by the disease and compare policies geared towards stalling its evolution as a global pandemic. In this setting, the virus' basic reproductive number R\_0, which characterizes the average number of secondary cases generated by each primary case, takes on a significant importance in the quantification of the potential scope of the pandemic. Yet, in most models, R\_0 is assumed to be a universal constant for the virus across outbreak clusters and populations -- thus neglecting the inherent variability of the transmission process due to varying population densities, demographics, temporal factors, etc. In fact, it can be shown that the reproduction number is highly variable. Considering its expected value thus leads to biased or loose results in the reported predictive scenarios, especially as these are tailored to a given country or region. The goal of this paper is the examination of the impact of the reproductive number R's variability on important output metrics, and the percolation of this variability in projected scenarios so as to provide uncertainty quantification. In this perspective, instead of considering a single R\_0, we consider a distribution of reproductive numbers R and devise a simple Bayesian hierarchical model that builds upon current methods for estimating the R to integrate its heterogeneity. We then simulate the spread of the epidemic, and the impact of different social distancing strategies using a probabilistic framework that models hospital occupancy. This shows the strong impact of this added variability on the reported results. We emphasize that our goal is not to replace benchmark methods for estimating the basic reproductive numbers, but rather to discuss the importance of the impact of R's heterogeneity on uncertainty quantification for the current COVID-19 pandemic.},
	language = {en},
	urldate = {2020-05-19},
	journal = {arXiv:2004.05272 [q-bio, stat]},
	author = {Donnat, Claire and Holmes, Susan},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.05272},
	keywords = {Statistics - Applications, Quantitative Biology - Populations and Evolution},
	file = {Donnat and Holmes - 2020 - Modeling the Heterogeneity in COVID-19's Reproduct.pdf:/mnt/data/Google Drive/Zotero/storage/J5RVUD26/Donnat and Holmes - 2020 - Modeling the Heterogeneity in COVID-19's Reproduct.pdf:application/pdf}
}

@article{donnatModelingHeterogeneityCOVID192020a,
	title = {Modeling the {Heterogeneity} in {COVID}-19's {Reproductive} {Number} and its {Impact} on {Predictive} {Scenarios}},
	url = {http://arxiv.org/abs/2004.05272},
	abstract = {The current COVID-19 pandemic is leading experts to assess the risks posed by the disease and compare policies geared towards stalling its evolution as a global pandemic. In this setting, the virus' basic reproductive number R\_0, which characterizes the average number of secondary cases generated by each primary case, takes on a significant importance in the quantification of the potential scope of the pandemic. Yet, in most models, R\_0 is assumed to be a universal constant for the virus across outbreak clusters and populations -- thus neglecting the inherent variability of the transmission process due to varying population densities, demographics, temporal factors, etc. In fact, it can be shown that the reproduction number is highly variable. Considering its expected value thus leads to biased or loose results in the reported predictive scenarios, especially as these are tailored to a given country or region. The goal of this paper is the examination of the impact of the reproductive number R's variability on important output metrics, and the percolation of this variability in projected scenarios so as to provide uncertainty quantification. In this perspective, instead of considering a single R\_0, we consider a distribution of reproductive numbers R and devise a simple Bayesian hierarchical model that builds upon current methods for estimating the R to integrate its heterogeneity. We then simulate the spread of the epidemic, and the impact of different social distancing strategies using a probabilistic framework that models hospital occupancy. This shows the strong impact of this added variability on the reported results. We emphasize that our goal is not to replace benchmark methods for estimating the basic reproductive numbers, but rather to discuss the importance of the impact of R's heterogeneity on uncertainty quantification for the current COVID-19 pandemic.},
	language = {en},
	urldate = {2020-05-25},
	journal = {arXiv:2004.05272 [q-bio, stat]},
	author = {Donnat, Claire and Holmes, Susan},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.05272},
	keywords = {Statistics - Applications, Quantitative Biology - Populations and Evolution},
	file = {Donnat and Holmes - 2020 - Modeling the Heterogeneity in COVID-19's Reproduct.pdf:/mnt/data/Google Drive/Zotero/storage/VWXSMSET/Donnat and Holmes - 2020 - Modeling the Heterogeneity in COVID-19's Reproduct.pdf:application/pdf}
}

@article{donnatModelingHeterogeneityCOVID192020b,
	title = {Modeling the {Heterogeneity} in {COVID}-19's {Reproductive} {Number} and its {Impact} on {Predictive} {Scenarios}},
	url = {http://arxiv.org/abs/2004.05272},
	abstract = {The current COVID-19 pandemic is leading experts to assess the risks posed by the disease and compare policies geared towards stalling its evolution as a global pandemic. In this setting, the virus' basic reproductive number R\_0, which characterizes the average number of secondary cases generated by each primary case, takes on a significant importance in the quantification of the potential scope of the pandemic. Yet, in most models, R\_0 is assumed to be a universal constant for the virus across outbreak clusters and populations -- thus neglecting the inherent variability of the transmission process due to varying population densities, demographics, temporal factors, etc. In fact, it can be shown that the reproduction number is highly variable. Considering its expected value thus leads to biased or loose results in the reported predictive scenarios, especially as these are tailored to a given country or region. The goal of this paper is the examination of the impact of the reproductive number R's variability on important output metrics, and the percolation of this variability in projected scenarios so as to provide uncertainty quantification. In this perspective, instead of considering a single R\_0, we consider a distribution of reproductive numbers R and devise a simple Bayesian hierarchical model that builds upon current methods for estimating the R to integrate its heterogeneity. We then simulate the spread of the epidemic, and the impact of different social distancing strategies using a probabilistic framework that models hospital occupancy. This shows the strong impact of this added variability on the reported results. We emphasize that our goal is not to replace benchmark methods for estimating the basic reproductive numbers, but rather to discuss the importance of the impact of R's heterogeneity on uncertainty quantification for the current COVID-19 pandemic.},
	language = {en},
	urldate = {2020-05-25},
	journal = {arXiv:2004.05272 [q-bio, stat]},
	author = {Donnat, Claire and Holmes, Susan},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.05272},
	keywords = {Statistics - Applications, Quantitative Biology - Populations and Evolution},
	file = {Donnat and Holmes - 2020 - Modeling the Heterogeneity in COVID-19's Reproduct.pdf:/mnt/data/Google Drive/Zotero/storage/KUST69GH/Donnat and Holmes - 2020 - Modeling the Heterogeneity in COVID-19's Reproduct.pdf:application/pdf}
}

@article{yaoBayesianAggregation2019,
	title = {Bayesian {Aggregation}},
	url = {http://arxiv.org/abs/1912.11218},
	abstract = {A general challenge in statistics is prediction in the presence of multiple candidate models or learning algorithms. Model aggregation tries to combine all predictive distributions from individual models, which is more stable and flexible than single model selection. In this article we describe when and how to aggregate models under the lens of Bayesian decision theory. Among two widely used methods, Bayesian model averaging (BMA) and Bayesian stacking, we compare their predictive performance, and review their theoretical optimality, probabilistic interpretation, practical implementation, and extensions in complex models.},
	urldate = {2020-05-25},
	journal = {arXiv:1912.11218 [stat]},
	author = {Yao, Yuling},
	month = dec,
	year = {2019},
	note = {arXiv: 1912.11218},
	keywords = {Statistics - Methodology},
	file = {arXiv Fulltext PDF:/mnt/data/Google Drive/Zotero/storage/U3WDTIZ5/Yao - 2019 - Bayesian Aggregation.pdf:application/pdf;arXiv.org Snapshot:/mnt/data/Google Drive/Zotero/storage/Y2K797FW/1912.html:text/html}
}

@article{nowotarskiComputingElectricitySpot2015,
	title = {Computing electricity spot price prediction intervals using quantile regression and forecast averaging},
	volume = {30},
	issn = {1613-9658},
	url = {https://doi.org/10.1007/s00180-014-0523-0},
	doi = {10.1007/s00180-014-0523-0},
	abstract = {We examine possible accuracy gains from forecast averaging in the context of interval forecasts of electricity spot prices. First, we test whether constructing empirical prediction intervals (PI) from combined electricity spot price forecasts leads to better forecasts than those obtained from individual methods. Next, we propose a new method for constructing PI—Quantile Regression Averaging (QRA)—which utilizes the concept of quantile regression and a pool of point forecasts of individual (i.e. not combined) models. While the empirical PI from combined forecasts do not provide significant gains, the QRA-based PI are found to be more accurate than those of the best individual model—the smoothed nonparametric autoregressive model.},
	language = {en},
	number = {3},
	urldate = {2020-05-27},
	journal = {Computational Statistics},
	author = {Nowotarski, Jakub and Weron, Rafał},
	month = sep,
	year = {2015},
	pages = {791--803},
	file = {Springer Full Text PDF:/mnt/data/Google Drive/Zotero/storage/73B9W3G7/Nowotarski and Weron - 2015 - Computing electricity spot price prediction interv.pdf:application/pdf}
}

@article{rafteryUsingBayesianModel2005,
	title = {Using {Bayesian} {Model} {Averaging} to {Calibrate} {Forecast} {Ensembles}},
	volume = {133},
	issn = {0027-0644},
	url = {https://journals.ametsoc.org/doi/full/10.1175/MWR2906.1},
	doi = {10.1175/MWR2906.1},
	abstract = {Ensembles used for probabilistic weather forecasting often exhibit a spread-error correlation, but they tend to be underdispersive. This paper proposes a statistical method for postprocessing ensembles based on Bayesian model averaging (BMA), which is a standard method for combining predictive distributions from different sources. The BMA predictive probability density function (PDF) of any quantity of interest is a weighted average of PDFs centered on the individual bias-corrected forecasts, where the weights are equal to posterior probabilities of the models generating the forecasts and reflect the models' relative contributions to predictive skill over the training period. The BMA weights can be used to assess the usefulness of ensemble members, and this can be used as a basis for selecting ensemble members; this can be useful given the cost of running large ensembles. The BMA PDF can be represented as an unweighted ensemble of any desired size, by simulating from the BMA predictive distribution. The BMA predictive variance can be decomposed into two components, one corresponding to the between-forecast variability, and the second to the within-forecast variability. Predictive PDFs or intervals based solely on the ensemble spread incorporate the first component but not the second. Thus BMA provides a theoretical explanation of the tendency of ensembles to exhibit a spread-error correlation but yet be underdispersive. The method was applied to 48-h forecasts of surface temperature in the Pacific Northwest in January–June 2000 using the University of Washington fifth-generation Pennsylvania State University–NCAR Mesoscale Model (MM5) ensemble. The predictive PDFs were much better calibrated than the raw ensemble, and the BMA forecasts were sharp in that 90\% BMA prediction intervals were 66\% shorter on average than those produced by sample climatology. As a by-product, BMA yields a deterministic point forecast, and this had root-mean-square errors 7\% lower than the best of the ensemble members and 8\% lower than the ensemble mean. Similar results were obtained for forecasts of sea level pressure. Simulation experiments show that BMA performs reasonably well when the underlying ensemble is calibrated, or even overdispersed.},
	number = {5},
	urldate = {2020-06-02},
	journal = {Monthly Weather Review},
	author = {Raftery, Adrian E. and Gneiting, Tilmann and Balabdaoui, Fadoua and Polakowski, Michael},
	month = may,
	year = {2005},
	note = {Publisher: American Meteorological Society},
	pages = {1155--1174},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/BVZ3BIIP/Raftery et al. - 2005 - Using Bayesian Model Averaging to Calibrate Foreca.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/HZJ3R7W2/MWR2906.html:text/html}
}

@article{rubinBayesianBootstrap1981,
	title = {The {Bayesian} {Bootstrap}},
	volume = {9},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1176345338},
	doi = {10.1214/aos/1176345338},
	abstract = {The Bayesian bootstrap is the Bayesian analogue of the bootstrap. Instead of simulating the sampling distribution of a statistic estimating a parameter, the Bayesian bootstrap simulates the posterior distribution of the parameter; operationally and inferentially the methods are quite similar. Because both methods of drawing inferences are based on somewhat peculiar model assumptions and the resulting inferences are generally sensitive to these assumptions, neither method should be applied without some consideration of the reasonableness of these model assumptions. In this sense, neither method is a true bootstrap procedure yielding inferences unaided by external assumptions.},
	language = {EN},
	number = {1},
	urldate = {2020-06-02},
	journal = {Annals of Statistics},
	author = {Rubin, Donald B.},
	month = jan,
	year = {1981},
	mrnumber = {MR600538},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Dirichlet, jackknife, Model-free inference},
	pages = {130--134},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/EIYG5S5U/Rubin - 1981 - The Bayesian Bootstrap.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/3HHY4A4F/1176345338.html:text/html}
}

@article{clarkeComparingBayesModel,
	title = {Comparing {Bayes} {Model} {Averaging} and {Stacking} {When} {Model} {Approximation} {Error} {Cannot} be {Ignored}},
	abstract = {We compare Bayes Model Averaging, BMA, to a non-Bayes form of model averaging called stacking. In stacking, the weights are no longer posterior probabilities of models; they are obtained by a technique based on cross-validation. When the correct data generating model (DGM) is on the list of models under consideration BMA is never worse than stacking and often is demonstrably better, provided that the noise level is of order commensurate with the coefﬁcients and explanatory variables. Here, however, we focus on the case that the correct DGM is not on the model list and may not be well approximated by the elements on the model list.},
	language = {en},
	author = {Clarke, Bertrand},
	pages = {30},
	file = {Clarke - Comparing Bayes Model Averaging and Stacking When .pdf:/mnt/data/Google Drive/Zotero/storage/SFV2ENGU/Clarke - Comparing Bayes Model Averaging and Stacking When .pdf:application/pdf}
}

@article{fragosoBayesianModelAveraging2018,
	title = {Bayesian {Model} {Averaging}: {A} {Systematic} {Review} and {Conceptual} {Classification}},
	volume = {86},
	copyright = {© 2017 The Authors. International Statistical Review © 2017 International Statistical Institute},
	issn = {1751-5823},
	shorttitle = {Bayesian {Model} {Averaging}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/insr.12243},
	doi = {10.1111/insr.12243},
	abstract = {Bayesian model averaging (BMA) provides a coherent and systematic mechanism for accounting for model uncertainty. It can be regarded as an direct application of Bayesian inference to the problem of model selection, combined estimation and prediction. BMA produces a straightforward model choice criterion and less risky predictions. However, the application of BMA is not always straightforward, leading to diverse assumptions and situational choices on its different aspects. Despite the widespread application of BMA in the literature, there were not many accounts of these differences and trends besides a few landmark revisions in the late 1990s and early 2000s, therefore not accounting for advancements made in the last decades. In this work, we present an account of these developments through a careful content analysis of 820 articles in BMA published between 1996 and 2016. We also develop a conceptual classification scheme to better describe this vast literature, understand its trends and future directions and provide guidance for the researcher interested in both the application and development of the methodology. The results of the classification scheme and content review are then used to discuss the present and future of the BMA literature.},
	language = {en},
	number = {1},
	urldate = {2020-06-02},
	journal = {International Statistical Review},
	author = {Fragoso, Tiago M. and Bertoli, Wesley and Louzada, Francisco},
	year = {2018},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/insr.12243},
	keywords = {Bayesian model averaging, conceptual classification scheme, qualitative content analysis, systematic review},
	pages = {1--28},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/6M4VGYYN/Fragoso et al. - 2018 - Bayesian Model Averaging A Systematic Review and .pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/XG55P67G/insr.html:text/html}
}

@article{svenssonNoteGenerationTimes2007,
	title = {A note on generation times in epidemic models},
	volume = {208},
	issn = {0025-5564},
	url = {http://www.sciencedirect.com/science/article/pii/S0025556406002094},
	doi = {10.1016/j.mbs.2006.10.010},
	abstract = {The time between the infection of a primary case and one of its secondary cases is called a generation time. The distribution (and mean) of the generation times is derived for a rather general class of epidemic models. The relation to assumptions on distributions of latency times and infectious times or more generally on random time varying infectiousness, is investigated. Serial times, defined as the times between occurrence of observable events in the progress of an infectious disease (e.g., the onset of clinical symptoms), are also considered.},
	language = {en},
	number = {1},
	urldate = {2020-06-05},
	journal = {Mathematical Biosciences},
	author = {Svensson, Åke},
	month = jul,
	year = {2007},
	keywords = {Epidemic models, Generation times, Serial times, Transmission intervals},
	pages = {300--311},
	file = {ScienceDirect Snapshot:/mnt/data/Google Drive/Zotero/storage/UDUF9FLS/S0025556406002094.html:text/html}
}

@article{brittonEstimationEmergingEpidemics2019,
	title = {Estimation in emerging epidemics: biases and remedies},
	volume = {16},
	shorttitle = {Estimation in emerging epidemics},
	url = {https://royalsocietypublishing.org/doi/full/10.1098/rsif.2018.0670},
	doi = {10.1098/rsif.2018.0670},
	abstract = {When analysing new emerging infectious disease outbreaks, one typically has observational data over a limited period of time and several parameters to estimate, such as growth rate, the basic reproduction number R0, the case fatality rate and distributions of serial intervals, generation times, latency and incubation times and times between onset of symptoms, notification, death and recovery/discharge. These parameters form the basis for predicting a future outbreak, planning preventive measures and monitoring the progress of the disease outbreak. We study inference problems during the emerging phase of an outbreak, and point out potential sources of bias, with emphasis on: contact tracing backwards in time, replacing generation times by serial intervals, multiple potential infectors and censoring effects amplified by exponential growth. These biases directly affect the estimation of, for example, the generation time distribution and the case fatality rate, but can then propagate to other estimates such as R0 and growth rate. We propose methods to remove or at least reduce bias using statistical modelling. We illustrate the theory by numerical examples and simulations.},
	number = {150},
	urldate = {2020-06-05},
	journal = {Journal of The Royal Society Interface},
	author = {Britton, Tom and Scalia Tomba, Gianpaolo},
	month = jan,
	year = {2019},
	note = {Publisher: Royal Society},
	pages = {20180670},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/UE77Z7A2/Britton and Scalia Tomba - 2019 - Estimation in emerging epidemics biases and remed.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/DZF7TEZF/rsif.2018.html:text/html}
}

@article{bracherEvaluatingEpidemicForecasts2020,
	title = {Evaluating epidemic forecasts in an interval format},
	url = {http://arxiv.org/abs/2005.12881},
	abstract = {For practical reasons, many forecasts of case, hospitalization and death counts in the context of the current COVID-19 pandemic are issued in the form of central predictive intervals at various levels. This is also the case for the forecasts collected in the COVID-19 Forecast Hub run by the UMass-Amherst Influenza Forecasting Center of Excellence. Forecast evaluation metrics like the logarithmic score, which has been applied in several infectious disease forecasting challenges, are then not available as they require full predictive distributions. This note provides an overview of how established methods for the evaluation of quantile and interval forecasts can be applied to epidemic forecasts. Specifically, we discuss the computation and interpretation of the weighted interval score, which is a proper score that approximates the continuous ranked probability score. It can be interpreted as a generalization of the absolute error to probabilistic forecasts and allows for a simple decomposition into a measure of sharpness and penalties for over- and underprediction.},
	urldate = {2020-06-06},
	journal = {arXiv:2005.12881 [q-bio, stat]},
	author = {Bracher, Johannes and Ray, Evan L. and Gneiting, Tilmann and Reich, Nicholas G.},
	month = may,
	year = {2020},
	note = {arXiv: 2005.12881},
	keywords = {Statistics - Applications, Quantitative Biology - Populations and Evolution},
	file = {arXiv Fulltext PDF:/mnt/data/Google Drive/Zotero/storage/ICGZJDHT/Bracher et al. - 2020 - Evaluating epidemic forecasts in an interval forma.pdf:application/pdf;arXiv.org Snapshot:/mnt/data/Google Drive/Zotero/storage/T3UZEKSU/2005.html:text/html}
}

@article{bracherEvaluatingEpidemicForecasts2020a,
	title = {Evaluating epidemic forecasts in an interval format},
	url = {http://arxiv.org/abs/2005.12881},
	abstract = {For practical reasons, many forecasts of case, hospitalization and death counts in the context of the current COVID-19 pandemic are issued in the form of central predictive intervals at various levels. This is also the case for the forecasts collected in the COVID19 Forecast Hub run by the UMass-Amherst Inﬂuenza Forecasting Center of Excellence. Forecast evaluation metrics like the logarithmic score, which has been applied in several infectious disease forecasting challenges, are then not available as they require full predictive distributions. This note provides an overview of how established methods for the evaluation of quantile and interval forecasts can be applied to epidemic forecasts. Speciﬁcally, we discuss the computation and interpretation of the weighted interval score, which is a proper score that approximates the continuous ranked probability score. It can be interpreted as a generalization of the absolute error to probabilistic forecasts and allows for a simple decomposition into a measure of sharpness and penalties for over- and underprediction.},
	language = {en},
	urldate = {2020-06-06},
	journal = {arXiv:2005.12881 [q-bio, stat]},
	author = {Bracher, Johannes and Ray, Evan L. and Gneiting, Tilmann and Reich, Nicholas G.},
	month = may,
	year = {2020},
	note = {arXiv: 2005.12881},
	keywords = {Statistics - Applications, Quantitative Biology - Populations and Evolution},
	file = {Bracher et al. - 2020 - Evaluating epidemic forecasts in an interval forma.pdf:/mnt/data/Google Drive/Zotero/storage/CUL2NHPC/Bracher et al. - 2020 - Evaluating epidemic forecasts in an interval forma.pdf:application/pdf}
}

@article{flaxmanEstimatingEffectsNonpharmaceutical2020,
	title = {Estimating the effects of non-pharmaceutical interventions on {COVID}-19 in {Europe}},
	copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-020-2405-7},
	doi = {10.1038/s41586-020-2405-7},
	abstract = {Following the emergence of a novel coronavirus1 (SARS-CoV-2) and its spread outside of China, Europe has experienced large epidemics. In response, many European countries have implemented unprecedented non-pharmaceutical interventions such as closure of schools and national lockdowns. We study the impact of major interventions across 11 European countries for the period from the start of COVID-19 until the 4th of May 2020 when lockdowns started to be lifted. Our model calculates backwards from observed deaths to estimate transmission that occurred several weeks prior, allowing for the time lag between infection and death. We use partial pooling of information between countries with both individual and shared effects on the reproduction number. Pooling allows more information to be used, helps overcome data idiosyncrasies, and enables more timely estimates. Our model relies on fixed estimates of some epidemiological parameters such as the infection fatality rate, does not include importation or subnational variation and assumes that changes in the reproduction number are an immediate response to interventions rather than gradual changes in behavior. Amidst the ongoing pandemic, we rely on death data that is incomplete, with systematic biases in reporting, and subject to future consolidation. We estimate that, for all the countries we consider, current interventions have been sufficient to drive the reproduction number \$\$\{R\}\_\{t\}\$\$Rt below 1 (probability \$\$\{R\}\_\{t\}{\textbackslash},\$\$Rt{\textless} 1.0 is 99.9\%) and achieve epidemic control. We estimate that, across all 11 countries, between 12 and 15 million individuals have been infected with SARS-CoV-2 up to 4th May, representing between 3.2\% and 4.0\% of the population. Our results show that major non-pharmaceutical interventions and lockdown in particular have had a large effect on reducing transmission. Continued intervention should be considered to keep transmission of SARS-CoV-2 under control.},
	language = {en},
	urldate = {2020-06-18},
	journal = {Nature},
	author = {Flaxman, Seth and Mishra, Swapnil and Gandy, Axel and Unwin, H. Juliette T. and Mellan, Thomas A. and Coupland, Helen and Whittaker, Charles and Zhu, Harrison and Berah, Tresnia and Eaton, Jeffrey W. and Monod, Mélodie and Ghani, Azra C. and Donnelly, Christl A. and Riley, Steven M. and Vollmer, Michaela A. C. and Ferguson, Neil M. and Okell, Lucy C. and Bhatt, Samir},
	month = jun,
	year = {2020},
	note = {Publisher: Nature Publishing Group},
	pages = {1--8},
	file = {Full Text:/mnt/data/Google Drive/Zotero/storage/7BXLI453/Flaxman et al. - 2020 - Estimating the effects of non-pharmaceutical inter.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/69J4UAFM/s41586-020-2405-7.html:text/html}
}

@article{morrisUsingSimulationStudies2019,
	title = {Using simulation studies to evaluate statistical methods},
	volume = {38},
	copyright = {© 2019 The Authors. Statistics in Medicine  Published by John Wiley \& Sons Ltd.},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.8086},
	doi = {10.1002/sim.8086},
	abstract = {Simulation studies are computer experiments that involve creating data by pseudo-random sampling. A key strength of simulation studies is the ability to understand the behavior of statistical methods because some “truth” (usually some parameter/s of interest) is known from the process of generating the data. This allows us to consider properties of methods, such as bias. While widely used, simulation studies are often poorly designed, analyzed, and reported. This tutorial outlines the rationale for using simulation studies and offers guidance for design, execution, analysis, reporting, and presentation. In particular, this tutorial provides a structured approach for planning and reporting simulation studies, which involves defining aims, data-generating mechanisms, estimands, methods, and performance measures (“ADEMP”); coherent terminology for simulation studies; guidance on coding simulation studies; a critical discussion of key performance measures and their estimation; guidance on structuring tabular and graphical presentation of results; and new graphical presentations. With a view to describing recent practice, we review 100 articles taken from Volume 34 of Statistics in Medicine, which included at least one simulation study and identify areas for improvement.},
	language = {en},
	number = {11},
	urldate = {2020-06-18},
	journal = {Statistics in Medicine},
	author = {Morris, Tim P. and White, Ian R. and Crowther, Michael J.},
	year = {2019},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.8086},
	keywords = {graphics for simulation, Monte Carlo, simulation design, simulation reporting, simulation studies},
	pages = {2074--2102},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/SLAE8FU9/Morris et al. - 2019 - Using simulation studies to evaluate statistical m.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/P294K42G/sim.html:text/html}
}

@article{hogeBayesianModelWeighting2020,
	title = {Bayesian {Model} {Weighting}: {The} {Many} {Faces} of {Model} {Averaging}},
	volume = {12},
	issn = {2073-4441},
	shorttitle = {Bayesian {Model} {Weighting}},
	url = {https://www.mdpi.com/2073-4441/12/2/309},
	doi = {10.3390/w12020309},
	abstract = {Model averaging makes it possible to use multiple models for one modelling task, like predicting a certain quantity of interest. Several Bayesian approaches exist that all yield a weighted average of predictive distributions. However, often, they are not properly applied which can lead to false conclusions. In this study, we focus on Bayesian Model Selection (BMS) and Averaging (BMA), Pseudo-BMS/BMA and Bayesian Stacking. We want to foster their proper use by, ﬁrst, clarifying their theoretical background and, second, contrasting their behaviours in an applied groundwater modelling task. We show that only Bayesian Stacking has the goal of model averaging for improved predictions by model combination. The other approaches pursue the quest of ﬁnding a single best model as the ultimate goal, and use model averaging only as a preliminary stage to prevent rash model choice. Improved predictions are thereby not guaranteed. In accordance with so-called M-settings that clarify the alleged relations between models and truth, we elicit which method is most promising.},
	language = {en},
	number = {2},
	urldate = {2020-06-20},
	journal = {Water},
	author = {Höge, Marvin and Guthke, Anneli and Nowak, Wolfgang},
	month = jan,
	year = {2020},
	pages = {309},
	file = {Höge et al. - 2020 - Bayesian Model Weighting The Many Faces of Model .pdf:/mnt/data/Google Drive/Zotero/storage/4J7B4XSH/Höge et al. - 2020 - Bayesian Model Weighting The Many Faces of Model .pdf:application/pdf}
}

@article{wilkinsonModellingApproachABC1977,
	title = {A modelling approach to {ABC}},
	language = {en},
	author = {Wilkinson, Richard},
	year = {1977},
	pages = {88},
	file = {Wilkinson - 1977 - A modelling approach to ABC.pdf:/mnt/data/Google Drive/Zotero/storage/95L75H5X/Wilkinson - 1977 - A modelling approach to ABC.pdf:application/pdf}
}

@article{robertsGaussianProcessesTimeseries2013,
	title = {Gaussian processes for time-series modelling},
	volume = {371},
	issn = {1364-503X, 1471-2962},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsta.2011.0550},
	doi = {10.1098/rsta.2011.0550},
	abstract = {In this paper we offer a gentle introduction to Gaussian processes for timeseries data analysis. The conceptual framework of Bayesian modelling for timeseries data is discussed and the foundations of Bayesian non-parametric modelling presented for Gaussian processes. We discuss how domain knowledge inﬂuences design of the Gaussian process models and provide case examples to highlight the approaches.},
	language = {en},
	number = {1984},
	urldate = {2020-07-10},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Roberts, S. and Osborne, M. and Ebden, M. and Reece, S. and Gibson, N. and Aigrain, S.},
	month = feb,
	year = {2013},
	pages = {20110550},
	file = {Roberts et al. - 2013 - Gaussian processes for time-series modelling.pdf:/mnt/data/Google Drive/Zotero/storage/7G2A6VD8/Roberts et al. - 2013 - Gaussian processes for time-series modelling.pdf:application/pdf}
}

@article{robertsGaussianProcessesTimeseries2013a,
	title = {Gaussian processes for time-series modelling},
	volume = {371},
	issn = {1364-503X, 1471-2962},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsta.2011.0550},
	doi = {10.1098/rsta.2011.0550},
	abstract = {In this paper we offer a gentle introduction to Gaussian processes for timeseries data analysis. The conceptual framework of Bayesian modelling for timeseries data is discussed and the foundations of Bayesian non-parametric modelling presented for Gaussian processes. We discuss how domain knowledge inﬂuences design of the Gaussian process models and provide case examples to highlight the approaches.},
	language = {en},
	number = {1984},
	urldate = {2020-07-12},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Roberts, S. and Osborne, M. and Ebden, M. and Reece, S. and Gibson, N. and Aigrain, S.},
	month = feb,
	year = {2013},
	pages = {20110550},
	file = {Roberts et al. - 2013 - Gaussian processes for time-series modelling.pdf:/mnt/data/Google Drive/Zotero/storage/KDQXM7AN/Roberts et al. - 2013 - Gaussian processes for time-series modelling.pdf:application/pdf}
}

@article{taylorComparisonAggregationMethods,
	title = {A {Comparison} of {Aggregation} {Methods} for {Probabilistic} {Forecasts} of {COVID}-19 {Mortality} in the {United} {States}},
	abstract = {The COVID-19 pandemic has placed forecasting models at the forefront of health policy making. Predictions of mortality and hospitalization help governments meet planning and resource allocation challenges. In this paper, we consider the weekly forecasting of the cumulative mortality due to COVID19 at the national and state level in the U.S. Optimal decision-making requires a forecast of a probability distribution, rather than just a single point forecast. Interval forecasts are also important, as they can support decision making and provide situational awareness. We consider the case where probabilistic forecasts have been provided by multiple forecasting teams, and we aggregate the forecasts to extract the wisdom of the crowd. With only limited information available regarding the historical accuracy of the forecasting teams, we consider aggregation methods that do not rely on a record of past accuracy. In this empirical paper, we evaluate the accuracy of aggregation methods that have been previously proposed for interval forecasts and predictions of probability distributions. These include the use of the simple average, the median, and trimming methods, which enable robust estimation and allow the aggregate forecast to reduce the impact of a tendency for the forecasting teams to be under- or overconfident. We use data that has been made publicly available from the COVID-19 Forecast Hub. Our finding is that, while the simple average provides a reasonable benchmark, greater forecast accuracy can be achieved using the median, and some forms of trimming.},
	language = {en},
	author = {Taylor, Kathryn S and Taylor, James W},
	pages = {32},
	file = {Taylor and Taylor - A Comparison of Aggregation Methods for Probabilis.pdf:/mnt/data/Google Drive/Zotero/storage/7FWJYPEV/Taylor and Taylor - A Comparison of Aggregation Methods for Probabilis.pdf:application/pdf}
}



@article{dongInteractiveWebbasedDashboard2020,
	title = {An interactive web-based dashboard to track {COVID}-19 in real time},
	volume = {20},
	issn = {1473-3099, 1474-4457},
	url = {https://www.thelancet.com/journals/laninf/article/PIIS1473-3099(20)30120-1/abstract},
	doi = {10.1016/S1473-3099(20)30120-1},
	abstract = {In December, 2019, a local outbreak of pneumonia of initially unknown cause was detected
in Wuhan (Hubei, China), and was quickly determined to be caused by a novel coronavirus,1
namely severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). The outbreak
has since spread to every province of mainland China as well as 27 other countries
and regions, with more than 70 000 confirmed cases as of Feb 17, 2020.2 In response
to this ongoing public health emergency, we developed an online interactive dashboard,
hosted by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University,
Baltimore, MD, USA, to visualise and track reported cases of coronavirus disease 2019
(COVID-19) in real time.},
	language = {English},
	number = {5},
	urldate = {2020-08-07},
	journal = {The Lancet Infectious Diseases},
	author = {Dong, Ensheng and Du, Hongru and Gardner, Lauren},
	month = may,
	year = {2020},
	pmid = {32087114},
	note = {Publisher: Elsevier},
	pages = {533--534},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/9YPUUGK6/Dong et al. - 2020 - An interactive web-based dashboard to track COVID-.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/2SPI59IE/fulltext.html:text/html}
}

@article{reichAccuracyRealtimeMultimodel2019,
	title = {Accuracy of real-time multi-model ensemble forecasts for seasonal influenza in the {U}.{S}.},
	volume = {15},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007486},
	doi = {10.1371/journal.pcbi.1007486},
	abstract = {Seasonal influenza results in substantial annual morbidity and mortality in the United States and worldwide. Accurate forecasts of key features of influenza epidemics, such as the timing and severity of the peak incidence in a given season, can inform public health response to outbreaks. As part of ongoing efforts to incorporate data and advanced analytical methods into public health decision-making, the United States Centers for Disease Control and Prevention (CDC) has organized seasonal influenza forecasting challenges since the 2013/2014 season. In the 2017/2018 season, 22 teams participated. A subset of four teams created a research consortium called the FluSight Network in early 2017. During the 2017/2018 season they worked together to produce a collaborative multi-model ensemble that combined 21 separate component models into a single model using a machine learning technique called stacking. This approach creates a weighted average of predictive densities where the weight for each component is determined by maximizing overall ensemble accuracy over past seasons. In the 2017/2018 influenza season, one of the largest seasonal outbreaks in the last 15 years, this multi-model ensemble performed better on average than all individual component models and placed second overall in the CDC challenge. It also outperformed the baseline multi-model ensemble created by the CDC that took a simple average of all models submitted to the forecasting challenge. This project shows that collaborative efforts between research teams to develop ensemble forecasting approaches can bring measurable improvements in forecast accuracy and important reductions in the variability of performance from year to year. Efforts such as this, that emphasize real-time testing and evaluation of forecasting models and facilitate the close collaboration between public health officials and modeling researchers, are essential to improving our understanding of how best to use forecasts to improve public health response to seasonal and emerging epidemic threats.},
	language = {en},
	number = {11},
	urldate = {2020-08-07},
	journal = {PLOS Computational Biology},
	author = {Reich, Nicholas G. and McGowan, Craig J. and Yamana, Teresa K. and Tushar, Abhinav and Ray, Evan L. and Osthus, Dave and Kandula, Sasikiran and Brooks, Logan C. and Crawford-Crudell, Willow and Gibson, Graham Casey and Moore, Evan and Silva, Rebecca and Biggerstaff, Matthew and Johansson, Michael A. and Rosenfeld, Roni and Shaman, Jeffrey},
	month = nov,
	year = {2019},
	note = {Publisher: Public Library of Science},
	keywords = {Epidemiology, Infectious disease surveillance, Forecasting, Infectious diseases, Public and occupational health, ensemble, Body weight, Influenza, Seasons, forecast, prediction},
	pages = {e1007486},
	annote = {Paper shows that ensembles are better than individual models for predictions},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/IJEAL83T/Reich et al. - 2019 - Accuracy of real-time multi-model ensemble forecas.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/A2M4D7R7/article.html:text/html}
}

@article{gneitingMakingEvaluatingPoint2010,
	title = {Making and {Evaluating} {Point} {Forecasts}},
	url = {http://arxiv.org/abs/0912.0902},
	abstract = {Typically, point forecasting methods are compared and assessed by means of an error measure or scoring function, such as the absolute error or the squared error. The individual scores are then averaged over forecast cases, to result in a summary measure of the predictive performance, such as the mean absolute error or the (root) mean squared error. I demonstrate that this common practice can lead to grossly misguided inferences, unless the scoring function and the forecasting task are carefully matched. Effective point forecasting requires that the scoring function be specified ex ante, or that the forecaster receives a directive in the form of a statistical functional, such as the mean or a quantile of the predictive distribution. If the scoring function is specified ex ante, the forecaster can issue the optimal point forecast, namely, the Bayes rule. If the forecaster receives a directive in the form of a functional, it is critical that the scoring function be consistent for it, in the sense that the expected score is minimized when following the directive. A functional is elicitable if there exists a scoring function that is strictly consistent for it. Expectations, ratios of expectations and quantiles are elicitable. For example, a scoring function is consistent for the mean functional if and only if it is a Bregman function. It is consistent for a quantile if and only if it is generalized piecewise linear. Similar characterizations apply to ratios of expectations and to expectiles. Weighted scoring functions are consistent for functionals that adapt to the weighting in peculiar ways. Not all functionals are elicitable; for instance, conditional value-at-risk is not, despite its popularity in quantitative finance.},
	urldate = {2020-08-09},
	journal = {arXiv:0912.0902 [math, stat]},
	author = {Gneiting, Tilmann},
	month = mar,
	year = {2010},
	note = {arXiv: 0912.0902},
	keywords = {Statistics - Methodology, Mathematics - Statistics Theory, Mathematics - Probability},
	file = {arXiv Fulltext PDF:/mnt/data/Google Drive/Zotero/storage/EG5KRUWL/Gneiting - 2010 - Making and Evaluating Point Forecasts.pdf:application/pdf;arXiv.org Snapshot:/mnt/data/Google Drive/Zotero/storage/8VSGH5LC/0912.html:text/html}
}

@article{nowotarskiComputingElectricitySpot2015a,
	title = {Computing electricity spot price prediction intervals using quantile regression and forecast averaging},
	volume = {30},
	issn = {1613-9658},
	url = {https://doi.org/10.1007/s00180-014-0523-0},
	doi = {10.1007/s00180-014-0523-0},
	abstract = {We examine possible accuracy gains from forecast averaging in the context of interval forecasts of electricity spot prices. First, we test whether constructing empirical prediction intervals (PI) from combined electricity spot price forecasts leads to better forecasts than those obtained from individual methods. Next, we propose a new method for constructing PI—Quantile Regression Averaging (QRA)—which utilizes the concept of quantile regression and a pool of point forecasts of individual (i.e. not combined) models. While the empirical PI from combined forecasts do not provide significant gains, the QRA-based PI are found to be more accurate than those of the best individual model—the smoothed nonparametric autoregressive model.},
	language = {en},
	number = {3},
	urldate = {2020-08-10},
	journal = {Computational Statistics},
	author = {Nowotarski, Jakub and Weron, Rafał},
	month = sep,
	year = {2015},
	keywords = {ensemble, qra, Quantile Regression Average},
	pages = {791--803},
	file = {Springer Full Text PDF:/mnt/data/Google Drive/Zotero/storage/9RL8366L/Nowotarski and Weron - 2015 - Computing electricity spot price prediction interv.pdf:application/pdf}
}

@misc{samabbottEpiforecastsEpiSoonStable2020,
	title = {epiforecasts/{EpiSoon}: {Stable} forecasting release},
	shorttitle = {epiforecasts/{EpiSoon}},
	url = {https://zenodo.org/record/3833807},
	abstract = {Forecasting the effective reproduction number over short timescales},
	urldate = {2020-08-12},
	publisher = {Zenodo},
	author = {Abbott, Sam and Bosse, Nikos I. and DeWitt, Michael and Rau, Andrea and Chateigner, Aurelien and Mareschal, Sylvain and Hellewell, Joel},
	month = may,
	year = {2020},
	doi = {10.5281/zenodo.3833807},
	file = {Zenodo Snapshot:/mnt/data/Google Drive/Zotero/storage/WNU6Z9R3/3833807.html:text/html}
}

@article{abbottEstimatingTimevaryingReproduction2020,
	title = {Estimating the time-varying reproduction number of {SARS}-{CoV}-2 using national and subnational case counts},
	volume = {5},
	issn = {2398-502X},
	url = {https://wellcomeopenresearch.org/articles/5-112/v1},
	doi = {10.12688/wellcomeopenres.16006.1},
	abstract = {Background:
              Interventions are now in place worldwide to reduce transmission of the novel coronavirus. Assessing temporal variations in transmission in different countries is essential for evaluating the effectiveness of public health interventions and the impact of changes in policy.
            
            
              Methods:
              We use case notification data to generate daily estimates of the time-dependent reproduction number in different regions and countries. Our modelling framework, based on open source tooling, accounts for reporting delays, so that temporal variations in reproduction number estimates can be compared directly with the times at which interventions are implemented.
            
            
              Results:
              We provide three example uses of our framework. First, we demonstrate how the toolset displays temporal changes in the reproduction number. Second, we show how the framework can be used to reconstruct case counts by date of infection from case counts by date of notification, as well as to estimate the reproduction number. Third, we show how maps can be generated to clearly show if case numbers are likely to decrease or increase in different regions. Results are shown for regions and countries worldwide on our website (
              https://epiforecasts.io/covid/
              ) and are updated daily. Our tooling is provided as an open-source R package to allow replication by others.
            
            
              Conclusions:
              This decision-support tool can be used to assess changes in virus transmission in different regions and countries worldwide. This allows policymakers to assess the effectiveness of current interventions, and will be useful for inferring whether or not transmission will increase when interventions are lifted. As well as providing daily updates on our website, we also provide adaptable computing code so that our approach can be used directly by researchers and policymakers on confidential datasets. We hope that our tool will be used to support decisions in countries worldwide throughout the ongoing COVID-19 pandemic.},
	language = {en},
	urldate = {2020-08-12},
	journal = {Wellcome Open Research},
	author = {Abbott, Sam and Hellewell, Joel and Thompson, Robin N. and Sherratt, Katharine and Gibbs, Hamish P. and Bosse, Nikos I. and Munday, James D. and Meakin, Sophie and Doughty, Emma L. and Chun, June Young and Chan, Yung-Wai Desmond and Finger, Flavio and Campbell, Paul and Endo, Akira and Pearson, Carl A. B. and Gimma, Amy and Russell, Tim and {CMMID COVID modelling group} and Flasche, Stefan and Kucharski, Adam J. and Eggo, Rosalind M. and Funk, Sebastian},
	month = jun,
	year = {2020},
	pages = {112},
	file = {Estimating the time-varying reproduction... | Wellcome Open Research:/mnt/data/Google Drive/Zotero/storage/VABAHAUK/v1.html:text/html}
}

@article{gneitingCalibratedProbabilisticForecasting2005,
	title = {Calibrated {Probabilistic} {Forecasting} {Using} {Ensemble} {Model} {Output} {Statistics} and {Minimum} {CRPS} {Estimation}},
	volume = {133},
	issn = {0027-0644},
	url = {https://journals.ametsoc.org/mwr/article/133/5/1098/67504/Calibrated-Probabilistic-Forecasting-Using},
	doi = {10.1175/MWR2904.1},
	language = {en},
	number = {5},
	urldate = {2020-08-12},
	journal = {Monthly Weather Review},
	author = {Gneiting, Tilmann and Raftery, Adrian E. and Westveld, Anton H. and Goldman, Tom},
	month = may,
	year = {2005},
	note = {Publisher: American Meteorological Society},
	pages = {1098--1118},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/DWIEDUKB/Gneiting et al. - 2005 - Calibrated Probabilistic Forecasting Using Ensembl.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/UKQZH4WN/Calibrated-Probabilistic-Forecasting-Using.html:text/html}
}

@article{nishiuraEffectiveReproductionNumber2009,
	title = {The {Effective} {Reproduction} {Number} as a {Prelude} to {Statistical} {Estimation} of {Time}-{Dependent} {Epidemic} {Trends}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7121794/},
	doi = {10.1007/978-90-481-2313-1_5},
	abstract = {Although the basic reproduction number, R
0, is useful for understanding the transmissibility of a disease and designing various intervention strategies, the classic threshold quantity theoretically assumes that the epidemic first occurs in a fully susceptible population, and hence, R
0 is essentially a mathematically defined quantity. In many instances, it is of practical importance to evaluate time-dependent variations in the transmission potential of infectious diseases. Explanation of the time course of an epidemic can be partly achieved by estimating the effective reproduction number, R(t), defined as the actual average number of secondary cases per primary case at calendar time t (for t {\textgreater}0). R(t) shows time-dependent variation due to the decline in susceptible individuals (intrinsic factors) and the implementation of control measures (extrinsic factors). If R(t){\textless}1, it suggests that the epidemic is in decline and may be regarded as being under control at time t (vice versa, if R(t){\textgreater}1). This chapter describes the primer of mathematics and statistics of R(t) and discusses other similar markers of transmissibility as a function of time.},
	urldate = {2020-08-12},
	journal = {Mathematical and Statistical Estimation Approaches in Epidemiology},
	author = {Nishiura, Hiroshi and Chowell, Gerardo},
	year = {2009},
	pmid = {null},
	pmcid = {PMC7121794},
	pages = {103--121},
	file = {PubMed Central Full Text PDF:/mnt/data/Google Drive/Zotero/storage/MIV8KGGK/Nishiura and Chowell - 2009 - The Effective Reproduction Number as a Prelude to .pdf:application/pdf}
}

@article{gosticPracticalConsiderationsMeasuring2020,
	title = {Practical considerations for measuring the effective reproductive number, {Rt}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7325187/},
	doi = {10.1101/2020.06.18.20134858},
	abstract = {Estimation of the effective reproductive number, Rt, is important for detecting changes in disease transmission over time. During the COVID-19 pandemic, policymakers and public health officials are using Rt to assess the effectiveness of interventions and to inform policy. However, estimation of Rt from available data presents several challenges, with critical implications for the interpretation of the course of the pandemic. The purpose of this document is to summarize these challenges, illustrate them with examples from synthetic data, and, where possible, make methodological recommendations. For near real-time estimation of Rt, we recommend the approach of Cori et al. (2013), which uses data from before time t and empirical estimates of the distribution of time between infections. Methods that require data from after time t, such as Wallinga and Teunis (2004), are conceptually and methodologically less suited for near real-time estimation, but may be appropriate for some retrospective analyses. We advise against using methods derived from Bettencourt and Ribeiro (2008), as the resulting Rt estimates may be biased if the underlying structural assumptions are not met. A challenge common to all approaches is reconstruction of the time series of new infections from observations occurring long after the moment of transmission. Naive approaches for dealing with observation delays, such as subtracting delays sampled from a distribution, can introduce bias. We provide suggestions for how to mitigate this and other technical challenges and highlight open problems in Rt estimation.},
	urldate = {2020-08-12},
	journal = {medRxiv},
	author = {Gostic, Katelyn M. and McGough, Lauren and Baskerville, Ed and Abbott, Sam and Joshi, Keya and Tedijanto, Christine and Kahn, Rebecca and Niehus, Rene and Hay, James and de Salazar, Pablo and Hellewell, Joel and Meakin, Sophie and Munday, James and Bosse, Nikos I. and Sherrat, Katharine and Thompson, Robin N. and White, Laura F. and Huisman, Jana S. and Scire, Jérémie and Bonhoeffer, Sebastian and Stadler, Tanja and Wallinga, Jacco and Funk, Sebastian and Lipsitch, Marc and Cobey, Sarah},
	month = jun,
	year = {2020},
	pmid = {32607522},
	pmcid = {PMC7325187},
	file = {PubMed Central Full Text PDF:/mnt/data/Google Drive/Zotero/storage/2DQ39CTT/Gostic et al. - 2020 - Practical considerations for measuring the effecti.pdf:application/pdf}
}

@misc{nationalhealthcommissionoftheprcFeb13Daily,
	title = {Feb 13: {Daily} briefing on novel coronavirus cases in {China}},
	url = {http://en.nhc.gov.cn/2020-02/13/c_76512.htm},
	urldate = {2020-08-12},
	author = {{National Health Commission of the PRC}},
	year = {2020},
	file = {Feb 13\: Daily briefing on novel coronavirus cases in China:/mnt/data/Google Drive/Zotero/storage/62JJKCDL/c_76512.html:text/html}
}

@misc{timothywrusselljoelhellewellsamabbottnickgoldinghamishgibbschristopherijarviskevinvanzandvoortcmmidncovworkinggroupstefanflascherosalindmeggowjohnedmundsadamjkucharskiUsingDelayadjustedCase2020,
	title = {Using a delay-adjusted case fatality ratio to estimate under-reporting},
	url = {https://cmmid.github.io/topics/covid19/global_cfr_estimates.html},
	abstract = {Using a corrected case fatality ratio, we calculate estimates of the level of under-reporting for any country with greater than ten deaths},
	language = {en},
	urldate = {2020-08-12},
	journal = {CMMID Repository},
	author = {{Timothy W Russell, Joel Hellewell, Sam Abbott, Nick Golding, Hamish Gibbs, Christopher I Jarvis, Kevin van Zandvoort, CMMID nCov working group, Stefan Flasche, Rosalind M Eggo, W John Edmunds \& Adam J Kucharski}},
	month = mar,
	year = {2020},
	file = {Snapshot:/mnt/data/Google Drive/Zotero/storage/KJ34HWBU/global_cfr_estimates.html:text/html}
}

@incollection{brauerCompartmentalModelsEpidemiology2008,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Mathematics}},
	title = {Compartmental {Models} in {Epidemiology}},
	isbn = {978-3-540-78911-6},
	url = {https://doi.org/10.1007/978-3-540-78911-6_2},
	abstract = {We describe and analyze compartmental models for disease transmission. We begin with models for epidemics, showing how to calculate the basic reproduction number and the final size of the epidemic. We also study models with multiple compartments, including treatment or isolation of infectives. We then consider models including births and deaths in which there may be an endemic equilibrium and study the asymptotic stability of equilibria. We conclude by studying age of infection models which give a unifying framework for more complicated compartmental models.},
	language = {en},
	urldate = {2020-08-12},
	booktitle = {Mathematical {Epidemiology}},
	publisher = {Springer},
	author = {Brauer, Fred},
	editor = {Brauer, Fred and van den Driessche, Pauline and Wu, Jianhong},
	year = {2008},
	doi = {10.1007/978-3-540-78911-6_2},
	keywords = {Compartmental Model, Contact Rate, Endemic Equilibrium, Epidemic Model, Reproduction Number},
	pages = {19--79},
	file = {Springer Full Text PDF:/mnt/data/Google Drive/Zotero/storage/28HGWCZ9/Brauer - 2008 - Compartmental Models in Epidemiology.pdf:application/pdf}
}

@article{angusProbabilityIntegralTransform1994,
	title = {The {Probability} {Integral} {Transform} and {Related} {Results}},
	volume = {36},
	issn = {0036-1445},
	url = {https://epubs.siam.org/doi/abs/10.1137/1036146},
	doi = {10.1137/1036146},
	abstract = {A simple proof of the probability integral transform theorem in probability and statistics is given that depends only on probabilistic concepts and elementary properties of continuous functions. This proof yields the theorem in its fullest generality. A similar theorem that forms the basis for the inverse method of random number generation is also discussed and contrasted to the probability integral transform theorem. Typical applications are discussed. Despite their generality and far reaching consequences, these theorems are remarkable in their simplicity and ease of proof.},
	number = {4},
	urldate = {2020-08-12},
	journal = {SIAM Review},
	author = {Angus, John E.},
	month = dec,
	year = {1994},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {652--654},
	file = {Snapshot:/mnt/data/Google Drive/Zotero/storage/8K3YQL5Q/1036146.html:text/html}
}

@article{dawidPresentPositionPotential1984,
	title = {Present {Position} and {Potential} {Developments}: {Some} {Personal} {Views} {Statistical} {Theory} the {Prequential} {Approach}},
	volume = {147},
	copyright = {© 1984 Royal Statistical Society},
	issn = {2397-2327},
	shorttitle = {Present {Position} and {Potential} {Developments}},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.2307/2981683},
	doi = {10.2307/2981683},
	abstract = {The prequential approach is founded on the premiss that the purpose of statistical inference is to make sequential probability forecasts for future observations, rather than to express information about parameters. Many traditional parametric concepts, such as consistency and efficiency, prove to have natural counterparts in this formulation, which sheds new light on these and suggests fruitful extensions.},
	language = {en},
	number = {2},
	urldate = {2020-08-12},
	journal = {Journal of the Royal Statistical Society: Series A (General)},
	author = {Dawid, A. P.},
	year = {1984},
	note = {\_eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.2307/2981683},
	keywords = {consistency, efficiency, likelihood, prequential principle, probability forecasting},
	pages = {278--290},
	file = {Snapshot:/mnt/data/Google Drive/Zotero/storage/UXRWFPAE/2981683.html:text/html;Submitted Version:/mnt/data/Google Drive/Zotero/storage/PX9RNJBW/Dawid - 1984 - Present Position and Potential Developments Some .pdf:application/pdf}
}

@article{hamillInterpretationRankHistograms2001,
	title = {Interpretation of {Rank} {Histograms} for {Verifying} {Ensemble} {Forecasts}},
	volume = {129},
	issn = {0027-0644},
	url = {https://journals.ametsoc.org/mwr/article/129/3/550/66423/Interpretation-of-Rank-Histograms-for-Verifying},
	doi = {10.1175/1520-0493(2001)129<0550:IORHFV>2.0.CO;2},
	language = {en},
	number = {3},
	urldate = {2020-08-12},
	journal = {Monthly Weather Review},
	author = {Hamill, Thomas M.},
	month = mar,
	year = {2001},
	note = {Publisher: American Meteorological Society},
	pages = {550--560},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/TFD3LRCD/Hamill - 2001 - Interpretation of Rank Histograms for Verifying En.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/AQIRNSKE/Interpretation-of-Rank-Histograms-for-Verifying.html:text/html}
}

@article{czadoPredictiveModelAssessment2009a,
	title = {Predictive {Model} {Assessment} for {Count} {Data}},
	volume = {65},
	copyright = {© 2009, The International Biometric Society},
	issn = {1541-0420},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1541-0420.2009.01191.x},
	doi = {10.1111/j.1541-0420.2009.01191.x},
	abstract = {We discuss tools for the evaluation of probabilistic forecasts and the critique of statistical models for count data. Our proposals include a nonrandomized version of the probability integral transform, marginal calibration diagrams, and proper scoring rules, such as the predictive deviance. In case studies, we critique count regression models for patent data, and assess the predictive performance of Bayesian age-period-cohort models for larynx cancer counts in Germany. The toolbox applies in Bayesian or classical and parametric or nonparametric settings and to any type of ordered discrete outcomes.},
	language = {en},
	number = {4},
	urldate = {2020-08-12},
	journal = {Biometrics},
	author = {Czado, Claudia and Gneiting, Tilmann and Held, Leonhard},
	year = {2009},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1541-0420.2009.01191.x},
	keywords = {Forecast verification, Model diagnostics, Probability integral transform, Proper scoring rule, Calibration, Predictive deviance},
	pages = {1254--1261},
	annote = {Introduces a non-randomised version of the PIT for count data. Has refererences on randomized PIT},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/SKRXXEYJ/Czado et al. - 2009 - Predictive Model Assessment for Count Data.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/Y3Z8AY8F/j.1541-0420.2009.01191.html:text/html}
}

@article{smithDiagnosticChecksNonstandard1985,
	title = {Diagnostic checks of non-standard time series models},
	volume = {4},
	issn = {0277-6693},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/for.3980040305},
	doi = {10.1002/for.3980040305},
	abstract = {Abstract Diagnostic checks have become a standard tool for helping to assess the adequacy of a forecasting system since Box and Jenkins' (1970) ARIMA modelling technique became popular. However, most of the research has developed checks for normal or second-order stationary models. This paper gives various diagnostic checks that can be performed simply on nonnormal, non-standard models such as the class of multiprocess models (Harrison and Stevens, 1976), where residuals are definitely not normal. The performance to date of these models can then be objectively scrutinized on-line. Examples, including a generalized cusum technique, are given to illustrate the effectiveness of the techniques on specific series.},
	number = {3},
	urldate = {2020-08-13},
	journal = {Journal of Forecasting},
	author = {Smith, J. Q.},
	month = jan,
	year = {1985},
	note = {Publisher: John Wiley \& Sons, Ltd},
	keywords = {Time-series analysis, Cusum, Diagnostic checks, Non-normal models},
	pages = {283--291},
	file = {Smith - 1985 - Diagnostic checks of non-standard time series mode.pdf:/mnt/data/Google Drive/Zotero/storage/Y2MDS4DR/Smith - 1985 - Diagnostic checks of non-standard time series mode.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/EJF98Y4J/for.html:text/html}
}

@article{mathesonScoringRulesContinuous1976,
	title = {Scoring {Rules} for {Continuous} {Probability} {Distributions}},
	volume = {22},
	issn = {0025-1909},
	url = {https://pubsonline.informs.org/doi/abs/10.1287/mnsc.22.10.1087},
	doi = {10.1287/mnsc.22.10.1087},
	abstract = {Personal, or subjective, probabilities are used as inputs to many inferential and decision-making models, and various procedures have been developed for the elicitation of such probabilities. Included among these elicitation procedures are scoring rules, which involve the computation of a score based on the assessor's stated probabilities and on the event that actually occurs. The development of scoring rules has, in general, been restricted to the elicitation of discrete probability distributions. In this paper, families of scoring rules for the elicitation of continuous probability distributions are developed and discussed.},
	number = {10},
	urldate = {2020-08-13},
	journal = {Management Science},
	author = {Matheson, James E. and Winkler, Robert L.},
	month = jun,
	year = {1976},
	note = {Publisher: INFORMS},
	pages = {1087--1096},
	file = {Matheson and Winkler - 1976 - Scoring Rules for Continuous Probability Distribut.pdf:/mnt/data/Google Drive/Zotero/storage/SVJ7YPP7/Matheson and Winkler - 1976 - Scoring Rules for Continuous Probability Distribut.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/H5CNZS4U/mnsc.22.10.html:text/html}
}

@article{murphyNoteRankedProbability1971,
	title = {A {Note} on the {Ranked} {Probability} {Score}},
	volume = {10},
	issn = {0021-8952},
	url = {https://journals.ametsoc.org/jamc/article/10/1/155/348891/A-Note-on-the-Ranked-Probability-Score},
	doi = {10.1175/1520-0450(1971)010<0155:ANOTRP>2.0.CO;2},
	language = {en},
	number = {1},
	urldate = {2020-08-13},
	journal = {Journal of Applied Meteorology},
	author = {Murphy, Allan H.},
	month = feb,
	year = {1971},
	note = {Publisher: American Meteorological Society},
	keywords = {RPS, ranked probability score},
	pages = {155--156},
	annote = {Also one of the first mentions of RPS
 },
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/K7IF4UNP/Murphy - 1971 - A Note on the Ranked Probability Score.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/PRWJQBMK/A-Note-on-the-Ranked-Probability-Score.html:text/html}
}

@article{epsteinScoringSystemProbability1969,
	title = {A {Scoring} {System} for {Probability} {Forecasts} of {Ranked} {Categories}},
	volume = {8},
	issn = {0021-8952},
	url = {https://journals.ametsoc.org/jamc/article/8/6/985/352208/A-Scoring-System-for-Probability-Forecasts-of},
	doi = {10.1175/1520-0450(1969)008<0985:ASSFPF>2.0.CO;2},
	language = {en},
	number = {6},
	urldate = {2020-08-13},
	journal = {Journal of Applied Meteorology},
	author = {Epstein, Edward S.},
	month = dec,
	year = {1969},
	note = {Publisher: American Meteorological Society},
	keywords = {RPS, ranked probability score},
	pages = {985--987},
	annote = {First proposal of the ranked probability score RPS.},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/XAVX39GC/Epstein - 1969 - A Scoring System for Probability Forecasts of Rank.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/CVK2YPKP/A-Scoring-System-for-Probability-Forecasts-of.html:text/html}
}

@article{murphyRankedProbabilityScore1969,
	title = {On the “{Ranked} {Probability} {Score}”},
	volume = {8},
	issn = {0021-8952},
	url = {https://journals.ametsoc.org/jamc/article/8/6/988/352211/On-the-Ranked-Probability-Score},
	doi = {10.1175/1520-0450(1969)008<0988:OTPS>2.0.CO;2},
	language = {en},
	number = {6},
	urldate = {2020-08-13},
	journal = {Journal of Applied Meteorology},
	author = {Murphy, Allan H.},
	month = dec,
	year = {1969},
	note = {Publisher: American Meteorological Society},
	pages = {988--989},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/UQL3KTF4/Murphy - 1969 - On the “Ranked Probability Score”.pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/WRJSISSC/On-the-Ranked-Probability-Score.html:text/html}
}

@article{goodRationalDecisions1952,
	title = {Rational {Decisions}},
	volume = {14},
	issn = {0035-9246},
	url = {https://www.jstor.org/stable/2984087},
	abstract = {This paper deals first with the relationship between the theory of probability and the theory of rational behaviour. A method is then suggested for encouraging people to make accurate probability estimates, a connection with the theory of information being mentioned. Finally Wald's theory of statistical decision functions is summarised and generalised and its relation to the theory of rational behaviour is discussed.},
	number = {1},
	urldate = {2020-08-13},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Good, I. J.},
	year = {1952},
	note = {Publisher: [Royal Statistical Society, Wiley]},
	keywords = {Log Score, LogS},
	pages = {107--114},
	annote = {Probably first mention of the Log Score for binary predictions},
	file = {2020 - Rational Decisions.pdf:/mnt/data/Google Drive/Zotero/storage/23458422/2020 - Rational Decisions.pdf:application/pdf}
}

@article{gelmanUnderstandingPredictiveInformation2014a,
	title = {Understanding predictive information criteria for {Bayesian} models},
	volume = {24},
	issn = {0960-3174, 1573-1375},
	url = {http://link.springer.com/10.1007/s11222-013-9416-2},
	doi = {10.1007/s11222-013-9416-2},
	abstract = {We review the Akaike, deviance, and Watanabe-Akaike information criteria from a Bayesian perspective, where the goal is to estimate expected out-of-sample-prediction error using a biascorrected adjustment of within-sample error. We focus on the choices involved in setting up these measures, and we compare them in three simple examples, one theoretical and two applied. The contribution of this review is to put all these information criteria into a Bayesian predictive context and to better understand, through small examples, how these methods can apply in practice.},
	language = {en},
	number = {6},
	urldate = {2020-08-13},
	journal = {Statistics and Computing},
	author = {Gelman, Andrew and Hwang, Jessica and Vehtari, Aki},
	month = nov,
	year = {2014},
	pages = {997--1016},
	file = {Gelman et al. - 2014 - Understanding predictive information criteria for .pdf:/mnt/data/Google Drive/Zotero/storage/U9RQUUR4/Gelman et al. - 2014 - Understanding predictive information criteria for .pdf:application/pdf}
}

@article{shannonMathematicalTheoryCommunication1948,
	title = {A {Mathematical} {Theory} of {Communication}},
	language = {en},
	author = {Shannon, C E},
	year = {1948},
	pages = {55},
	file = {Shannon - A Mathematical Theory of Communication.pdf:/mnt/data/Google Drive/Zotero/storage/TMT7GQ6U/Shannon - A Mathematical Theory of Communication.pdf:application/pdf}
}

@article{brockerReliabilitySufficiencyDecomposition2009,
	title = {Reliability, sufficiency, and the decomposition of proper scores},
	volume = {135},
	issn = {00359009, 1477870X},
	url = {http://doi.wiley.com/10.1002/qj.456},
	doi = {10.1002/qj.456},
	abstract = {Scoring rules are an important tool for evaluating the performance of probabilistic forecasting schemes. A scoring rule is called strictly proper if its expectation is optimal if and only if the forecast probability represents the true distribution of the target. In the binary case, strictly proper scoring rules allow for a decomposition into terms related to the resolution and to the reliability of the forecast. This fact is particularly well known for the Brier Score. In this paper, this result is extended to forecasts for ﬁnite–valued targets. Both resolution and reliability are shown to have a positive eﬀect on the score. It is demonstrated that resolution and reliability are directly related to forecast attributes which are desirable on grounds independent of the notion of scores. This ﬁnding can be considered an epistemological justiﬁcation of measuring forecast quality by proper scores. A link is provided to the original work of DeGroot and Fienberg (1982), extending their concepts of suﬃciency and reﬁnement. The relation to the conjectured sharpness principle of Gneiting et al. (2005a) is elucidated.},
	language = {en},
	number = {643},
	urldate = {2020-08-13},
	journal = {Quarterly Journal of the Royal Meteorological Society},
	author = {Bröcker, Jochen},
	month = jul,
	year = {2009},
	pages = {1512--1519},
	file = {Bröcker - 2009 - Reliability, sufficiency, and the decomposition of.pdf:/mnt/data/Google Drive/Zotero/storage/PPAUUYD5/Bröcker - 2009 - Reliability, sufficiency, and the decomposition of.pdf:application/pdf}
}

@article{lerchForecasterDilemmaExtreme2017,
	title = {Forecaster’s {Dilemma}: {Extreme} {Events} and {Forecast} {Evaluation}},
	volume = {32},
	issn = {0883-4237, 2168-8745},
	shorttitle = {Forecaster’s {Dilemma}},
	url = {https://projecteuclid.org/euclid.ss/1491465630},
	doi = {10.1214/16-STS588},
	abstract = {In public discussions of the quality of forecasts, attention typically focuses on the predictive performance in cases of extreme events. However, the restriction of conventional forecast evaluation methods to subsets of extreme observations has unexpected and undesired effects, and is bound to discredit skillful forecasts when the signal-to-noise ratio in the data generating process is low. Conditioning on outcomes is incompatible with the theoretical assumptions of established forecast evaluation methods, thereby confronting forecasters with what we refer to as the forecaster’s dilemma. For probabilistic forecasts, proper weighted scoring rules have been proposed as decision-theoretically justifiable alternatives for forecast evaluation with an emphasis on extreme events. Using theoretical arguments, simulation experiments and a real data study on probabilistic forecasts of U.S. inflation and gross domestic product (GDP) growth, we illustrate and discuss the forecaster’s dilemma along with potential remedies.},
	language = {EN},
	number = {1},
	urldate = {2020-08-15},
	journal = {Statistical Science},
	author = {Lerch, Sebastian and Thorarinsdottir, Thordis L. and Ravazzolo, Francesco and Gneiting, Tilmann},
	month = feb,
	year = {2017},
	mrnumber = {MR3634309},
	zmnumber = {06946266},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Diebold–Mariano test, hindsight bias, likelihood ratio test, Neyman–Pearson lemma, predictive performance, probabilistic forecast, proper weighted scoring rule, rare and extreme events},
	pages = {106--127},
	file = {Full Text PDF:/mnt/data/Google Drive/Zotero/storage/6T79RWK7/Lerch et al. - 2017 - Forecaster’s Dilemma Extreme Events and Forecast .pdf:application/pdf;Snapshot:/mnt/data/Google Drive/Zotero/storage/GCBLD2K7/1491465630.html:text/html}
}

@article{taylorComparisonAggregationMethodsa,
	title = {A {Comparison} of {Aggregation} {Methods} for {Probabilistic} {Forecasts} of {COVID}-19 {Mortality} in the {United} {States}},
	abstract = {The COVID-19 pandemic has placed forecasting models at the forefront of health policy making. Predictions of mortality and hospitalization help governments meet planning and resource allocation challenges. In this paper, we consider the weekly forecasting of the cumulative mortality due to COVID19 at the national and state level in the U.S. Optimal decision-making requires a forecast of a probability distribution, rather than just a single point forecast. Interval forecasts are also important, as they can support decision making and provide situational awareness. We consider the case where probabilistic forecasts have been provided by multiple forecasting teams, and we aggregate the forecasts to extract the wisdom of the crowd. With only limited information available regarding the historical accuracy of the forecasting teams, we consider aggregation methods that do not rely on a record of past accuracy. In this empirical paper, we evaluate the accuracy of aggregation methods that have been previously proposed for interval forecasts and predictions of probability distributions. These include the use of the simple average, the median, and trimming methods, which enable robust estimation and allow the aggregate forecast to reduce the impact of a tendency for the forecasting teams to be under- or overconfident. We use data that has been made publicly available from the COVID-19 Forecast Hub. Our finding is that, while the simple average provides a reasonable benchmark, greater forecast accuracy can be achieved using the median, and some forms of trimming.},
	language = {en},
	author = {Taylor, Kathryn S and Taylor, James W},
	pages = {32},
	file = {Taylor and Taylor - A Comparison of Aggregation Methods for Probabilis.pdf:/mnt/data/Google Drive/Zotero/storage/4EKVPIND/Taylor and Taylor - A Comparison of Aggregation Methods for Probabilis.pdf:application/pdf}
}
@Book{hyndmanrobjForecastingPrinciplesPractice2019,
  title = {Forecasting: {{Principles}} and {{Practice}}},
  shorttitle = {Forecasting},
  author = {{Hyndman, Rob J and Athanasopoulos, George}},
  year = {2019},
  abstract = {3rd edition},
  file = {/mnt/data/Google Drive/Zotero/storage/ZWX4B54Y/fpp3.html},
}
@Misc{umass-amherstinfluenzaforecastingcenterofexcellenceCovid19forecasthubOrg2020,
  title = {Covid19forecasthub.Org},
  author = {{UMass-Amherst Influenza Forecasting Center of Excellence}},
  year = {2020},
  file = {/mnt/data/Google Drive/Zotero/storage/J5JLPCXS/covid19forecasthub.org.html},
  howpublished = {https://covid19forecasthub.org/},
  journal = {https://github.com/reichlab/covid19-forecast-hub},
}
@TechReport{woodyProjectionsFirstwaveCOVID192020,
  title = {Projections for First-Wave {{COVID}}-19 Deaths across the {{US}} Using Social-Distancing Measures Derived from Mobile Phones},
  author = {Spencer Woody and Mauricio {Garcia Tec} and Maytal Dahan and Kelly Gaither and Michael Lachmann and Spencer Fox and Lauren Ancel Meyers and James G Scott},
  year = {2020},
  month = {apr},
  institution = {{Infectious Diseases (except HIV/AIDS)}},
  doi = {10.1101/2020.04.16.20068163},
  abstract = {We propose a Bayesian model for projecting first-wave COVID-19 deaths in all 50 U.S. states. Our model's projections are based on data derived from mobile-phone GPS traces, which allows us to estimate how social-distancing behavior is {"}flattening the curve{"} in each state. In a two-week look-ahead test of out-of-sample forecasting accuracy, our model significantly outperforms the widely used model from the Institute for Health Metrics and Evaluation (IHME), achieving 42\% lower prediction error: 13.2 deaths per day average error across all U.S. states, versus 22.8 deaths per day average error for the IHME model. Our model also provides an accurate, if slightly conservative, assessment of forecasting accuracy: in the same look-ahead test, 98\% of data points fell within the model's 95\% credible intervals. Our model's projections are updated daily at https://covid-19. tacc.utexas.edu/projections/},
  file = {/mnt/data/Google Drive/Zotero/storage/ADLHTZZN/Woody et al. - 2020 - Projections for first-wave COVID-19 deaths across .pdf},
  language = {en},
  type = {Preprint},
}
@Article{andersonAsymptoticTheoryCertain1952,
  title = {Asymptotic {{Theory}} of {{Certain}} {"}{{Goodness}} of {{Fit}}{"} {{Criteria Based}} on {{Stochastic Processes}}},
  author = {T. W. Anderson and D. A. Darling},
  year = {1952},
  volume = {23},
  pages = {193--212},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851},
  abstract = {The statistical problem treated is that of testing the hypothesis that n independent, identically distributed random variables have a specified continuous distribution function F(x). If Fn(x) is the empirical cumulative distribution function and {$\psi$}(t) is some nonnegative weight function (0 {$\leq$} t {$\leq$} 1), we consider \$n\^\{\textbackslash frac\{1\}\{2\}\} \textbackslash sup\_\{-\textbackslash infty\vphantom\}},
  journal = {The Annals of Mathematical Statistics},
  number = {2},
}
@Misc{ryantibshiraniQuantileStacking2020,
  title = {Quantile {{Stacking}}},
  author = {{Tibshirani, Ryan}},
  year = {2020},
  file = {/mnt/data/Google Drive/Zotero/storage/LYC38F6E/stacking_example.html},
  howpublished = {https://ryantibs.github.io/quantgen/stacking\_example.html},
}
@Article{rafteryBayesianModelAveraging1997,
  title = {Bayesian {{Model Averaging}} for {{Linear Regression Models}}},
  author = {Adrian E. Raftery and David Madigan and Jennifer A. Hoeting},
  year = {1997},
  month = {mar},
  volume = {92},
  pages = {179--191},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.1997.10473615},
  abstract = {We consider the problem of accounting for model uncertainty in linear regression models. Conditioning on a single selected model ignores model uncertainty, and thus leads to the underestimation of uncertainty when making inferences about quantities of interest. A Bayesian solution to this problem involves averaging over all possible models (i.e., combinations of predictors) when making inferences about quantities of interest. This approach is often not practical. In this article we offer two alternative approaches. First, we describe an ad hoc procedure, ``Occam's window,'' which indicates a small set of models over which a model average can be computed. Second, we describe a Markov chain Monte Carlo approach that directly approximates the exact solution. In the presence of model uncertainty, both of these model averaging procedures provide better predictive performance than any single model that might reasonably have been selected. In the extreme case where there are many candidate predictors but no relationship between any of them and the response, standard variable selection procedures often choose some subset of variables that yields a high R 2 and a highly significant overall F value. In this situation, Occam's window usually indicates the null model (or a small number of models including the null model) as the only one (or ones) to be considered thus largely resolving the problem of selecting significant models when there is no signal in the data. Software to implement our methods is available from StatLib.},
  file = {/mnt/data/Google Drive/Zotero/storage/8PJJD37C/Raftery et al. - 1997 - Bayesian Model Averaging for Linear Regression Mod.pdf;/mnt/data/Google Drive/Zotero/storage/UCQQ9WVM/01621459.1997.html},
  journal = {Journal of the American Statistical Association},
  keywords = {Bayes factor,Markov chain Monte Carlo model composition,Model uncertainty,Occam's window,Posterior model probability},
  number = {437},
}
@Article{hoetingBayesianModelAveraging1999,
  title = {Bayesian {{Model Averaging}}: {{A Tutorial}}},
  shorttitle = {Bayesian {{Model Averaging}}},
  author = {Jennifer A. Hoeting and David Madigan and Adrian E. Raftery and Chris T. Volinsky},
  year = {1999},
  volume = {14},
  pages = {382--401},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0883-4237},
  abstract = {Standard statistical practice ignores model uncertainty. Data analysts typically select a model from some class of models and then proceed as if the selected model had generated the data. This approach ignores the uncertainty in model selection, leading to over-confident inferences and decisions that are more risky than one thinks they are. Bayesian model averaging (BMA) provides a coherent mechanism for accounting for this model uncertainty. Several methods for implementing BMA have recently emerged. We discuss these methods and present a number of examples. In these examples, BMA provides improved out-of-sample predictive performance. We also provide a catalogue of currently available BMA software.},
  journal = {Statistical Science},
  number = {4},
}
@Article{brockerReliabilitySufficiencyDecomposition2009a,
  title = {{Reliability, sufficiency, and the decomposition of proper scores}},
  author = {Jochen Br{\"o}cker},
  year = {2009},
  volume = {135},
  pages = {1512--1519},
  issn = {1477-870X},
  doi = {10.1002/qj.456},
  abstract = {Scoring rules are an important tool for evaluating the performance of probabilistic forecasting schemes. A scoring rule is called strictly proper if its expectation is optimal if and only if the forecast probability represents the true distribution of the target. In the binary case, strictly proper scoring rules allow for a decomposition into terms related to the resolution and the reliability of a forecast. This fact is particularly well known for the Brier Score. In this article, this result is extended to forecasts for finite-valued targets. Both resolution and reliability are shown to have a positive effect on the score. It is demonstrated that resolution and reliability are directly related to forecast attributes that are desirable on grounds independent of the notion of scores. This finding can be considered an epistemological justification of measuring forecast quality by proper scoring rules. A link is provided to the original work of DeGroot and Fienberg, extending their concepts of sufficiency and refinement. The relation to the conjectured sharpness principle of Gneiting, et al., is elucidated. Copyright \textcopyright{} 2009 Royal Meteorological Society},
  annotation = {\_eprint: https://rmets.onlinelibrary.wiley.com/doi/pdf/10.1002/qj.456},
  copyright = {Copyright \textcopyright{} 2009 Royal Meteorological Society},
  file = {/mnt/data/Google Drive/Zotero/storage/MGX929X8/Bröcker - 2009 - Reliability, sufficiency, and the decomposition of.pdf;/mnt/data/Google Drive/Zotero/storage/52SCPZK3/qj.html},
  journal = {Quarterly Journal of the Royal Meteorological Society},
  keywords = {probabilistic forecasts,reliability,resolution,scoring rules},
  language = {fr},
  number = {643},
}

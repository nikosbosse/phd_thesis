@article{10.12688/wellcomeopenres.19380.1,
  title = {Human Judgement Forecasting of {{COVID-19}} in the {{UK}} [Version 1; Peer Review: 1 Approved with Reservations]},
  author = {Bosse, {\relax NI} and Abbott, S and Bracher, J and {van Leeuwen}, E and Cori, A and Funk, S},
  year = {2023},
  journal = {Wellcome Open Research},
  volume = {8},
  number = {416},
  doi = {10.12688/wellcomeopenres.19380.1}
}

@article{2020.03.27.20043752,
  title = {Forecasting {{COVID-19}} Impact on Hospital Bed-Days, {{ICU-days}}, Ventilator-Days and Deaths by {{US}} State in the next 4 Months},
  author = {{Murray} and JL, Christopher},
  year = {2020},
  journal = {medRxiv},
  eprint = {https://www.medrxiv.org/content/early/2020/03/30/2020.03.27.20043752.full.pdf},
  publisher = {Cold Spring Harbor Laboratory Press},
  doi = {10.1101/2020.03.27.20043752},
  abstract = {Importance This study presents the first set of estimates of predicted health service utilization and deaths due to COVID-19 by day for the next 4 months for each state in the US.Objective To determine the extent and timing of deaths and excess demand for hospital services due to COVID-19 in the US.Design, Setting, and Participants This study used data on confirmed COVID-19 deaths by day from WHO websites and local and national governments; data on hospital capacity and utilization for US states; and observed COVID-19 utilization data from select locations to develop a statistical model forecasting deaths and hospital utilization against capacity by state for the US over the next 4 months.Exposure(s) COVID-19.Main outcome(s) and measure(s) Deaths, bed and ICU occupancy, and ventilator use.Results Compared to licensed capacity and average annual occupancy rates, excess demand from COVID-19 at the peak of the pandemic in the second week of April is predicted to be 64,175 (95\% UI 7,977 to 251,059) total beds and 17,380 (95\% UI 2,432 to 57,955) ICU beds. At the peak of the pandemic, ventilator use is predicted to be 19,481 (95\% UI 9,767 to 39,674). The date of peak excess demand by state varies from the second week of April through May. We estimate that there will be a total of 81,114 (95\% UI 38,242 to 162,106) deaths from COVID-19 over the next 4 months in the US. Deaths from COVID-19 are estimated to drop below 10 deaths per day between May 31 and June 6.Conclusions and Relevance In addition to a large number of deaths from COVID-19, the epidemic in the US will place a load well beyond the current capacity of hospitals to manage, especially for ICU care. These estimates can help inform the development and implementation of strategies to mitigate this gap, including reducing non-COVID-19 demand for services and temporarily increasing system capacity. These are urgently needed given that peak volumes are estimated to be only three weeks away. The estimated excess demand on hospital systems is predicated on the enactment of social distancing measures in all states that have not done so already within the next week and maintenance of these measures throughout the epidemic, emphasizing the importance of implementing, enforcing, and maintaining these measures to mitigate hospital system overload and prevent deaths.Data availability statement A full list of data citations are available by contacting the corresponding author.Funding Statement Bill \&amp; Melinda Gates Foundation and the State of WashingtonQuestion Assuming social distancing measures are maintained, what are the forecasted gaps in available health service resources and number of deaths from the COVID-19 pandemic for each state in the United States?Findings Using a statistical model, we predict excess demand will be 64,175 (95\% UI 7,977 to 251,059) total beds and 17,380 (95\% UI 2,432 to 57,955) ICU beds at the peak of COVID-19. Peak ventilator use is predicted to be 19,481 (95\% UI 9,767 to 39,674) ventilators. Peak demand will be in the second week of April. We estimate 81,114 (95\% UI 38,242 to 162,106) deaths in the United States from COVID-19 over the next 4 months.Meaning Even with social distancing measures enacted and sustained, the peak demand for hospital services due to the COVID-19 pandemic is likely going to exceed capacity substantially. Alongside the implementation and enforcement of social distancing measures, there is an urgent need to develop and implement plans to reduce non-COVID-19 demand for and temporarily increase capacity of health facilities.Competing Interest StatementThe authors have declared no competing interest.Clinical TrialN/AFunding StatementFunding was provided by the Bill \&amp; Melinda Gates Found and the State of Washington.Author DeclarationsAll relevant ethical guidelines have been followed; any necessary IRB and/or ethics committee approvals have been obtained and details of the IRB/oversight body are included in the manuscript.YesAll necessary patient/participant consent has been obtained and the appropriate institutional forms have been archived.YesI understand that all clinical trials and any other prospective interventional studies must be registered with an ICMJE-approved registry, such as ClinicalTrials.gov. I confirm that any such study reported in the manuscript has been registered and the trial registration ID is provided (note: if posting a prospective study registered retrospectively, please provide a statement in the trial ID field explaining why the study was not registered in advance).YesI have followed all appropriate research reporting guidelines and uploaded the relevant EQUATOR Network research reporting checklist(s) and other pertinent material as supplementary files, if applicable.YesA full list of data citations are available by contacting the corresponding author.},
  elocation-id = {2020.03.27.20043752}
}

@article{abbottEstimatingIncreaseReproduction2021,
  title = {Estimating the Increase in Reproduction Number Associated with the {{Delta}} Variant Using Local Area Dynamics in {{England}}},
  author = {Abbott, Sam and Group, CMMID COVID-19 Working and Kucharski, Adam J. and Funk, Sebastian},
  year = {2021},
  month = dec,
  pages = {2021.11.30.21267056},
  doi = {10.1101/2021.11.30.21267056},
  urldate = {2023-03-29},
  abstract = {Background Local estimates of the time-varying effective reproduction number (Rt) of COVID-19 in England became increasingly heterogeneous during April and May 2021. This may have been attributable to the spread of the Delta SARS-CoV-2 variant. This paper documents real-time analysis that aimed to investigate the association between changes in the proportion of positive cases that were S-gene positive, an indicator of the Delta variant against a background of the previously predominant Alpha variant, and the estimated time-varying Rt at the level of upper-tier local authorities (UTLA). Method We explored the relationship between the proportion of samples that were S-gene positive and the Rt of test-positive cases over time from the 23 February 2021 to the 25 May 2021. Effective reproduction numbers were estimated using the EpiNow2 R package independently for each local authority using two different estimates of the generation time. We then fit a range of regression models to estimate a multiplicative relationship between S-gene positivity and weekly mean Rt estimate. Results We found evidence of an association between increased mean Rt estimates and the proportion of S-gene positives across all models evaluated with the magnitude of the effect increasing as model flexibility was decreased. Models that adjusted for either national level or NHS region level time-varying residuals were found to fit the data better, suggesting potential unexplained confounding. Conclusions Our results indicated that even after adjusting for time-varying residuals between NHS regions, S-gene positivity was associated with an increase in the effective reproduction number of COVID-19. These findings were robust across a range of models and generation time assumptions, though the specific effect size was variable depending on the assumptions used. The lower bound of the estimated effect indicated that the reproduction number of Delta was above 1 in almost all local authorities throughout the period of investigation.},
  copyright = {{\copyright} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/NS9YJ49P/Abbott et al. - 2021 - Estimating the increase in reproduction number ass.pdf}
}

@article{abbottEstimatingTimevaryingReproduction2020a,
  title = {Estimating the Time-Varying Reproduction Number of {{SARS-CoV-2}} Using National and Subnational Case Counts},
  author = {Abbott, Sam and Hellewell, Joel and Thompson, Robin N. and Sherratt, Katharine and Gibbs, Hamish P. and Bosse, Nikos I. and Munday, James D. and Meakin, Sophie and Doughty, Emma L. and Chun, June Young and Chan, Yung-Wai Desmond and Finger, Flavio and Campbell, Paul and Endo, Akira and Pearson, Carl A. B. and Gimma, Amy and Russell, Tim and modelling Group, CMMID COVID and Flasche, Stefan and Kucharski, Adam J. and Eggo, Rosalind M. and Funk, Sebastian},
  year = {2020},
  month = jun,
  doi = {10.12688/wellcomeopenres.16006.1},
  urldate = {2021-10-14},
  abstract = {Background: Interventions are now in place worldwide to reduce transmission of the novel coronavirus. Assessing temporal variations in transmission in different countries is essential for evaluating the effectiveness of public health interventions and the impact of changes in policy. Methods: We use case notification data to generate daily estimates of the time-dependent reproduction number in different regions and countries. Our modelling framework, based on open source tooling, accounts for reporting delays, so that temporal variations in reproduction number estimates can be compared directly with the times at which interventions are implemented. Results: We provide three example uses of our framework. First, we demonstrate how the toolset displays temporal changes in the reproduction number. Second, we show how the framework can be used to reconstruct case counts by date of infection from case counts by date of notification, as well as to estimate the reproduction number. Third, we show how maps can be generated to clearly show if case numbers are likely to decrease or increase in different regions. Results are shown for regions and countries worldwide on our website ( https://epiforecasts.io/covid/ ) and are updated daily. Our tooling is provided as an open-source R package to allow replication by others. Conclusions: This decision-support tool can be used to assess changes in virus transmission in different regions and countries worldwide. This allows policymakers to assess the effectiveness of current interventions, and will be useful for inferring whether or not transmission will increase when interventions are lifted. As well as providing daily updates on our website, we also provide adaptable computing code so that our approach can be used directly by researchers and policymakers on confidential datasets. We hope that our tool will be used to support decisions in countries worldwide throughout the ongoing COVID-19 pandemic.},
  copyright = {http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  keywords = {covid-19,forecasting,SARS-CoV-2,surveillance,time-varying reproduction number},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/JTKE62HJ/Abbott et al. - 2020 - Estimating the time-varying reproduction number of.pdf}
}

@misc{abbottEvaluatingEpidemiologicallyMotivated2022,
  title = {Evaluating an Epidemiologically Motivated Surrogate Model of a Multi-Model Ensemble},
  author = {Abbott, Sam and Sherratt, Katharine and Bosse, Nikos and Gruson, Hugo and Bracher, Johannes and Funk, Sebastian},
  year = {2022},
  month = oct,
  pages = {2022.10.12.22280917},
  publisher = {medRxiv},
  doi = {10.1101/2022.10.12.22280917},
  urldate = {2023-01-15},
  abstract = {Multi-model and multi-team ensemble forecasts have become widely used to generate reliable short-term predictions of infectious disease spread. Notably, various public health agencies have used them to leverage academic disease modelling during the COVID-19 pandemic. However, ensemble forecasts are difficult to interpret and require extensive effort from numerous participating groups as well as a coordination team. In other fields, resource usage has been reduced by training simplified models that reproduce some of the observed behaviour of more complex models. Here we used observations of the behaviour of the European COVID-19 Forecast Hub ensemble combined with our own forecasting experience to identify a set of properties present in current ensemble forecasts. We then developed a parsimonious forecast model intending to mirror these properties. We assess forecasts generated from this model in real time over six months (the 15th of January 2022 to the 19th of July 2022) and for multiple European countries. We focused on forecasts of cases one to four weeks ahead and compared them to those by the European forecast hub ensemble. We find that the surrogate model behaves qualitatively similarly to the ensemble in many instances, though with increased uncertainty and poorer performance around periods of peak incidence (as measured by the Weighted Interval Score). The performance differences, however, seem to be partially due to a subset of time points, and the proposed model appears better probabilistically calibrated than the ensemble. We conclude that our simplified forecast model may have captured some of the dynamics of the hub ensemble, but more work is needed to understand the implicit epidemiological model that it represents.},
  archiveprefix = {medRxiv},
  copyright = {{\copyright} 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  annotation = {note: Preprint, {\textbackslash}url\{https://www.medrxiv.org/content/10.1101/2022.10.12.22280917v1\}},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/63KP8PMA/Abbott et al. - 2022 - Evaluating an epidemiologically motivated surrogat.pdf}
}

@misc{abbottTransmissibilityNovelCoronavirus2020,
  title = {The Transmissibility of Novel {{Coronavirus}} in the Early Stages of the 2019-20 Outbreak in {{Wuhan}}: {{Exploring}} Initial Point-Source Exposure Sizes and Durations Using Scenario Analysis},
  shorttitle = {The Transmissibility of Novel {{Coronavirus}} in the Early Stages of the 2019-20 Outbreak in {{Wuhan}}},
  author = {Abbott, Sam and Hellewell, Joel and Munday, James and nCoV working Group, {\relax CMMID} and Funk, Sebastian},
  year = {2020},
  month = feb,
  number = {5:17},
  institution = {Wellcome Open Research},
  doi = {10.12688/wellcomeopenres.15718.1},
  urldate = {2021-10-14},
  abstract = {Background : The current novel coronavirus outbreak appears to have originated from a point-source exposure event at Huanan seafood wholesale market in Wuhan, China. There is still uncertainty around the scale and duration of this exposure event. This has implications for the estimated transmissibility of the coronavirus and as such, these potential scenarios should be explored. \&nbsp; Methods : We used a stochastic branching process model, parameterised with available data where possible and otherwise informed by the 2002-2003 Severe Acute Respiratory Syndrome (SARS) outbreak, to simulate the Wuhan outbreak. We evaluated scenarios for the following parameters: the size, and duration of the initial transmission event, the serial interval, and the reproduction number (R0). We restricted model simulations based on the number of observed cases on the 25th of January, accepting samples that were within a 5\% interval on either side of this estimate. Results : Using a pre-intervention SARS-like serial interval suggested a larger initial transmission event and a higher R0 estimate. Using a SARs-like serial interval we found that the most likely scenario produced an R0 estimate between 2-2.7 (90\% credible interval (CrI)). A pre-intervention SARS-like serial interval resulted in an R0 estimate between 2-3 (90\% CrI). There were other plausible scenarios with smaller events sizes and longer duration that had comparable R0 estimates. There were very few simulations that were able to reproduce the observed data when R0 was less than 1. Conclusions : Our results indicate that an R0 of less than 1 was highly unlikely unless the size of the initial exposure event was much greater than currently reported. We found that R0 estimates were comparable across scenarios with decreasing event size and increasing duration. Scenarios with a pre-intervention SARS-like serial interval resulted in a higher R0 and were equally plausible to scenarios with SARs-like serial intervals.},
  copyright = {http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  keywords = {coronavirus,modelling,outbreak,transmission,wuhan},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/RJAYMJKC/Abbott et al. - 2020 - The transmissibility of novel Coronavirus in the e.pdf}
}

@misc{aerzteblattSARSCoV2DiagnostikRKIPasst2020,
  title = {{SARS-CoV-2-Diagnostik: RKI passt Testempfehlungen an}},
  shorttitle = {{SARS-CoV-2-Diagnostik}},
  author = {{\"A}rzteblatt, Redaktion Deutsches, Deutscher {\"A}rzteverlag GmbH},
  year = {2020},
  month = nov,
  journal = {Deutsches {\"A}rzteblatt},
  urldate = {2021-05-30},
  abstract = {Berlin -- Mittels einer Anpassung der Testkriterien f{\"u}r SARS-CoV-2-Infektionen an die Herbst- und Wintersaison will das Robert-Koch-Institut (RKI) eine... \#COVID-19},
  howpublished = {https://www.aerzteblatt.de/nachrichten/118001/SARS-CoV-2-Diagnostik-RKI-passt-Testempfehlungen-an},
  langid = {ngerman}
}

@article{alhussainAssuranceClinicalTrial2020,
  title = {Assurance for Clinical Trial Design with Normally Distributed Outcomes: {{Eliciting}} Uncertainty about Variances},
  shorttitle = {Assurance for Clinical Trial Design with Normally Distributed Outcomes},
  author = {Alhussain, Ziyad A. and Oakley, Jeremy E.},
  year = {2020},
  month = nov,
  journal = {Pharmaceutical Statistics},
  volume = {19},
  number = {6},
  pages = {827--839},
  issn = {1539-1604, 1539-1612},
  doi = {10.1002/pst.2040},
  urldate = {2021-11-06},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/SM27I346/Alhussain and Oakley - 2020 - Assurance for clinical trial design with normally .pdf}
}

@article{andersonAsymptoticTheoryCertain1952,
  title = {Asymptotic {{Theory}} of {{Certain}} "{{Goodness}} of {{Fit}}" {{Criteria Based}} on {{Stochastic Processes}}},
  author = {Anderson, T. W. and Darling, D. A.},
  year = {1952},
  journal = {The Annals of Mathematical Statistics},
  volume = {23},
  number = {2},
  eprint = {2236446},
  eprinttype = {jstor},
  pages = {193--212},
  publisher = {Institute of Mathematical Statistics},
  issn = {0003-4851},
  urldate = {2020-08-21},
  abstract = {The statistical problem treated is that of testing the hypothesis that n independent, identically distributed random variables have a specified continuous distribution function F(x). If Fn(x) is the empirical cumulative distribution function and {$\psi$}(t) is some nonnegative weight function (0 {$\leq$} t {$\leq$} 1), we consider \$n{\textasciicircum}\{{\textbackslash}frac\{1\}\{2\}\} {\textbackslash}sup\_\{-{\textbackslash}infty\vphantom\}}
}

@article{angusProbabilityIntegralTransform1994,
  title = {The {{Probability Integral Transform}} and {{Related Results}}},
  author = {Angus, John E.},
  year = {1994},
  month = dec,
  journal = {SIAM Review},
  volume = {36},
  number = {4},
  pages = {652--654},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0036-1445},
  doi = {10.1137/1036146},
  urldate = {2020-08-12},
  abstract = {A simple proof of the probability integral transform theorem in probability and statistics is given that depends only on probabilistic concepts and elementary properties of continuous functions. This proof yields the theorem in its fullest generality. A similar theorem that forms the basis for the inverse method of random number generation is also discussed and contrasted to the probability integral transform theorem. Typical applications are discussed. Despite their generality and far reaching consequences, these theorems are remarkable in their simplicity and ease of proof.},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/8K3YQL5Q/1036146.html}
}

@article{anscombeTransformationPoissonBinomial1948,
  title = {The {{Transformation}} of {{Poisson}}, {{Binomial}} and {{Negative-Binomial Data}}},
  author = {Anscombe, F. J.},
  year = {1948},
  journal = {Biometrika},
  volume = {35},
  number = {3/4},
  eprint = {2332343},
  eprinttype = {jstor},
  pages = {246--254},
  publisher = {[Oxford University Press, Biometrika Trust]},
  issn = {0006-3444},
  doi = {10.2307/2332343},
  urldate = {2022-12-07}
}

@article{arvanIntegratingHumanJudgement2019,
  title = {Integrating Human Judgement into Quantitative Forecasting Methods: {{A}} Review},
  shorttitle = {Integrating Human Judgement into Quantitative Forecasting Methods},
  author = {Arvan, Meysam and Fahimnia, Behnam and Reisi, Mohsen and Siemsen, Enno},
  year = {2019},
  month = jul,
  journal = {Omega},
  volume = {86},
  pages = {237--252},
  issn = {0305-0483},
  doi = {10.1016/j.omega.2018.07.012},
  urldate = {2023-02-01},
  abstract = {Product forecasts are a critical input into sourcing, procurement, production, inventory, logistics, finance and marketing decisions. Numerous quantitative models have been developed and applied to generate and improve product forecasts. The use of human judgement, either solely or in conjunction with quantitative models, has been well researched in the academic literature and is a popular forecasting approach in industry practice. In the context of judgemental forecasting, methods that integrate an expert's judgement into quantitative forecasting models are commonly referred to as ``integrating forecasting'' methods. This paper presents a systematic review of the literature of judgemental demand forecasting with a focus placed on integrating methods. We explore the role of expert opinion and contextual information and discuss the application of behaviourally informed support systems. We also provide important directions for further research in these areas.},
  langid = {english},
  keywords = {Behavioural operations,Forecasting,Integrating methods,Judgement,Review},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/J62DMSUX/Arvan et al. - 2019 - Integrating human judgement into quantitative fore.pdf;/Users/nikos/github-synced/zotero-nikos/storage/KA544AL7/S0305048317311155.html}
}

@article{atanasovDistillingWisdomCrowds2016,
  title = {Distilling the {{Wisdom}} of {{Crowds}}: {{Prediction Markets}} vs. {{Prediction Polls}}},
  shorttitle = {Distilling the {{Wisdom}} of {{Crowds}}},
  author = {Atanasov, Pavel and Rescober, Phillip and Stone, Eric and Swift, Samuel A. and {Servan-Schreiber}, Emile and Tetlock, Philip and Ungar, Lyle and Mellers, Barbara},
  year = {2016},
  month = apr,
  journal = {Management Science},
  volume = {63},
  number = {3},
  pages = {691--706},
  publisher = {INFORMS},
  issn = {0025-1909},
  doi = {10.1287/mnsc.2015.2374},
  urldate = {2021-05-30},
  abstract = {We report the results of the first large-scale, long-term, experimental test between two crowdsourcing methods: prediction markets and prediction polls. More than 2,400 participants made forecasts on 261 events over two seasons of a geopolitical prediction tournament. Forecasters were randomly assigned to either prediction markets (continuous double auction markets) in which they were ranked based on earnings, or prediction polls in which they submitted probability judgments, independently or in teams, and were ranked based on Brier scores. In both seasons of the tournament, prices from the prediction market were more accurate than the simple mean of forecasts from prediction polls. However, team prediction polls outperformed prediction markets when forecasts were statistically aggregated using temporal decay, differential weighting based on past performance, and recalibration. The biggest advantage of prediction polls was at the beginning of long-duration questions. Results suggest that prediction polls with proper scoring feedback, collaboration features, and statistical aggregation are an attractive alternative to prediction markets for distilling the wisdom of crowds.This paper was accepted by Uri Gneezy, behavioral economics.},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/I4SLHN59/Atanasov et al. - 2016 - Distilling the Wisdom of Crowds Prediction Market.pdf;/Users/nikos/github-synced/zotero-nikos/storage/KGDWE6LP/mnsc.2015.html}
}

@article{atanasovDistillingWisdomCrowds2017,
  title = {Distilling the {{Wisdom}} of {{Crowds}}: {{Prediction Markets}} vs. {{Prediction Polls}}},
  shorttitle = {Distilling the {{Wisdom}} of {{Crowds}}},
  author = {Atanasov, Pavel and Rescober, Phillip and Stone, Eric and Swift, Samuel A. and {Servan-Schreiber}, Emile and Tetlock, Philip and Ungar, Lyle and Mellers, Barbara},
  year = {2017},
  month = mar,
  journal = {Management Science},
  volume = {63},
  number = {3},
  pages = {691--706},
  issn = {0025-1909, 1526-5501},
  doi = {10.1287/mnsc.2015.2374},
  urldate = {2021-10-13},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/562HFUJE/Atanasov et al. - 2017 - Distilling the Wisdom of Crowds Prediction Market.pdf}
}

@article{augenblickBeliefMovementUncertainty,
  title = {Belief {{Movement}}, {{Uncertainty Reduction}}, \& {{Rational Updating}}},
  author = {Augenblick, Ned and Rabin, Matthew},
  pages = {54},
  abstract = {When a Bayesian learns new information and changes her beliefs, she must on average become concomitantly more certain about the state of the world. Consequently, it is rare for a Bayesian to frequently shift beliefs substantially while remaining relatively uncertain, or, conversely, become very con{\dots}dent with relatively little belief movement. We formalize this intuition by developing speci{\dots}c measures of movement and uncertainty reduction given a Bayesian's changing beliefs over time, showing that these measures are equal in expectation, and creating consequent statistical tests for Bayesianess. We then show connections between these two core concepts and four common psychological biases, suggesting that the test might be particularly good at detecting these biases. We provide support for this conclusion by simulating the performance of our test and other martingale tests. Finally, we apply our test to datasets of individual, algorithmic, and market beliefs.},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/HGNQN5FV/Augenblick and Rabin - Belief Movement, Uncertainty Reduction, & Rational.pdf}
}

@article{bartlettSquareRootTransformation1936,
  title = {The {{Square Root Transformation}} in {{Analysis}} of {{Variance}}},
  author = {Bartlett, M. S.},
  year = {1936},
  journal = {Supplement to the Journal of the Royal Statistical Society},
  volume = {3},
  number = {1},
  eprint = {2983678},
  eprinttype = {jstor},
  pages = {68--78},
  publisher = {[Wiley, Royal Statistical Society]},
  issn = {1466-6162},
  doi = {10.2307/2983678},
  urldate = {2022-12-07}
}

@article{bastIncreasedRiskHospitalisation2021,
  title = {Increased Risk of Hospitalisation and Death with the Delta Variant in the {{USA}}},
  author = {Bast, Elizabeth and Tang, Fei and Dahn, Jason and Palacio, Ana},
  year = {2021},
  month = dec,
  journal = {The Lancet Infectious Diseases},
  volume = {21},
  number = {12},
  pages = {1629--1630},
  publisher = {Elsevier},
  issn = {1473-3099, 1474-4457},
  doi = {10.1016/S1473-3099(21)00685-X},
  urldate = {2023-03-21},
  langid = {english},
  pmid = {34838221},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/XFZ5A5LB/Bast et al. - 2021 - Increased risk of hospitalisation and death with t.pdf}
}

@misc{beckerMasterThesisRike,
  title = {Master Thesis {{Rike Becker}}},
  author = {Becker, Friederike},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/KTA4PHHG/Master thesis Rike Becker.pdf}
}

@misc{bellegoDealingLogsZeros2022,
  title = {Dealing with {{Logs}} and {{Zeros}} in {{Regression Models}}},
  author = {Bell{\'e}go, Christophe and Benatia, David and Pape, Louis},
  year = {2022},
  month = mar,
  number = {arXiv:2203.11820},
  eprint = {2203.11820},
  publisher = {arXiv},
  urldate = {2023-01-12},
  abstract = {Log-linear models are prevalent in empirical research. Yet, how to handle zeros in the dependent variable remains an unsettled issue. This article clarifies it and addresses the log of zero by developing a new family of estimators called iterated Ordinary Least Squares (iOLS). This family nests standard approaches such as log-linear and Poisson regressions, offers several computational advantages, and corresponds to the correct way to perform the popular \${\textbackslash}log(Y+1)\$ transformation. We extend it to the endogenous regressor setting (i2SLS) and overcome other common issues with Poisson models, such as controlling for many fixed-effects. We also develop specification tests to help researchers select between alternative estimators. Finally, our methods are illustrated through numerical simulations and replications of landmark publications.},
  archiveprefix = {arxiv},
  keywords = {Economics - Econometrics,Statistics - Methodology},
  annotation = {note: Preprint, {\textbackslash}url\{https://arxiv.org/abs/2203.11820\}},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/C7KFPQU8/Bell√©go et al. - 2022 - Dealing with Logs and Zeros in Regression Models.pdf;/Users/nikos/github-synced/zotero-nikos/storage/UC4DFC6L/2203.html}
}

@article{bellemareCramerDistanceSolution2017a,
  title = {The {{Cramer Distance}} as a {{Solution}} to {{Biased Wasserstein Gradients}}},
  author = {Bellemare, Marc G. and Danihelka, Ivo and Dabney, Will and Mohamed, Shakir and Lakshminarayanan, Balaji and Hoyer, Stephan and Munos, R{\'e}mi},
  year = {2017},
  month = may,
  journal = {arXiv:1705.10743 [cs, stat]},
  eprint = {1705.10743},
  primaryclass = {cs, stat},
  urldate = {2021-07-21},
  abstract = {The Wasserstein probability metric has received much attention from the machine learning community. Unlike the Kullback-Leibler divergence, which strictly measures change in probability, the Wasserstein metric reflects the underlying geometry between outcomes. The value of being sensitive to this geometry has been demonstrated, among others, in ordinal regression and generative modelling. In this paper we describe three natural properties of probability divergences that reflect requirements from machine learning: sum invariance, scale sensitivity, and unbiased sample gradients. The Wasserstein metric possesses the first two properties but, unlike the Kullback-Leibler divergence, does not possess the third. We provide empirical evidence suggesting that this is a serious issue in practice. Leveraging insights from probabilistic forecasting we propose an alternative to the Wasserstein metric, the Cram{\'e}r distance. We show that the Cram{\'e}r distance possesses all three desired properties, combining the best of the Wasserstein and Kullback-Leibler divergences. To illustrate the relevance of the Cram{\'e}r distance in practice we design a new algorithm, the Cram{\'e}r Generative Adversarial Network (GAN), and show that it performs significantly better than the related Wasserstein GAN.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/FZD6WK7Q/Bellemare et al. - 2017 - The Cramer Distance as a Solution to Biased Wasser.pdf}
}

@article{bentzienDecompositionGraphicalPortrayal2014,
  title = {Decomposition and Graphical Portrayal of the Quantile Score},
  author = {Bentzien, Sabrina and Friederichs, Petra},
  year = {2014},
  journal = {Quarterly Journal of the Royal Meteorological Society},
  volume = {140},
  number = {683},
  pages = {1924--1934},
  issn = {1477-870X},
  doi = {10.1002/qj.2284},
  urldate = {2023-12-06},
  abstract = {This study expands the pool of verification methods for probabilistic weather and climate predictions by a decomposition of the quantile score (QS). The QS is a proper score function and evaluates predictive quantiles on a set of forecast--observation pairs. We introduce a decomposition of the QS in reliability, resolution and uncertainty and discuss the biases of the decomposition. Further, a reliability diagram for quantile forecasts is presented. Verification with the QS and its decomposition is illustrated on precipitation forecasts derived from the mesoscale weather prediction ensemble COSMO-DE-EPS of the German Meteorological Service. We argue that the QS is ready to become as popular as the Brier score in forecast verification.},
  copyright = {{\copyright} 2013 Royal Meteorological Society},
  langid = {english},
  keywords = {ensemble forecasting,forecast verification,probabilistic forecasting,quantile score,reliability,resolution,score decomposition},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/9BEVI827/qj.html}
}

@techreport{bernicebrownDELPHIPROCESSMETHODOLOGY1968,
  title = {{{DELPHI PROCESS}}: {{A METHODOLOGY USED FOR THE ELICITATION OF OPINIONS OF EXPERTS}}},
  shorttitle = {{{DELPHI PROCESS}}},
  author = {{Bernice Brown}},
  year = {1968},
  month = feb,
  urldate = {2023-02-04},
  abstract = {The Delphi process is representative of an important class of techniques dealing with decision making situations. It involves one of the methodological aspects of modern practice in operations research, namely the reliance on judgment of experts.},
  chapter = {Technical Reports},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/QLCMMTNL/AD0675981.pdf}
}

@article{bhattSemiMechanisticBayesianModeling,
  title = {Semi-{{Mechanistic Bayesian}} Modeling of {{COVID-19}} with {{Renewal Processes}}},
  author = {Bhatt, Samir and Ferguson, Neil and Flaxman, Seth and Gandy, Axel and Mishra, Swapnil and Scott, James A},
  pages = {14},
  abstract = {We propose a general Bayesian approach to modeling epidemics such as COVID-19. The approach grew out of specific analyses conducted during the pandemic, in particular an analysis concerning the effects of non-pharmaceutical interventions (NPIs) in reducing COVID-19 transmission in 11 European countries. The model parameterizes the time varying reproduction number Rt through a regression framework in which covariates can e.g. be governmental interventions or changes in mobility patterns. This allows a joint fit across regions and partial pooling to share strength. This innovation was critical to our timely estimates of the impact of lockdown and other NPIs in the European epidemics, whose validity was borne out by the subsequent course of the epidemic. Our framework provides a fully generative model for latent infections and observations deriving from them, including deaths, cases, hospitalizations, ICU admissions and seroprevalence surveys. One issue surrounding our model's use during the COVID-19 pandemic is the confounded nature of NPIs and mobility. We use our framework to explore this issue. We have open sourced an R package epidemia implementing our approach in Stan. Versions of the model are used by New York State, Tennessee and Scotland to estimate the current situation and make policy decisions.},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/LF4ENEU9/Bhatt et al. - Semi-Mechanistic Bayesian modeling of COVID-19 wit.pdf}
}

@article{bhattSemiMechanisticBayesianModeling2023,
  title = {Semi-{{Mechanistic Bayesian}} Modeling of {{COVID-19}} with {{Renewal Processes}}},
  author = {Bhatt, Samir and Ferguson, Neil and Flaxman, Seth and Gandy, Axel and Mishra, Swapnil and Scott, James A},
  year = {2023},
  month = feb,
  journal = {Journal of the Royal Statistical Society Series A: Statistics in Society},
  pages = {qnad030},
  issn = {0964-1998},
  doi = {10.1093/jrsssa/qnad030},
  urldate = {2023-04-10},
  abstract = {We propose a general Bayesian approach to modeling epidemics such as COVID-19. The approach grew out of specific analyses conducted during the pandemic, in particular an analysis concerning the effects of non-pharmaceutical interventions (NPIs) in reducing COVID-19 transmission in 11 European countries (Flaxman et al., 2020b). The model parameterizes the time varying reproduction number Rt through a multilevel regression framework in which covariates can be governmental interventions, changes in mobility patterns, or other behavioural measures. Bayesian multilevel modelling allows a joint fit across regions, with partial pooling to share strength. This innovation was critical to our timely estimates of the impact of lockdown and other NPIs in the European epidemics: estimates from countries at later stages in their epidemics informed those of countries at earlier stages. Originally released as Imperial College Report 13 Flaxman et al. (2020a) on 30 March 2020, the validity of this approach was borne out by the subsequent course of the epidemic. Our framework provides a fully generative model for latent infections and derived observations, including deaths, cases, hospitalizations, ICU admissions and seroprevalence surveys. One issue surrounding our model's use during the COVID-19 pandemic is the confounded nature of NPIs and mobility. We explore this issue using our R package epidemia which implements the approach in Stan. Versions of our model were used in an ongoing way by New York State, Tennessee and Scotland to estimate the current epidemic situation and make policy decisions.},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/C6L8WA5T/Bhatt et al. - 2023 - Semi-Mechanistic Bayesian modeling of COVID-19 wit.pdf;/Users/nikos/github-synced/zotero-nikos/storage/7ZE8RQSF/7060037.html}
}

@article{bhattSemimechanisticBayesianModelling2023,
  title = {Semi-Mechanistic {{Bayesian}} Modelling of {{COVID-19}} with Renewal Processes},
  author = {Bhatt, Samir and Ferguson, Neil and Flaxman, Seth and Gandy, Axel and Mishra, Swapnil and Scott, James A},
  year = {2023},
  month = oct,
  journal = {Journal of the Royal Statistical Society Series A: Statistics in Society},
  volume = {186},
  number = {4},
  pages = {601--615},
  issn = {0964-1998},
  doi = {10.1093/jrsssa/qnad030},
  urldate = {2024-06-16},
  abstract = {We propose a general Bayesian approach to modelling epidemics such as COVID-19. The approach grew out of specific analyses conducted during the pandemic, in particular, an analysis concerning the effects of non-pharmaceutical interventions (NPIs) in reducing COVID-19 transmission in 11 European countries. The model parameterises the time-varying reproduction number Rt through a multilevel regression framework in which covariates can be governmental interventions, changes in mobility patterns, or other behavioural measures. Bayesian multilevel modelling allows a joint fit across regions, with partial pooling to share strength. This innovation was critical to our timely estimates of the impact of lockdown and other NPIs in the European epidemics: estimates from countries at later stages in their epidemics informed those of countries at earlier stages. Originally released as Imperial College Reports, the validity of this approach was borne out by the subsequent course of the epidemic. Our framework provides a fully generative model for latent infections and derived observations, including deaths, cases, hospitalizations, ICU admissions, and seroprevalence surveys. In this article, we additionally explore the confounded nature of NPIs and mobility. Versions of our model were used by New York State, Tennessee, and Scotland to estimate the current epidemic situation and make policy decisions.},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/QTH7GIE3/Bhatt et al. - 2023 - Semi-mechanistic Bayesian modelling of COVID-19 wi.pdf;/Users/nikos/github-synced/zotero-nikos/storage/KA9RGBVM/7060037.html}
}

@article{bickelComparisonsQuadraticSpherical2007,
  title = {Some {{Comparisons}} among {{Quadratic}}, {{Spherical}}, and {{Logarithmic Scoring Rules}}},
  author = {Bickel, J.},
  year = {2007},
  month = jun,
  journal = {Decision Analysis},
  volume = {4},
  pages = {49--65},
  doi = {10.1287/deca.1070.0089},
  abstract = {Strictly proper scoring rules continue to play an important role in probability assessment. Although many such rules have been developed, relatively little guidance exists as to which rule is the most appropriate. In this paper, we discuss two important properties of quadratic, spherical, and logarithmic scoring rules. From an ex post perspective, we compare their rank order properties and conclude that both quadratic and spherical scoring perform poorly in this regard, relative to logarithmic. Second, from an ex ante perspective, we demonstrate that in many situations, logarithmic scoring is the method least affected by a nonlinear utility function. These results suggest that logarithmic scoring is superior when rank order results are important and/or when the assessor has a nonlinear utility function. In addition to these results, and perhaps more important, we demonstrate that nonlinear utility induces relatively little deviation from the optimal assessment under an assumption of risk neutrality. These results provide both comfort and guidance to those who would like to use scoring rules as part of the assessment process.},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/FSI6YM2I/Masterarbeit Rike Becker.pdf;/Users/nikos/github-synced/zotero-nikos/storage/UHJCIPYB/Bickel - 2007 - Some Comparisons among Quadratic, Spherical, and L.pdf}
}

@article{biggerstaffResultsCentersDisease2016,
  title = {Results from the Centers for Disease Control and Prevention's Predict the 2013--2014 {{Influenza Season Challenge}}},
  author = {Biggerstaff, Matthew and Alper, David and Dredze, Mark and Fox, Spencer and Fung, Isaac Chun-Hai and Hickmann, Kyle S. and Lewis, Bryan and Rosenfeld, Roni and Shaman, Jeffrey and Tsou, Ming-Hsiang and Velardi, Paola and Vespignani, Alessandro and Finelli, Lyn and {for the Influenza Forecasting Contest Working Group}},
  year = {2016},
  month = jul,
  journal = {BMC Infectious Diseases},
  volume = {16},
  number = {1},
  pages = {357},
  issn = {1471-2334},
  doi = {10.1186/s12879-016-1669-x},
  urldate = {2021-10-13},
  abstract = {Early insights into the timing of the start, peak, and intensity of the influenza season could be useful in planning influenza prevention and control activities. To encourage development and innovation in influenza forecasting, the Centers for Disease Control and Prevention (CDC) organized a challenge to predict the 2013--14 Unites States influenza season.},
  keywords = {Forecasting,Influenza,Modeling,Prediction},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/C554EIY4/Biggerstaff et al. - 2016 - Results from the centers for disease control and p.pdf;/Users/nikos/github-synced/zotero-nikos/storage/UFQ992UF/s12879-016-1669-x.html}
}

@article{bolinLocalScaleInvariance2023,
  title = {Local Scale Invariance and Robustness of Proper Scoring Rules},
  author = {Bolin, David and Wallin, Jonas},
  year = {2023},
  month = feb,
  journal = {Statistical Science},
  volume = {38},
  number = {1},
  pages = {140--159},
  publisher = {Institute of Mathematical Statistics},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/22-STS864},
  urldate = {2023-01-24},
  abstract = {Averages of proper scoring rules are often used to rank probabilistic forecasts. In many cases, the individual terms in these averages are based on observations and forecasts from different distributions. We show that some of the most popular proper scoring rules, such as the continuous ranked probability score (CRPS), give more importance to observations with large uncertainty, which can lead to unintuitive rankings. To describe this issue, we define the concept of local scale invariance for scoring rules. A new class of generalized proper kernel scoring rules is derived and as a member of this class we propose the scaled CRPS (SCRPS). This new proper scoring rule is locally scale invariant and, therefore, works in the case of varying uncertainty. Like the CRPS, it is computationally available for output from ensemble forecasts, and does not require the ability to evaluate densities of forecasts. We further define robustness of scoring rules, show why this also can be an important concept for average scores unless one is specifically interested in extremes, and derive new proper scoring rules that are robust against outliers. The theoretical findings are illustrated in three different applications from spatial statistics, stochastic volatility models and regression for count data.},
  keywords = {forecast ranking,Model selection,probabilistic forecasting,spatial statistics},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/VN5JJ4X2/Bolin and Wallin - 2023 - Local scale invariance and robustness of proper sc.pdf}
}

@article{bosseComparingHumanModelbased2022,
  title = {Comparing Human and Model-Based Forecasts of {{COVID-19}} in {{Germany}} and {{Poland}}},
  author = {Bosse, Nikos I. and Abbott, Sam and Bracher, Johannes and Hain, Habakuk and Quilty, Billy J. and Jit, Mark and Group, Centre for the Mathematical Modelling of Infectious Diseases COVID-19 Working and van Leeuwen, Edwin and Cori, Anne and Funk, Sebastian},
  year = {2022},
  month = sep,
  journal = {PLOS Computational Biology},
  volume = {18},
  number = {9},
  pages = {e1010405},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1010405},
  urldate = {2023-02-06},
  abstract = {Forecasts based on epidemiological modelling have played an important role in shaping public policy throughout the COVID-19 pandemic. This modelling combines knowledge about infectious disease dynamics with the subjective opinion of the researcher who develops and refines the model and often also adjusts model outputs. Developing a forecast model is difficult, resource- and time-consuming. It is therefore worth asking what modelling is able to add beyond the subjective opinion of the researcher alone. To investigate this, we analysed different real-time forecasts of cases of and deaths from COVID-19 in Germany and Poland over a 1-4 week horizon submitted to the German and Polish Forecast Hub. We compared crowd forecasts elicited from researchers and volunteers, against a) forecasts from two semi-mechanistic models based on common epidemiological assumptions and b) the ensemble of all other models submitted to the Forecast Hub. We found crowd forecasts, despite being overconfident, to outperform all other methods across all forecast horizons when forecasting cases (weighted interval score relative to the Hub ensemble 2 weeks ahead: 0.89). Forecasts based on computational models performed comparably better when predicting deaths (rel. WIS 1.26), suggesting that epidemiological modelling and human judgement can complement each other in important ways.},
  langid = {english},
  keywords = {Convolution,COVID 19,Data visualization,Epidemiology,Forecasting,Germany,Poland,Virus testing},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/QZ74W5WX/Bosse et al. - 2022 - Comparing human and model-based forecasts of COVID.pdf}
}

@article{bosseEvaluatingForecastsScoringutils2022,
  title = {Evaluating {{Forecasts}} with Scoringutils in {{R}}},
  author = {Bosse, Nikos I. and Gruson, Hugo and Cori, Anne and {van Leeuwen}, Edwin and Funk, Sebastian and Abbott, Sam},
  year = {2022},
  month = may,
  journal = {arXiv},
  eprint = {2205.07090},
  primaryclass = {stat},
  doi = {10.48550/arXiv.2205.07090},
  urldate = {2023-01-15},
  abstract = {Evaluating forecasts is essential in order to understand and improve forecasting and make forecasts useful to decision-makers. Much theoretical work has been done on the development of proper scoring rules and other scoring metrics that can help evaluate forecasts. In practice, however, conducting a forecast evaluation and comparison of different forecasters remains challenging. In this paper we introduce scoringutils, an R package that aims to greatly facilitate this process. It is especially geared towards comparing multiple forecasters, regardless of how forecasts were created, and visualising results. The package is able to handle missing forecasts and is the first R package to offer extensive support for forecasts represented through predictive quantiles, a format used by several collaborative ensemble forecasting efforts. The paper gives a short introduction to forecast evaluation, discusses the metrics implemented in scoringutils and gives guidance on when they are appropriate to use, and illustrates the application of the package using example data of forecasts for COVID-19 cases and deaths submitted to the European Forecast Hub between May and September 2021},
  archiveprefix = {arxiv},
  keywords = {Statistics - Applications,Statistics - Computation,Statistics - Methodology},
  annotation = {Preprint, {\textbackslash}url\{https://arxiv.org/abs/2205.07090\}},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/QW8YLYQZ/Bosse et al. - 2022 - Evaluating Forecasts with scoringutils in R.pdf;/Users/nikos/github-synced/zotero-nikos/storage/DNAA9E73/2205.html}
}

@article{bosseScoringEpidemiologicalForecasts2023,
  title = {Scoring Epidemiological Forecasts on Transformed Scales},
  author = {Bosse, Nikos I. and Abbott, Sam and Cori, Anne and van Leeuwen, Edwin and Bracher, Johannes and Funk, Sebastian},
  year = {2023},
  month = aug,
  journal = {PLOS Computational Biology},
  volume = {19},
  number = {8},
  pages = {e1011393},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1011393},
  urldate = {2023-11-20},
  abstract = {Forecast evaluation is essential for the development of predictive epidemic models and can inform their use for public health decision-making. Common scores to evaluate epidemiological forecasts are the Continuous Ranked Probability Score (CRPS) and the Weighted Interval Score (WIS), which can be seen as measures of the absolute distance between the forecast distribution and the observation. However, applying these scores directly to predicted and observed incidence counts may not be the most appropriate due to the exponential nature of epidemic processes and the varying magnitudes of observed values across space and time. In this paper, we argue that transforming counts before applying scores such as the CRPS or WIS can effectively mitigate these difficulties and yield epidemiologically meaningful and easily interpretable results. Using the CRPS on log-transformed values as an example, we list three attractive properties: Firstly, it can be interpreted as a probabilistic version of a relative error. Secondly, it reflects how well models predicted the time-varying epidemic growth rate. And lastly, using arguments on variance-stabilizing transformations, it can be shown that under the assumption of a quadratic mean-variance relationship, the logarithmic transformation leads to expected CRPS values which are independent of the order of magnitude of the predicted quantity. Applying a transformation of log(x + 1) to data and forecasts from the European COVID-19 Forecast Hub, we find that it changes model rankings regardless of stratification by forecast date, location or target types. Situations in which models missed the beginning of upward swings are more strongly emphasised while failing to predict a downturn following a peak is less severely penalised when scoring transformed forecasts as opposed to untransformed ones. We conclude that appropriate transformations, of which the natural logarithm is only one particularly attractive option, should be considered when assessing the performance of different models in the context of infectious disease incidence.},
  langid = {english},
  keywords = {COVID 19,Epidemiology,Europe,Forecasting,Normal distribution,Probability distribution,Public and occupational health,Statistical dispersion},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/NDS44ERL/Bosse et al. - 2023 - Scoring epidemiological forecasts on transformed s.pdf}
}

@article{bosseSupplementaryInformationHuman2023,
  title = {Supplementary {{Information}} - {{Human Judgement}} Forecasting of {{COVID-19}} in the {{UK}}},
  author = {Bosse, Nikos and Abbott, Sam and Bracher, Johannes and van Leeuwen, Edwin and Cori, Anne and Funk, Sebastian},
  year = {2023},
  month = may,
  doi = {10.5281/zenodo.7897513},
  urldate = {2023-05-04},
  abstract = {Supplementary Information for the paper~"Human Judgement forecasting of COVID-19 in the UK" by Bosse et al, Wellcome Open Research (2023).},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/FRBXUTVA/Bosse et al. - 2023 - Supplementary Information - Human Judgement foreca.pdf}
}

@article{bosseTransformationForecastsEvaluating2023,
  title = {Transformation of Forecasts for Evaluating Predictive Performance in an Epidemiological Context},
  author = {Bosse, Nikos I. and Abbott, Sam and Cori, Anne and {van Leeuwen}, Edwin and Bracher, Johannes and Funk, Sebastian},
  year = {2023},
  month = jan,
  doi = {10.1101/2023.01.23.23284722},
  urldate = {2023-02-06},
  abstract = {Abstract           Forecast evaluation plays an essential role in the development cycle of predictive epidemic models and can inform their use for public health decision-making. Common scores to evaluate epidemiological forecasts are the Continuous Ranked Probability Score (CRPS) and the Weighted Interval Score (WIS), which are both measures of the absolute distance between the forecast distribution and the observation. They are commonly applied directly to predicted and observed incidence counts, but it can be questioned whether this yields the most meaningful results given the exponential nature of epidemic processes and the several orders of magnitude that observed values can span over space and time. In this paper, we argue that log transforming counts before applying scores such as the CRPS or WIS can effectively mitigate these difficulties and yield epidemiologically meaningful and easily interpretable results. We motivate the procedure threefold using the CRPS on log-transformed counts as an example: Firstly, it can be interpreted as a probabilistic version of a relative error. Secondly, it reflects how well models predicted the time-varying epidemic growth rate. And lastly, using arguments on variance-stabilizing transformations, it can be shown that under the assumption of a quadratic mean-variance relationship, the logarithmic transformation leads to expected CRPS values which are independent of the order of magnitude of the predicted quantity. Applying the log transformation to data and forecasts from the European COVID-19 Forecast Hub, we find that it changes model rankings regardless of stratification by forecast date, location or target types. Situations in which models missed the beginning of upward swings are more strongly emphasized while failing to predict a downturn following a peak is less severely penalized. We conclude that appropriate transformations, of which the natural logarithm is only one particularly attractive option, should be considered when assessing the performance of different models in the context of infectious disease incidence.},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/C7J3FZBC/Bosse et al. - 2023 - Transformation of forecasts for evaluating predict.pdf}
}

@article{boxAnalysisTransformations1964,
  title = {An {{Analysis}} of {{Transformations}}},
  author = {Box, G. E. P. and Cox, D. R.},
  year = {1964},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {26},
  number = {2},
  eprint = {2984418},
  eprinttype = {jstor},
  pages = {211--252},
  publisher = {[Royal Statistical Society, Wiley]},
  issn = {0035-9246},
  urldate = {2023-01-09},
  abstract = {In the analysis of data it is often assumed that observations y\textsubscript{1}, y\textsubscript{2}, ..., y\textsubscript{n} are independently normally distributed with constant variance and with expectations specified by a model linear in a set of parameters {\texttheta}. In this paper we make the less restrictive assumption that such a normal, homoscedastic, linear model is appropriate after some suitable transformation has been applied to the y's. Inferences about the transformation and about the parameters of the linear model are made by computing the likelihood function and the relevant posterior distribution. The contributions of normality, homoscedasticity and additivity to the transformation are separated. The relation of the present methods to earlier procedures for finding transformations is discussed. The methods are illustrated with examples.},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/SNB9W2HD/Box and Cox - 1964 - An Analysis of Transformations.pdf}
}

@book{boxTimeSeriesAnalysis1970,
  title = {Time {{Series Analysis}}: {{Forecasting}} and {{Control}}},
  shorttitle = {Time {{Series Analysis}}},
  author = {Box, George E. P. and Jenkins, Gwilym M.},
  year = {1970},
  publisher = {Holden-Day},
  abstract = {The book is concerned with the building of models for discrete time series and dynamic systems. It describes in detail how such models may be used to obtain optimal forecasts and optimal control action. All the techniques are illustrated with examples using economic and industrial data. In Part 1, models for stationary and nonstationary time series are introduced, and their use in forecasting is discussed and exemplified. Part II is devoted to model building, and procedures for model identification, estimation, and checking which are then applied to the forecasting of seasonal time series. Part III is concerned with the building of transfer function models relating the input and output of a dynamic system computed by noise. In Part IV it is shown how transfer function and time series models may be used to design optimal feedback and feedforward control schemes. Part V contains an outline of computer programs useful in making the needed calculations and also includes charts and tables of value in identifying the models.},
  googlebooks = {5BVfnXaq03oC},
  isbn = {978-0-8162-1094-7},
  langid = {english},
  keywords = {Mathematics / Applied,Mathematics / Probability & Statistics / Stochastic Processes,Mathematics / Probability & Statistics / Time Series}
}

@article{bracherComparisonCombinationRealtime2020,
  title = {Comparison and Combination of Real-Time {{COVID19}} Forecasts in {{Germany}} and {{Poland}}},
  author = {Bracher, Johannes},
  year = {2020},
  month = oct,
  publisher = {OSF},
  urldate = {2021-05-29},
  abstract = {Short-term forecasts of cases, deaths and hospitalizations can improve situational awareness and provide an additional element to inform public health decision making during the COVID19 pandemic. While early in the pandemic only few prediction models were available, there is now a growing number of forecasts based on diverse methods and data streams. This project consists in a systematic comparison of such forecasts made for Germany and Poland. The main goals are the following: - create a database of standardized and comparable short-term forecasts. - assess the degree of agreement or disagreement between different forecasts. - systematically and quantitatively evaluate forecasts in order to identify particularly reliable prediction methods. - assess whether combinations of different forecasts (ensemble forecasts) can lead to improved predictive performance. This pre-registration serves to ensure a transparent set of rules and criteria to guide this collaborative study. Many technical and methodological aspects of this project follow the US COVID19 Forecast Hub (https://covid19forecasthub.org/). We aim for compatibility with work from the US in terms of forecast targets, evaluation and various technical aspects.},
  langid = {american},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/2TL2XVR6/k8d39.html}
}

@article{bracherEvaluatingEpidemicForecasts2021,
  title = {Evaluating Epidemic Forecasts in an Interval Format},
  author = {Bracher, Johannes and Ray, Evan L. and Gneiting, Tilmann and Reich, Nicholas G.},
  year = {2021},
  month = feb,
  journal = {PLoS computational biology},
  volume = {17},
  number = {2},
  pages = {e1008618},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1008618},
  abstract = {For practical reasons, many forecasts of case, hospitalization, and death counts in the context of the current Coronavirus Disease 2019 (COVID-19) pandemic are issued in the form of central predictive intervals at various levels. This is also the case for the forecasts collected in the COVID-19 Forecast Hub (https://covid19forecasthub.org/). Forecast evaluation metrics like the logarithmic score, which has been applied in several infectious disease forecasting challenges, are then not available as they require full predictive distributions. This article provides an overview of how established methods for the evaluation of quantile and interval forecasts can be applied to epidemic forecasts in this format. Specifically, we discuss the computation and interpretation of the weighted interval score, which is a proper score that approximates the continuous ranked probability score. It can be interpreted as a generalization of the absolute error to probabilistic forecasts and allows for a decomposition into a measure of sharpness and penalties for over- and underprediction.},
  langid = {english},
  pmcid = {PMC7880475},
  pmid = {33577550},
  keywords = {Communicable Diseases,COVID-19,Forecasting,Humans,Pandemics,Probability,SARS-CoV-2},
  annotation = {note: DOI 10.1371/journal.pcbi.1008618},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/EX37R6J8/Bracher et al. - 2021 - Evaluating epidemic forecasts in an interval forma.pdf}
}

@article{bracherNationalSubnationalShortterm2022,
  title = {National and Subnational Short-Term Forecasting of {{COVID-19}} in {{Germany}} and {{Poland}} during Early 2021},
  author = {Bracher, Johannes and Wolffram, Daniel and Deuschel, Jannik and G{\"o}rgen, Konstantin and Ketterer, Jakob L. and Ullrich, Alexander and Abbott, Sam and Barbarossa, Maria V. and Bertsimas, Dimitris and Bhatia, Sangeeta and Bodych, Marcin and Bosse, Nikos I. and Burgard, Jan Pablo and Castro, Lauren and Fairchild, Geoffrey and Fiedler, Jochen and Fuhrmann, Jan and Funk, Sebastian and Gambin, Anna and Gogolewski, Krzysztof and Heyder, Stefan and Hotz, Thomas and Kheifetz, Yuri and Kirsten, Holger and Krueger, Tyll and Krymova, Ekaterina and Leith{\"a}user, Neele and Li, Michael L. and Meinke, Jan H. and Miasojedow, B{\l}a{\.z}ej and Michaud, Isaac J. and Mohring, Jan and Nouvellet, Pierre and Nowosielski, Jedrzej M. and Ozanski, Tomasz and Radwan, Maciej and Rakowski, Franciszek and Scholz, Markus and Soni, Saksham and Srivastava, Ajitesh and Gneiting, Tilmann and Schienle, Melanie},
  year = {2022},
  month = oct,
  journal = {Communications Medicine},
  volume = {2},
  number = {1},
  pages = {1--17},
  publisher = {Nature Publishing Group},
  issn = {2730-664X},
  doi = {10.1038/s43856-022-00191-8},
  urldate = {2023-02-22},
  abstract = {During the COVID-19 pandemic there has been a strong interest in forecasts of the short-term development of epidemiological indicators to inform decision makers. In this study we evaluate probabilistic real-time predictions of confirmed cases and deaths from COVID-19 in Germany and Poland for the period from January through April 2021.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Epidemiology,Viral infection},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/N3WD8QGU/Bracher et al. - 2022 - National and subnational short-term forecasting of.pdf}
}

@article{bracherPreregisteredShorttermForecasting2021,
  title = {A Pre-Registered Short-Term Forecasting Study of {{COVID-19}} in {{Germany}} and {{Poland}} during the Second Wave},
  author = {Bracher, J. and Wolffram, D. and Deuschel, J. and G{\"o}rgen, K. and Ketterer, J. L. and Ullrich, A. and Abbott, S. and Barbarossa, M. V. and Bertsimas, D. and Bhatia, S. and Bodych, M. and Bosse, N. I. and Burgard, J. P. and Castro, L. and Fairchild, G. and Fuhrmann, J. and Funk, S. and Gogolewski, K. and Gu, Q. and Heyder, S. and Hotz, T. and Kheifetz, Y. and Kirsten, H. and Krueger, T. and Krymova, E. and Li, M. L. and Meinke, J. H. and Michaud, I. J. and Niedzielewski, K. and O{\.z}a{\'n}ski, T. and Rakowski, F. and Scholz, M. and Soni, S. and Srivastava, A. and Zieli{\'n}ski, J. and Zou, D. and Gneiting, T. and Schienle, M.},
  year = {2021},
  month = aug,
  journal = {Nature Communications},
  volume = {12},
  number = {1},
  pages = {5173},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-021-25207-0},
  urldate = {2023-02-22},
  abstract = {Disease modelling has had considerable policy impact during the ongoing COVID-19 pandemic, and it is increasingly acknowledged that combining multiple models can improve the reliability of outputs. Here we report insights from ten weeks of collaborative short-term forecasting of COVID-19 in Germany and Poland (12 October--19 December 2020). The study period covers the onset of the second wave in both countries, with tightening non-pharmaceutical interventions (NPIs) and subsequently a decay (Poland) or plateau and renewed increase (Germany) in reported cases. Thirteen independent teams provided probabilistic real-time forecasts of COVID-19 cases and deaths. These were reported for lead times of one to four weeks, with evaluation focused on one- and two-week horizons, which are less affected by changing NPIs. Heterogeneity between forecasts was considerable both in terms of point predictions and forecast spread. Ensemble forecasts showed good relative performance, in particular in terms of coverage, but did not clearly dominate single-model predictions. The study was preregistered and will be followed up in future phases of the pandemic.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Computational models,Epidemiology,SARS-CoV-2,Statistics},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/7YDBDE7G/Bracher et al. - 2021 - A pre-registered short-term forecasting study of C.pdf}
}

@article{bracherPreregisteredShorttermForecasting2021a,
  title = {A Pre-Registered Short-Term Forecasting Study of {{COVID-19}} in {{Germany}} and {{Poland}} during the Second Wave},
  author = {Bracher, Johannes and Wolffram, Daniel and Deuschel, Jannik and G{\"o}rgen, Konstantin and Ketterer, Jakob L. and Ullrich, Alexander and Abbott, Sam and Barbarossa, Maria V. and Bertsimas, Dimitris and Bhatia, Sangeeta and Bodych, Marcin and Bosse, Nikos I. and Burgard, Jan Pablo and Castro, Lauren and Fairchild, Geoffrey and Fuhrmann, Jan and Funk, Sebastian and Gogolewski, Krzysztof and Gu, Quanquan and Heyder, Stefan and Hotz, Thomas and Kheifetz, Yuri and Kirsten, Holger and Krueger, Tyll and Krymova, Ekaterina and Li, Michael L. and Meinke, Jan H. and Michaud, Isaac J. and Niedzielewski, K. and O{\.z}a{\'n}ski, T. and Rakowski, F. and Scholz, Markus and Soni, Saksham and Srivastava, Ajitesh and Zieli{\'n}ski, J. and Zou, Difan and Gneiting, Tilmann and Schienle, Melanie},
  year = {2021},
  month = aug,
  journal = {Nature Communications},
  volume = {12},
  number = {1},
  pages = {5173},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-021-25207-0},
  urldate = {2023-12-07},
  abstract = {Disease modelling has had considerable policy impact during the ongoing COVID-19 pandemic, and it is increasingly acknowledged that combining multiple models can improve the reliability of outputs. Here we report insights from ten weeks of collaborative short-term forecasting of COVID-19 in Germany and Poland (12 October--19 December 2020). The study period covers the onset of the second wave in both countries, with tightening non-pharmaceutical interventions (NPIs) and subsequently a decay (Poland) or plateau and renewed increase (Germany) in reported cases. Thirteen independent teams provided probabilistic real-time forecasts of COVID-19 cases and deaths. These were reported for lead times of one to four weeks, with evaluation focused on one- and two-week horizons, which are less affected by changing NPIs. Heterogeneity between forecasts was considerable both in terms of point predictions and forecast spread. Ensemble forecasts showed good relative performance, in particular in terms of coverage, but did not clearly dominate single-model predictions. The study was preregistered and will be followed up in future phases of the pandemic.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Computational models,Epidemiology,SARS-CoV-2,Statistics},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/8ADJWVQ6/Bracher et al. - 2021 - A pre-registered short-term forecasting study of C.pdf}
}

@article{bracherShorttermForecastingCOVID192021,
  title = {Short-Term Forecasting of {{COVID-19}} in {{Germany}} and {{Poland}} during the Second Wave -- a Preregistered Study},
  author = {Bracher, Johannes and Wolffram, Daniel and Deuschel, J. and G{\"o}rgen, K. and Ketterer, J. L. and Ullrich, A. and Abbott, S. and Barbarossa, M. V. and Bertsimas, D. and Bhatia, S. and Bodych, M. and Bosse, Nikos I. and Burgard, J. P. and Castro, L. and Fairchild, G. and Fuhrmann, J. and Funk, S. and Gogolewski, K. and Gu, Q. and Heyder, S. and Hotz, T. and Kheifetz, Y. and Kirsten, H. and Krueger, T. and Krymova, E. and Li, M. L. and Meinke, J. H. and Michaud, I. J. and Niedzielewski, K. and O{\.z}a{\'n}ski, T. and Rakowski, F. and Scholz, M. and Soni, S. and Srivastava, A. and Zieli{\'n}ski, J. and Zou, D. and Gneiting, T. and Schienle, M.},
  year = {2021},
  month = jan,
  journal = {medRxiv},
  pages = {2020.12.24.20248826},
  publisher = {Cold Spring Harbor Laboratory Press},
  doi = {10.1101/2020.12.24.20248826},
  urldate = {2021-04-01},
  abstract = {{$<$}h3{$>$}Abstract{$<$}/h3{$>$} {$<$}p{$>$}We report insights from ten weeks of collaborative COVID-19 forecasting for Germany and Poland (12 October -- 19 December 2020). The study period covers the onset of the second wave in both countries, with tightening non-pharmaceutical interventions (NPIs) and subsequently a decay (Poland) or plateau and renewed increase (Germany) in reported cases. Thirteen independent teams provided probabilistic real-time forecasts of COVID-19 cases and deaths. These were reported for lead times of one to four weeks, with evaluation focused on one- and two-week horizons, which are less affected by changing NPIs. Heterogeneity between forecasts was considerable both in terms of point predictions and forecast spread. Ensemble forecasts showed good relative performance, in particular in terms of coverage, but did not clearly dominate single-model predictions. The study was preregistered and will be followed up in future phases of the pandemic.{$<$}/p{$>$}},
  copyright = {{\copyright} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/BHPBLCD9/Bracher et al. - 2021 - Short-term forecasting of COVID-19 in Germany and .pdf;/Users/nikos/github-synced/zotero-nikos/storage/I3ULULUZ/2020.12.24.20248826v2.html}
}

@article{brehmerProperizationConstructingProper2020,
  title = {Properization: Constructing Proper Scoring Rules via {{Bayes}} Acts},
  shorttitle = {Properization},
  author = {Brehmer, Jonas R. and Gneiting, Tilmann},
  year = {2020},
  month = jun,
  journal = {Annals of the Institute of Statistical Mathematics},
  volume = {72},
  number = {3},
  pages = {659--673},
  issn = {1572-9052},
  doi = {10.1007/s10463-019-00705-7},
  urldate = {2021-03-26},
  abstract = {Scoring rules serve to quantify predictive performance. A scoring rule is proper if truth telling is an optimal strategy in expectation. Subject to customary regularity conditions, every scoring rule can be made proper, by applying a special case of the Bayes act construction studied by Gr{\"u}nwald and Dawid (Ann Stat 32:1367--1433, 2004) and Dawid (Ann Inst Stat Math 59:77--93, 2007), to which we refer as properization. We discuss examples from the recent literature and apply the construction to create new types, and reinterpret existing forms, of proper scoring rules and consistent scoring functions. In an abstract setting, we formulate sufficient conditions under which Bayes acts exist and scoring rules can be made proper.},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/5RSYDJY5/Brehmer and Gneiting - 2020 - Properization constructing proper scoring rules v.pdf}
}

@article{brierVerificationForecastsExpressed1950,
  title = {Verification of {{Forecasts Expressed}} in {{Terms}} of {{Probability}}},
  author = {Brier, Glenn W.},
  year = {1950},
  month = jan,
  journal = {Monthly Weather Review},
  volume = {78},
  number = {1},
  pages = {1--3},
  publisher = {American Meteorological Society},
  issn = {1520-0493, 0027-0644},
  doi = {10.1175/1520-0493(1950)078<0001:VOFEIT>2.0.CO;2},
  urldate = {2022-01-21},
  abstract = {Abstract No Abstract Available.},
  chapter = {Monthly Weather Review},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/ZCBG3Z38/Brier - 1950 - VERIFICATION OF FORECASTS EXPRESSED IN TERMS OF PR.pdf;/Users/nikos/github-synced/zotero-nikos/storage/I83583N3/1520-0493_1950_078_0001_vofeit_2_0_co_2.html}
}

@article{briggsAssessingSkillYes2005,
  title = {Assessing the Skill of Yes/No Predictions},
  author = {Briggs, William and Ruppert, David},
  year = {2005},
  month = sep,
  journal = {Biometrics},
  volume = {61},
  number = {3},
  pages = {799--807},
  issn = {0006-341X},
  doi = {10.1111/j.1541-0420.2005.00347.x},
  abstract = {Should healthy, middle-aged women receive precautionary mammograms? Should trauma surgeons use the popular TRISS score to predict the likelihood of patient survival? These are examples of questions confronting us when we decide whether to use a yes/no prediction. In order to trust a prediction we must show that it is more valuable than would be our best guess of the future in the absence of the prediction. Calculating value means identifying our loss should the prediction err and examining the past performance of the prediction with respect to that loss. A statistical test to do this is developed. Predictions that pass this test are said to have skill. Only skillful predictions should be used. Graphical and numerical methods to identify skill will be demonstrated. The usefulness of mammograms is explored.},
  langid = {english},
  pmid = {16135031},
  keywords = {Adult,Breast Neoplasms,False Negative Reactions,False Positive Reactions,Female,Forecasting,Humans,Mammography,Models Statistical,Trauma Severity Indices}
}

@article{brockerDecompositionsProperScores,
  title = {Decompositions of {{Proper Scores}}},
  author = {Brocker, Jochen},
  pages = {9},
  abstract = {Scoring rules are an important tool for evaluating the performance of probabilistic forecasts. A popular example is the Brier score, which allows for a decomposition into terms related to the sharpness (or information content) and to the reliability of the forecast. This feature renders the Brier score a very intuitive measure of forecast quality. In this paper, it is demonstrated that all strictly proper scoring rules allow for a similar decomposition into reliability and sharpness related terms. This finding underpins the importance of proper scores and yields further credence to the practice of measuring forecast quality by proper scores. Furthermore, the effect of averaging multiple probabilistic forecasts on the score is discussed. It is well known that the Brier score of a mixture of several forecasts is never worse that the average score of the individual forecasts. This property hinges on the convexity of the Brier score, a property not universal among proper scores. Arguably, this phenomenon portends epistemological questions which require clarification.},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/P9IB98P5/Brocker - Decompositions of Proper Scores.pdf}
}

@article{brockerReliabilitySufficiencyDecomposition2009a,
  title = {{Reliability, sufficiency, and the decomposition of proper scores}},
  author = {Br{\"o}cker, Jochen},
  year = {2009},
  journal = {Quarterly Journal of the Royal Meteorological Society},
  volume = {135},
  number = {643},
  pages = {1512--1519},
  issn = {1477-870X},
  doi = {10.1002/qj.456},
  urldate = {2020-09-05},
  abstract = {Scoring rules are an important tool for evaluating the performance of probabilistic forecasting schemes. A scoring rule is called strictly proper if its expectation is optimal if and only if the forecast probability represents the true distribution of the target. In the binary case, strictly proper scoring rules allow for a decomposition into terms related to the resolution and the reliability of a forecast. This fact is particularly well known for the Brier Score. In this article, this result is extended to forecasts for finite-valued targets. Both resolution and reliability are shown to have a positive effect on the score. It is demonstrated that resolution and reliability are directly related to forecast attributes that are desirable on grounds independent of the notion of scores. This finding can be considered an epistemological justification of measuring forecast quality by proper scoring rules. A link is provided to the original work of DeGroot and Fienberg, extending their concepts of sufficiency and refinement. The relation to the conjectured sharpness principle of Gneiting, et al., is elucidated. Copyright {\copyright} 2009 Royal Meteorological Society},
  copyright = {Copyright {\copyright} 2009 Royal Meteorological Society},
  langid = {french},
  keywords = {probabilistic forecasts,reliability,resolution,scoring rules},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/MGX929X8/Br√∂cker - 2009 - Reliability, sufficiency, and the decomposition of.pdf;/Users/nikos/github-synced/zotero-nikos/storage/52SCPZK3/qj.html}
}

@article{brooksNonmechanisticForecastsSeasonal2018,
  title = {Nonmechanistic Forecasts of Seasonal Influenza with Iterative One-Week-Ahead Distributions},
  author = {Brooks, Logan C. and Farrow, David C. and Hyun, Sangwon and Tibshirani, Ryan J. and Rosenfeld, Roni},
  editor = {Viboud, Cecile},
  year = {2018},
  month = jun,
  journal = {PLOS Computational Biology},
  volume = {14},
  number = {6},
  pages = {e1006134},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1006134},
  urldate = {2020-03-29},
  abstract = {Accurate and reliable forecasts of seasonal epidemics of infectious disease can assist in the design of countermeasures and increase public awareness and preparedness. This article describes two main contributions we made recently toward this goal: a novel approach to probabilistic modeling of surveillance time series based on ``delta densities'', and an optimization scheme for combining output from multiple forecasting methods into an adaptively weighted ensemble. Delta densities describe the probability distribution of the change between one observation and the next, conditioned on available data; chaining together nonparametric estimates of these distributions yields a model for an entire trajectory. Corresponding distributional forecasts cover more observed events than alternatives that treat the whole season as a unit, and improve upon multiple evaluation metrics when extracting key targets of interest to public health officials. Adaptively weighted ensembles integrate the results of multiple forecasting methods, such as delta density, using weights that can change from situation to situation. We treat selection of optimal weightings across forecasting methods as a separate estimation task, and describe an estimation procedure based on optimizing cross-validation performance. We consider some details of the data generation process, including data revisions and holiday effects, both in the construction of these forecasting methods and when performing retrospective evaluation. The delta density method and an adaptively weighted ensemble of other forecasting methods each improve significantly on the next best ensemble component when applied separately, and achieve even better cross-validated performance when used in conjunction. We submitted real-time forecasts based on these contributions as part of CDC's 2015/2016 FluSight Collaborative Comparison. Among the fourteen submissions that season, this system was ranked by CDC as the most accurate.},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/KWJ8KSUS/Brooks et al. - 2018 - Nonmechanistic forecasts of seasonal influenza wit.pdf}
}

@article{castroTurningPointEnd2020,
  title = {The Turning Point and End of an Expanding Epidemic Cannot Be Precisely Forecast},
  author = {Castro, Mario and Ares, Sa{\'u}l and Cuesta, Jos{\'e} A. and Manrubia, Susanna},
  year = {2020},
  month = oct,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {117},
  number = {42},
  pages = {26190--26196},
  publisher = {National Academy of Sciences},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2007868117},
  urldate = {2021-07-27},
  abstract = {Epidemic spread is characterized by exponentially growing dynamics, which are intrinsically unpredictable. The time at which the growth in the number of infected individuals halts and starts decreasing cannot be calculated with certainty before the turning point is actually attained; neither can the end of the epidemic after the turning point. A susceptible--infected--removed (SIR) model with confinement (SCIR) illustrates how lockdown measures inhibit infection spread only above a threshold that we calculate. The existence of that threshold has major effects in predictability: A Bayesian fit to the COVID-19 pandemic in Spain shows that a slowdown in the number of newly infected individuals during the expansion phase allows one to infer neither the precise position of the maximum nor whether the measures taken will bring the propagation to the inhibition regime. There is a short horizon for reliable prediction, followed by a dispersion of the possible trajectories that grows extremely fast. The impossibility to predict in the midterm is not due to wrong or incomplete data, since it persists in error-free, synthetically produced datasets and does not necessarily improve by using larger datasets. Our study warns against precise forecasts of the evolution of epidemics based on mean-field, effective, or phenomenological models and supports that only probabilities of different outcomes can be confidently given.},
  chapter = {Biological Sciences},
  copyright = {Copyright {\copyright} 2020 the Author(s). Published by PNAS.. https://creativecommons.org/licenses/by-nc-nd/4.0/This open access article is distributed under Creative Commons Attribution-NonCommercial-NoDerivatives License 4.0 (CC BY-NC-ND).},
  langid = {english},
  pmid = {33004629},
  keywords = {Bayesian,epidemics,forecast,predictability},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/67SBVBNR/Castro et al. - 2020 - The turning point and end of an expanding epidemic.pdf;/Users/nikos/github-synced/zotero-nikos/storage/6VJES55N/26190.html}
}

@article{cavanaughAkaikeInformationCriterion2019,
  title = {The {{Akaike}} Information Criterion: {{Background}}, Derivation, Properties, Application, Interpretation, and Refinements},
  shorttitle = {The {{Akaike}} Information Criterion},
  author = {Cavanaugh, Joseph E. and Neath, Andrew A.},
  year = {2019},
  journal = {WIREs Computational Statistics},
  volume = {11},
  number = {3},
  pages = {e1460},
  issn = {1939-0068},
  doi = {10.1002/wics.1460},
  urldate = {2021-11-18},
  abstract = {The Akaike information criterion (AIC) is one of the most ubiquitous tools in statistical modeling. The first model selection criterion to gain widespread acceptance, AIC was introduced in 1973 by Hirotugu Akaike as an extension to the maximum likelihood principle. Maximum likelihood is conventionally applied to estimate the parameters of a model once the structure and dimension of the model have been formulated. Akaike's seminal idea was to combine into a single procedure the process of estimation with structural and dimensional determination. This article reviews the conceptual and theoretical foundations for AIC, discusses its properties and its predictive interpretation, and provides a synopsis of important practical issues pertinent to its application. Comparisons and delineations are drawn between AIC and its primary competitor, the Bayesian information criterion (BIC). In addition, the article covers refinements of AIC for settings where the asymptotic conditions and model specification assumptions that underlie the justification of AIC may be violated. This article is categorized under: Software for Computational Statistics {$>$} Artificial Intelligence and Expert Systems Statistical Models {$>$} Model Selection Statistical and Graphical Methods of Data Analysis {$>$} Modeling Methods and Algorithms Statistical and Graphical Methods of Data Analysis {$>$} Information Theoretic Methods},
  langid = {english},
  keywords = {AIC,Kullback-Leibler information,model selection criterion}
}

@misc{cdcCdcepiFlusightforecastdata2022,
  title = {Cdcepi/{{Flusight-forecast-data}}},
  author = {{CDC}},
  year = {2022},
  month = nov,
  urldate = {2022-11-17},
  howpublished = {CDC Epidemic Prediction Initiative}
}

@article{chatzilenaContemporaryStatisticalInference2019a,
  title = {Contemporary Statistical Inference for Infectious Disease Models Using {{Stan}}},
  author = {Chatzilena, Anastasia and {van Leeuwen}, Edwin and Ratmann, Oliver and Baguelin, Marc and Demiris, Nikolaos},
  year = {2019},
  month = dec,
  journal = {Epidemics},
  volume = {29},
  pages = {100367},
  issn = {17554365},
  doi = {10.1016/j.epidem.2019.100367},
  urldate = {2020-05-17},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/TM6MVY4E/Chatzilena et al. - 2019 - Contemporary statistical inference for infectious .pdf}
}

@misc{choeComparingSequentialForecasters2023,
  title = {Comparing {{Sequential Forecasters}}},
  author = {Choe, Yo Joong and Ramdas, Aaditya},
  year = {2023},
  month = sep,
  number = {arXiv:2110.00115},
  eprint = {2110.00115},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2110.00115},
  urldate = {2023-09-23},
  abstract = {Consider two forecasters, each making a single prediction for a sequence of events over time. We ask a relatively basic question: how might we compare these forecasters, either online or post-hoc, while avoiding unverifiable assumptions on how the forecasts and outcomes were generated? In this paper, we present a rigorous answer to this question by designing novel sequential inference procedures for estimating the time-varying difference in forecast scores. To do this, we employ confidence sequences (CS), which are sequences of confidence intervals that can be continuously monitored and are valid at arbitrary data-dependent stopping times ("anytime-valid"). The widths of our CSs are adaptive to the underlying variance of the score differences. Underlying their construction is a game-theoretic statistical framework, in which we further identify e-processes and p-processes for sequentially testing a weak null hypothesis -- whether one forecaster outperforms another on average (rather than always). Our methods do not make distributional assumptions on the forecasts or outcomes; our main theorems apply to any bounded scores, and we later provide alternative methods for unbounded scores. We empirically validate our approaches by comparing real-world baseball and weather forecasters.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Applications,Statistics - Machine Learning,Statistics - Methodology},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/ZBFRH7DW/Choe and Ramdas - 2023 - Comparing Sequential Forecasters.pdf;/Users/nikos/github-synced/zotero-nikos/storage/54N66BD3/2110.html}
}

@article{claeskensForecastCombinationPuzzle2016,
  title = {The Forecast Combination Puzzle: {{A}} Simple Theoretical Explanation},
  shorttitle = {The Forecast Combination Puzzle},
  author = {Claeskens, Gerda and Magnus, Jan R. and Vasnev, Andrey L. and Wang, Wendun},
  year = {2016},
  month = jul,
  journal = {International Journal of Forecasting},
  volume = {32},
  number = {3},
  pages = {754--762},
  issn = {01692070},
  doi = {10.1016/j.ijforecast.2015.12.005},
  urldate = {2021-07-02},
  abstract = {This paper offers a theoretical explanation for the stylized fact that forecast combinations with estimated optimal weights often perform poorly in applications. The properties of the forecast combination are typically derived under the assumption that the weights are fixed, while in practice they need to be estimated. If the fact that the weights are random rather than fixed is taken into account during the optimality derivation, then the forecast combination will be biased (even when the original forecasts are unbiased), and its variance will be larger than in the fixed-weight case. In particular, there is no guarantee that the `optimal' forecast combination will be better than the equal-weight case, or even improve on the original forecasts. We provide the underlying theory, some special cases, and a numerical illustration.},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/4TUUCQXG/Claeskens et al. - 2016 - The forecast combination puzzle A simple theoreti.pdf}
}

@article{clemenDebiasingExpertOverconfidencea,
  title = {Debiasing {{Expert Overconfidence}}: {{A Bayesian Calibration Model}}},
  author = {Clemen, Robert T and Lichtendahl, Kenneth C},
  pages = {16},
  abstract = {In a decision and risk analysis, experts may provide subjective probability distributions that encode their beliefs about future uncertain events. For continuous variables, experts often provide these judgments in the form of quantiles of the distribution (e.g., 5th, 50th, and 95th percentiles). Psychologists have shown, though, that such subjective distributions tend to be too narrow, representing overconfidence on the part of the expert. We propose an approach for modeling and debiasing expert overconfidence. Based on past performance data (previous assessments and realizations for a number of uncertain variables), and using Bayesian methods to update prior distributions on the model parameters, we show how our model can be used to debias expert probabilities. We develop and demonstrate both a single-expert model and a multiple-expert hierarchical model.},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/HKACWJE9/Clemen and Lichtendahl - Debiasing Expert OverconÔ¨Ådence A Bayesian Calibra.pdf}
}

@article{colon-gonzalezProbabilisticSeasonalDengue2021,
  title = {Probabilistic Seasonal Dengue Forecasting in {{Vietnam}}: {{A}} Modelling Study Using Superensembles},
  shorttitle = {Probabilistic Seasonal Dengue Forecasting in {{Vietnam}}},
  author = {{Col{\'o}n-Gonz{\'a}lez}, Felipe J. and Bastos, Leonardo Soares and Hofmann, Barbara and Hopkin, Alison and Harpham, Quillon and Crocker, Tom and Amato, Rosanna and Ferrario, Iacopo and Moschini, Francesca and James, Samuel and Malde, Sajni and Ainscoe, Eleanor and Nam, Vu Sinh and Tan, Dang Quang and Khoa, Nguyen Duc and Harrison, Mark and Tsarouchi, Gina and Lumbroso, Darren and Brady, Oliver J. and Lowe, Rachel},
  year = {2021},
  month = mar,
  journal = {PLOS Medicine},
  volume = {18},
  number = {3},
  pages = {e1003542},
  publisher = {Public Library of Science},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.1003542},
  urldate = {2021-03-06},
  abstract = {Background With enough advanced notice, dengue outbreaks can be mitigated. As a climate-sensitive disease, environmental conditions and past patterns of dengue can be used to make predictions about future outbreak risk. These predictions improve public health planning and decision-making to ultimately reduce the burden of disease. Past approaches to dengue forecasting have used seasonal climate forecasts, but the predictive ability of a system using different lead times in a year-round prediction system has been seldom explored. Moreover, the transition from theoretical to operational systems integrated with disease control activities is rare. Methods and findings We introduce an operational seasonal dengue forecasting system for Vietnam where Earth observations, seasonal climate forecasts, and lagged dengue cases are used to drive a superensemble of probabilistic dengue models to predict dengue risk up to 6 months ahead. Bayesian spatiotemporal models were fit to 19 years (2002--2020) of dengue data at the province level across Vietnam. A superensemble of these models then makes probabilistic predictions of dengue incidence at various future time points aligned with key Vietnamese decision and planning deadlines. We demonstrate that the superensemble generates more accurate predictions of dengue incidence than the individual models it incorporates across a suite of time horizons and transmission settings. Using historical data, the superensemble made slightly more accurate predictions (continuous rank probability score [CRPS] = 66.8, 95\% CI 60.6--148.0) than a baseline model which forecasts the same incidence rate every month (CRPS = 79.4, 95\% CI 78.5--80.5) at lead times of 1 to 3 months, albeit with larger uncertainty. The outbreak detection capability of the superensemble was considerably larger (69\%) than that of the baseline model (54.5\%). Predictions were most accurate in southern Vietnam, an area that experiences semi-regular seasonal dengue transmission. The system also demonstrated added value across multiple areas compared to previous practice of not using a forecast. We use the system to make a prospective prediction for dengue incidence in Vietnam for the period May to October 2020. Prospective predictions made with the superensemble were slightly more accurate (CRPS = 110, 95\% CI 102--575) than those made with the baseline model (CRPS = 125, 95\% CI 120--168) but had larger uncertainty. Finally, we propose a framework for the evaluation of probabilistic predictions. Despite the demonstrated value of our forecasting system, the approach is limited by the consistency of the dengue case data, as well as the lack of publicly available, continuous, and long-term data sets on mosquito control efforts and serotype-specific case data. Conclusions This study shows that by combining detailed Earth observation data, seasonal climate forecasts, and state-of-the-art models, dengue outbreaks can be predicted across a broad range of settings, with enough lead time to meaningfully inform dengue control. While our system omits some important variables not currently available at a subnational scale, the majority of past outbreaks could be predicted up to 3 months ahead. Over the next 2 years, the system will be prospectively evaluated and, if successful, potentially extended to other areas and other climate-sensitive disease systems.},
  langid = {english},
  keywords = {Decision making,Dengue fever,Forecasting,Humidity,Mosquitoes,Public and occupational health,Seasons,Vietnam},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/9FBQQDLX/Col√≥n-Gonz√°lez et al. - 2021 - Probabilistic seasonal dengue forecasting in Vietn.pdf;/Users/nikos/github-synced/zotero-nikos/storage/BQ47TPND/article.html}
}

@article{coriNewFrameworkSoftware2013a,
  title = {A {{New Framework}} and {{Software}} to {{Estimate Time-Varying Reproduction Numbers During Epidemics}}},
  author = {Cori, Anne and Ferguson, Neil M. and Fraser, Christophe and Cauchemez, Simon},
  year = {2013},
  month = nov,
  journal = {American Journal of Epidemiology},
  volume = {178},
  number = {9},
  pages = {1505--1512},
  issn = {0002-9262},
  doi = {10.1093/aje/kwt133},
  urldate = {2019-11-01},
  abstract = {Abstract.  The quantification of transmissibility during epidemics is essential to designing and adjusting public health responses. Transmissibility can be meas},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/3HRUA5K9/kwt133supp.doc;/Users/nikos/github-synced/zotero-nikos/storage/V5NSW7DF/Cori et al. - 2013 - A New Framework and Software to Estimate Time-Vary.pdf;/Users/nikos/github-synced/zotero-nikos/storage/FMCT4D4Z/governor.html}
}

@article{covidregionaldata,
  title = {Covidregionaldata: {{Subnational}} Data for the Covid-19 Outbreak},
  author = {Abbott, Sam and Sherratt, Katharine and Bevan, Jonnie and Gibbs, Hamish and Hellewell, Joel and Munday, James and Barks, Patrick and Campbell, Paul and Finger, Flavio and Funk, Sebastian},
  year = {2020},
  journal = {-},
  volume = {-},
  number = {-},
  pages = {-},
  doi = {10.5281/zenodo.3957539}
}

@misc{cramerCOVID19ForecastHub2020,
  title = {{{COVID-19 Forecast Hub}}: 4 {{December}} 2020 Snapshot},
  shorttitle = {{{COVID-19 Forecast Hub}}},
  author = {Cramer, Estee and Nicholas G Reich and Serena Yijin Wang and Jarad Niemi and Abdul Hannan and Katie House and Youyang Gu and Shanghong Xie and Steve Horstman and {aniruddhadiga} and Robert Walraven and {starkari} and Michael Lingzhi Li and Graham Gibson and Lauren Castro and Dean Karlen and Nutcha Wattanachit and {jinghuichen} and {zyt9lsb} and {aagarwal1996} and Spencer Woody and Evan Ray and Frost Tianjian Xu and Hannah Biegel and GuidoEspana and Xinyue X and Johannes Bracher and Elizabeth Lee and {har96} and {leyouz}},
  year = {2020},
  month = dec,
  publisher = {Zenodo},
  doi = {10.5281/zenodo.3963371},
  urldate = {2021-05-29},
  abstract = {This update to the COVID-19 Forecast Hub repository is a snapshot as of 4 December 2020 of the data hosted by and visualized at~https://covid19forecasthub.org/.},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/AVWA2UPE/4305938.html}
}

@article{cramerEvaluationIndividualEnsemble2021,
  title = {Evaluation of Individual and Ensemble Probabilistic Forecasts of {{COVID-19}} Mortality in the {{US}}},
  author = {Cramer, Estee and Ray, Evan L. and Lopez, Velma K. and Bracher, Johannes and Brennen, Andrea and Rivadeneira, Alvaro J. Castro and Gerding, Aaron and Gneiting, Tilmann and House, Katie H. and Huang, Yuxin and Jayawardena, Dasuni and Kanji, Abdul H. and Khandelwal, Ayush and Le, Khoa and M{\"u}hlemann, Anja and Niemi, Jarad and Shah, Apurv and Stark, Ariane and Wang, Yijin and Wattanachit, Nutcha and Zorn, Martha W. and Gu, Youyang and Jain, Sansiddh and Bannur, Nayana and Deva, Ayush and Kulkarni, Mihir and Merugu, Srujana and Raval, Alpan and Shingi, Siddhant and Tiwari, Avtansh and White, Jerome and Woody, Spencer and Dahan, Maytal and Fox, Spencer and Gaither, Kelly and Lachmann, Michael and Meyers, Lauren Ancel and Scott, James G. and Tec, Mauricio and Srivastava, Ajitesh and George, Glover E. and Cegan, Jeffrey C. and Dettwiller, Ian D. and England, William P. and Farthing, Matthew W. and Hunter, Robert H. and Lafferty, Brandon and Linkov, Igor and Mayo, Michael L. and Parno, Matthew D. and Rowland, Michael A. and Trump, Benjamin D. and Corsetti, Sabrina M. and Baer, Thomas M. and Eisenberg, Marisa C. and Falb, Karl and Huang, Yitao and Martin, Emily T. and McCauley, Ella and Myers, Robert L. and Schwarz, Tom and Sheldon, Daniel and Gibson, Graham Casey and Yu, Rose and Gao, Liyao and Ma, Yian and Wu, Dongxia and Yan, Xifeng and Jin, Xiaoyong and Wang, Yu-Xiang and Chen, YangQuan and Guo, Lihong and Zhao, Yanting and Gu, Quanquan and Chen, Jinghui and Wang, Lingxiao and Xu, Pan and Zhang, Weitong and Zou, Difan and Biegel, Hannah and Lega, Joceline and Snyder, Timothy L. and Wilson, Davison D. and McConnell, Steve and Walraven, Robert and Shi, Yunfeng and Ban, Xuegang and Hong, Qi-Jun and Kong, Stanley and Turtle, James A. and {Ben-Nun}, Michal and Riley, Pete and Riley, Steven and Koyluoglu, Ugur and DesRoches, David and Hamory, Bruce and Kyriakides, Christina and Leis, Helen and Milliken, John and Moloney, Michael and Morgan, James and Ozcan, Gokce and Schrader, Chris and Shakhnovich, Elizabeth and Siegel, Daniel and Spatz, Ryan and Stiefeling, Chris and Wilkinson, Barrie and Wong, Alexander and Gao, Zhifeng and Bian, Jiang and Cao, Wei and Ferres, Juan Lavista and Li, Chaozhuo and Liu, Tie-Yan and Xie, Xing and Zhang, Shun and Zheng, Shun and Vespignani, Alessandro and Chinazzi, Matteo and Davis, Jessica T. and Mu, Kunpeng and y Piontti, Ana Pastore and Xiong, Xinyue and Zheng, Andrew and Baek, Jackie and Farias, Vivek and Georgescu, Andreea and Levi, Retsef and Sinha, Deeksha and Wilde, Joshua and Penna, Nicolas D. and Celi, Leo A. and Sundar, Saketh and Cavany, Sean and Espa{\~n}a, Guido and Moore, Sean and Oidtman, Rachel and Perkins, Alex and Osthus, Dave and Castro, Lauren and Fairchild, Geoffrey and Michaud, Isaac and Karlen, Dean and Lee, Elizabeth C. and Dent, Juan and Grantz, Kyra H. and Kaminsky, Joshua and Kaminsky, Kathryn and Keegan, Lindsay T. and Lauer, Stephen A. and Lemaitre, Joseph C. and Lessler, Justin and Meredith, Hannah R. and {Perez-Saez}, Javier and Shah, Sam and Smith, Claire P. and Truelove, Shaun A. and Wills, Josh and Kinsey, Matt and Obrecht, R. F. and Tallaksen, Katharine and Burant, John C. and Wang, Lily and Gao, Lei and Gu, Zhiling and Kim, Myungjin and Li, Xinyi and Wang, Guannan and Wang, Yueying and Yu, Shan and Reiner, Robert C. and Barber, Ryan and Gaikedu, Emmanuela and Hay, Simon and Lim, Steve and Murray, Chris and Pigott, David and Prakash, B. Aditya and Adhikari, Bijaya and Cui, Jiaming and Rodr{\'i}guez, Alexander and Tabassum, Anika and Xie, Jiajia and Keskinocak, Pinar and Asplund, John and Baxter, Arden and Oruc, Buse Eylul and Serban, Nicoleta and Arik, Sercan O. and Dusenberry, Mike and Epshteyn, Arkady and Kanal, Elli and Le, Long T. and Li, Chun-Liang and Pfister, Tomas and Sava, Dario and Sinha, Rajarishi and Tsai, Thomas and Yoder, Nate and Yoon, Jinsung and Zhang, Leyou and Abbott, Sam and Bosse, Nikos I. and Funk, Sebastian and Hellewel, Joel and Meakin, Sophie R. and Munday, James D. and Sherratt, Katherine and Zhou, Mingyuan and Kalantari, Rahi and Yamana, Teresa K. and Pei, Sen and Shaman, Jeffrey and Ayer, Turgay and Adee, Madeline and Chhatwal, Jagpreet and Dalgic, Ozden O. and Ladd, Mary A. and Linas, Benjamin P. and Mueller, Peter and Xiao, Jade and Li, Michael L. and Bertsimas, Dimitris and Lami, Omar Skali and Soni, Saksham and Bouardi, Hamza Tazi and Wang, Yuanjia and Wang, Qinxia and Xie, Shanghong and Zeng, Donglin and Green, Alden and Bien, Jacob and Hu, Addison J. and Jahja, Maria and Narasimhan, Balasubramanian and Rajanala, Samyak and Rumack, Aaron and Simon, Noah and Tibshirani, Ryan and Tibshirani, Rob and Ventura, Valerie and Wasserman, Larry and O'Dea, Eamon B. and Drake, John M. and Pagano, Robert and Walker, Jo W. and Slayton, Rachel B. and Johansson, Michael and Biggerstaff, Matthew and Reich, Nicholas G.},
  year = {2021},
  month = feb,
  journal = {medRxiv},
  pages = {2021.02.03.21250974},
  publisher = {Cold Spring Harbor Laboratory Press},
  doi = {10.1101/2021.02.03.21250974},
  urldate = {2021-04-06},
  abstract = {{$<$}h3{$>$}Abstract{$<$}/h3{$>$} {$<$}p{$>$}Short-term probabilistic forecasts of the trajectory of the COVID-19 pandemic in the United States have served as a visible and important communication channel between the scientific modeling community and both the general public and decision-makers. Forecasting models provide specific, quantitative, and evaluable predictions that inform short-term decisions such as healthcare staffing needs, school closures, and allocation of medical supplies. In 2020, the COVID-19 Forecast Hub (https://covid19forecasthub.org/) collected, disseminated, and synthesized hundreds of thousands of specific predictions from more than 50 different academic, industry, and independent research groups. This manuscript systematically evaluates 23 models that regularly submitted forecasts of reported weekly incident COVID-19 mortality counts in the US at the state and national level. One of these models was a multi-model ensemble that combined all available forecasts each week. The performance of individual models showed high variability across time, geospatial units, and forecast horizons. Half of the models evaluated showed better accuracy than a na{\"i}ve baseline model. In combining the forecasts from all teams, the ensemble showed the best overall probabilistic accuracy of any model. Forecast accuracy degraded as models made predictions farther into the future, with probabilistic accuracy at a 20-week horizon more than 5 times worse than when predicting at a 1-week horizon. This project underscores the role that collaboration and active coordination between governmental public health agencies, academic modeling teams, and industry partners can play in developing modern modeling capabilities to support local, state, and federal response to outbreaks.{$<$}/p{$>$}},
  copyright = {{\copyright} 2021, Posted by Cold Spring Harbor Laboratory. This article is a US Government work. It is not subject to copyright under 17 USC 105 and is also made available for use under a CC0 license},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/W82X9ZN5/Cramer et al. - 2021 - Evaluation of individual and ensemble probabilisti.pdf;/Users/nikos/github-synced/zotero-nikos/storage/7MC6LGTC/2021.02.03.21250974v1.html}
}

@article{cramerEvaluationIndividualEnsemble2022,
  title = {Evaluation of Individual and Ensemble Probabilistic Forecasts of {{COVID-19}} Mortality in the {{United States}}},
  author = {Cramer, Estee Y. and Ray, Evan L. and Lopez, Velma K. and Bracher, Johannes and Brennen, Andrea and Castro Rivadeneira, Alvaro J. and Gerding, Aaron and Gneiting, Tilmann and House, Katie H. and Huang, Yuxin and Jayawardena, Dasuni and Kanji, Abdul H. and Khandelwal, Ayush and Le, Khoa and M{\"u}hlemann, Anja and Niemi, Jarad and Shah, Apurv and Stark, Ariane and Wang, Yijin and Wattanachit, Nutcha and Zorn, Martha W. and Gu, Youyang and Jain, Sansiddh and Bannur, Nayana and Deva, Ayush and Kulkarni, Mihir and Merugu, Srujana and Raval, Alpan and Shingi, Siddhant and Tiwari, Avtansh and White, Jerome and Abernethy, Neil F. and Woody, Spencer and Dahan, Maytal and Fox, Spencer and Gaither, Kelly and Lachmann, Michael and Meyers, Lauren Ancel and Scott, James G. and Tec, Mauricio and Srivastava, Ajitesh and George, Glover E. and Cegan, Jeffrey C. and Dettwiller, Ian D. and England, William P. and Farthing, Matthew W. and Hunter, Robert H. and Lafferty, Brandon and Linkov, Igor and Mayo, Michael L. and Parno, Matthew D. and Rowland, Michael A. and Trump, Benjamin D. and {Zhang-James}, Yanli and Chen, Samuel and Faraone, Stephen V. and Hess, Jonathan and Morley, Christopher P. and Salekin, Asif and Wang, Dongliang and Corsetti, Sabrina M. and Baer, Thomas M. and Eisenberg, Marisa C. and Falb, Karl and Huang, Yitao and Martin, Emily T. and McCauley, Ella and Myers, Robert L. and Schwarz, Tom and Sheldon, Daniel and Gibson, Graham Casey and Yu, Rose and Gao, Liyao and Ma, Yian and Wu, Dongxia and Yan, Xifeng and Jin, Xiaoyong and Wang, Yu-Xiang and Chen, YangQuan and Guo, Lihong and Zhao, Yanting and Gu, Quanquan and Chen, Jinghui and Wang, Lingxiao and Xu, Pan and Zhang, Weitong and Zou, Difan and Biegel, Hannah and Lega, Joceline and McConnell, Steve and Nagraj, V. P. and Guertin, Stephanie L. and {Hulme-Lowe}, Christopher and Turner, Stephen D. and Shi, Yunfeng and Ban, Xuegang and Walraven, Robert and Hong, Qi-Jun and Kong, Stanley and {van de Walle}, Axel and Turtle, James A. and {Ben-Nun}, Michal and Riley, Steven and Riley, Pete and Koyluoglu, Ugur and DesRoches, David and Forli, Pedro and Hamory, Bruce and Kyriakides, Christina and Leis, Helen and Milliken, John and Moloney, Michael and Morgan, James and Nirgudkar, Ninad and Ozcan, Gokce and Piwonka, Noah and Ravi, Matt and Schrader, Chris and Shakhnovich, Elizabeth and Siegel, Daniel and Spatz, Ryan and Stiefeling, Chris and Wilkinson, Barrie and Wong, Alexander and Cavany, Sean and Espa{\~n}a, Guido and Moore, Sean and Oidtman, Rachel and Perkins, Alex and Kraus, David and Kraus, Andrea and Gao, Zhifeng and Bian, Jiang and Cao, Wei and Lavista Ferres, Juan and Li, Chaozhuo and Liu, Tie-Yan and Xie, Xing and Zhang, Shun and Zheng, Shun and Vespignani, Alessandro and Chinazzi, Matteo and Davis, Jessica T. and Mu, Kunpeng and {Pastore y Piontti}, Ana and Xiong, Xinyue and Zheng, Andrew and Baek, Jackie and Farias, Vivek and Georgescu, Andreea and Levi, Retsef and Sinha, Deeksha and Wilde, Joshua and Perakis, Georgia and Bennouna, Mohammed Amine and {Nze-Ndong}, David and Singhvi, Divya and Spantidakis, Ioannis and Thayaparan, Leann and Tsiourvas, Asterios and Sarker, Arnab and Jadbabaie, Ali and Shah, Devavrat and Della Penna, Nicolas and Celi, Leo A. and Sundar, Saketh and Wolfinger, Russ and Osthus, Dave and Castro, Lauren and Fairchild, Geoffrey and Michaud, Isaac and Karlen, Dean and Kinsey, Matt and Mullany, Luke C. and {Rainwater-Lovett}, Kaitlin and Shin, Lauren and Tallaksen, Katharine and Wilson, Shelby and Lee, Elizabeth C. and Dent, Juan and Grantz, Kyra H. and Hill, Alison L. and Kaminsky, Joshua and Kaminsky, Kathryn and Keegan, Lindsay T. and Lauer, Stephen A. and Lemaitre, Joseph C. and Lessler, Justin and Meredith, Hannah R. and {Perez-Saez}, Javier and Shah, Sam and Smith, Claire P. and Truelove, Shaun A. and Wills, Josh and Marshall, Maximilian and Gardner, Lauren and Nixon, Kristen and Burant, John C. and Wang, Lily and Gao, Lei and Gu, Zhiling and Kim, Myungjin and Li, Xinyi and Wang, Guannan and Wang, Yueying and Yu, Shan and Reiner, Robert C. and Barber, Ryan and Gakidou, Emmanuela and Hay, Simon I. and Lim, Steve and Murray, Chris and Pigott, David and Gurung, Heidi L. and Baccam, Prasith and Stage, Steven A. and Suchoski, Bradley T. and Prakash, B. Aditya and Adhikari, Bijaya and Cui, Jiaming and Rodr{\'i}guez, Alexander and Tabassum, Anika and Xie, Jiajia and Keskinocak, Pinar and Asplund, John and Baxter, Arden and Oruc, Buse Eylul and Serban, Nicoleta and Arik, Sercan O. and Dusenberry, Mike and Epshteyn, Arkady and Kanal, Elli and Le, Long T. and Li, Chun-Liang and Pfister, Tomas and Sava, Dario and Sinha, Rajarishi and Tsai, Thomas and Yoder, Nate and Yoon, Jinsung and Zhang, Leyou and Abbott, Sam and Bosse, Nikos I. and Funk, Sebastian and Hellewell, Joel and Meakin, Sophie R. and Sherratt, Katharine and Zhou, Mingyuan and Kalantari, Rahi and Yamana, Teresa K. and Pei, Sen and Shaman, Jeffrey and Li, Michael L. and Bertsimas, Dimitris and Skali Lami, Omar and Soni, Saksham and Tazi Bouardi, Hamza and Ayer, Turgay and Adee, Madeline and Chhatwal, Jagpreet and Dalgic, Ozden O. and Ladd, Mary A. and Linas, Benjamin P. and Mueller, Peter and Xiao, Jade and Wang, Yuanjia and Wang, Qinxia and Xie, Shanghong and Zeng, Donglin and Green, Alden and Bien, Jacob and Brooks, Logan and Hu, Addison J. and Jahja, Maria and McDonald, Daniel and Narasimhan, Balasubramanian and Politsch, Collin and Rajanala, Samyak and Rumack, Aaron and Simon, Noah and Tibshirani, Ryan J. and Tibshirani, Rob and Ventura, Valerie and Wasserman, Larry and O'Dea, Eamon B. and Drake, John M. and Pagano, Robert and Tran, Quoc T. and Ho, Lam Si Tung and Huynh, Huong and Walker, Jo W. and Slayton, Rachel B. and Johansson, Michael A. and Biggerstaff, Matthew and Reich, Nicholas G.},
  year = {2022},
  month = apr,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {119},
  number = {15},
  pages = {e2113561119},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.2113561119},
  urldate = {2024-02-06},
  abstract = {Short-term probabilistic forecasts of the trajectory of the COVID-19 pandemic in the United States have served as a visible and important communication channel between the scientific modeling community and both the general public and decision-makers. Forecasting models provide specific, quantitative, and evaluable predictions that inform short-term decisions such as healthcare staffing needs, school closures, and allocation of medical supplies. Starting in April 2020, the US COVID-19 Forecast Hub (https://covid19forecasthub.org/) collected, disseminated, and synthesized tens of millions of specific predictions from more than 90 different academic, industry, and independent research groups. A multimodel ensemble forecast that combined predictions from dozens of groups every week provided the most consistently accurate probabilistic forecasts of incident deaths due to COVID-19 at the state and national level from April 2020 through October 2021. The performance of 27 individual models that submitted complete forecasts of COVID-19 deaths consistently throughout this year showed high variability in forecast skill across time, geospatial units, and forecast horizons. Two-thirds of the models evaluated showed better accuracy than a na{\"i}ve baseline model. Forecast accuracy degraded as models made predictions further into the future, with probabilistic error at a 20-wk horizon three to five times larger than when predicting at a 1-wk horizon. This project underscores the role that collaboration and active coordination between governmental public-health agencies, academic modeling teams, and industry partners can play in developing modern modeling capabilities to support local, state, and federal response to outbreaks.},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/KADV8FFW/Cramer et al. - 2022 - Evaluation of individual and ensemble probabilisti.pdf}
}

@misc{csetforetellCSETForetell2021,
  title = {{{CSET Foretell}}},
  author = {{CSET Foretell}},
  year = {2021},
  urldate = {2021-10-13},
  howpublished = {https://www.cset-foretell.com/},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/N3FR7YCN/www.cset-foretell.com.html}
}

@article{czadoPredictiveModelAssessment2009,
  title = {Predictive {{Model Assessment}} for {{Count Data}}},
  author = {Czado, Claudia and Gneiting, Tilmann and Held, Leonhard},
  year = {2009},
  journal = {Biometrics},
  volume = {65},
  number = {4},
  pages = {1254--1261},
  issn = {1541-0420},
  doi = {10.1111/j.1541-0420.2009.01191.x},
  urldate = {2020-08-12},
  abstract = {We discuss tools for the evaluation of probabilistic forecasts and the critique of statistical models for count data. Our proposals include a nonrandomized version of the probability integral transform, marginal calibration diagrams, and proper scoring rules, such as the predictive deviance. In case studies, we critique count regression models for patent data, and assess the predictive performance of Bayesian age-period-cohort models for larynx cancer counts in Germany. The toolbox applies in Bayesian or classical and parametric or nonparametric settings and to any type of ordered discrete outcomes.},
  copyright = {{\copyright} 2009, The International Biometric Society},
  langid = {english},
  keywords = {Calibration,Forecast verification,Model diagnostics,Predictive deviance,Probability integral transform,Proper scoring rule},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/SKRXXEYJ/Czado et al. - 2009 - Predictive Model Assessment for Count Data.pdf;/Users/nikos/github-synced/zotero-nikos/storage/Y3Z8AY8F/j.1541-0420.2009.01191.html}
}

@article{dalkeyExperimentalApplicationDELPHI1963,
  title = {An {{Experimental Application}} of the {{DELPHI Method}} to the {{Use}} of {{Experts}}},
  author = {Dalkey, Norman and Helmer, Olaf},
  year = {1963},
  month = apr,
  journal = {Management Science},
  volume = {9},
  number = {3},
  pages = {458--467},
  publisher = {INFORMS},
  issn = {0025-1909},
  doi = {10.1287/mnsc.9.3.458},
  urldate = {2023-02-04},
  abstract = {This paper gives an account of an experiment in the use of the so-called DELPHI method, which was devised in order to obtain the most reliable opinion consensus of a group of experts by subjecting them to a series of questionnaires in depth interspersed with controlled opinion feedback.}
}

@article{daviesHumanJudgementForecasting2022,
  title = {Human Judgement Forecasting Tournaments: {{A}} Feasibility Study Based on the {{COVID-19}} Pandemic with Public Health Practitioners in {{England}}},
  shorttitle = {Human Judgement Forecasting Tournaments},
  author = {Davies, Nathan and Ferris, Simon},
  year = {2022},
  month = jun,
  journal = {Public Health in Practice},
  volume = {3},
  pages = {100260},
  issn = {2666-5352},
  doi = {10.1016/j.puhip.2022.100260},
  urldate = {2023-02-01},
  abstract = {Objectives To explore the viability of running human judgement forecasting tournaments with public health practitioners, and to gather initial data on forecasting accuracy and participant perceptions of forecasting. Study design Quality improvement study comprising two COVID-19 forecasting tournaments using Brier Skill Score scoring and a follow-up participant questionnaire. Methods Over two forecasting tournaments, public health registrars in the East Midlands, UK, assigned probabilities to future possible binary events relating to COVID-19. Participants also completed a questionnaire on their experiences of forecasting. Results There were 17 participants in the first tournament and nine in the second tournament, with no new participants. In both tournaments, the majority of participants scored a Brier Skill Score above the benchmark of 0. The median Brier Skill Score improved slightly between the two tournaments. Participants reported luck and changing political climates as impacting their performance. Participants reported forecasting in their day job but had received no formal training to do so. Conclusions Forecasting is an important public health skill, and human judgement forecasting tournaments can be run amongst public health practitioners with little time and resource requirements. Further research would help identify whether training, teamwork or other interventions can improve public health forecasting accuracy.},
  langid = {english},
  keywords = {COVID-19,Forecasting,Public health policy,Public health training,Tournaments},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/D99USKGL/Davies and Ferris - 2022 - Human judgement forecasting tournaments A feasibi.pdf;/Users/nikos/github-synced/zotero-nikos/storage/LBVYQNU6/S2666535222000362.html}
}

@article{daviesHumanJudgementForecasting2022a,
  title = {Human Judgement Forecasting Tournaments: {{A}} Feasibility Study Based on the {{COVID-19}} Pandemic with Public Health Practitioners in {{England}}},
  shorttitle = {Human Judgement Forecasting Tournaments},
  author = {Davies, Nathan and Ferris, Simon},
  year = {2022},
  month = jun,
  journal = {Public Health in Practice},
  volume = {3},
  pages = {100260},
  issn = {2666-5352},
  doi = {10.1016/j.puhip.2022.100260},
  urldate = {2023-03-01},
  abstract = {Objectives To explore the viability of running human judgement forecasting tournaments with public health practitioners, and to gather initial data on forecasting accuracy and participant perceptions of forecasting. Study design Quality improvement study comprising two COVID-19 forecasting tournaments using Brier Skill Score scoring and a follow-up participant questionnaire. Methods Over two forecasting tournaments, public health registrars in the East Midlands, UK, assigned probabilities to future possible binary events relating to COVID-19. Participants also completed a questionnaire on their experiences of forecasting. Results There were 17 participants in the first tournament and nine in the second tournament, with no new participants. In both tournaments, the majority of participants scored a Brier Skill Score above the benchmark of 0. The median Brier Skill Score improved slightly between the two tournaments. Participants reported luck and changing political climates as impacting their performance. Participants reported forecasting in their day job but had received no formal training to do so. Conclusions Forecasting is an important public health skill, and human judgement forecasting tournaments can be run amongst public health practitioners with little time and resource requirements. Further research would help identify whether training, teamwork or other interventions can improve public health forecasting accuracy.},
  langid = {english},
  keywords = {COVID-19,Forecasting,Public health policy,Public health training,Tournaments},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/LSQ6QR9Q/Davies and Ferris - 2022 - Human judgement forecasting tournaments A feasibi.pdf;/Users/nikos/github-synced/zotero-nikos/storage/673Q9U2A/S2666535222000362.html}
}

@article{dawidCoherentDispersionCriteria1999,
  title = {Coherent Dispersion Criteria for Optimal Experimental Design},
  author = {Dawid, A. Philip and Sebastiani, Paola},
  year = {1999},
  month = mar,
  journal = {The Annals of Statistics},
  volume = {27},
  number = {1},
  pages = {65--81},
  publisher = {Institute of Mathematical Statistics},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1018031101},
  urldate = {2022-01-21},
  abstract = {We characterize those coherent design criteria which depend only on the dispersion matrix (assumed proper and nonsingular) of the ``state of nature,'' which may be a parameter-vector or a set of future observables, and describe the associated decision problems. Connections are established with the classical approach to optimal design theory for the normal linear model, based on concave functions of the information matrix. Implications of the theory for more general models are also considered.},
  keywords = {62C10,62K05,Bayesian decision theory,Coherence,concavity,dispersion standard,optimal design,optimality criterion,proper scoring rule,uncertainty function},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/BLEK95JQ/Dawid and Sebastiani - 1999 - Coherent dispersion criteria for optimal experimen.pdf;/Users/nikos/github-synced/zotero-nikos/storage/KIXNL5J2/1018031101.html}
}

@article{dawidPresentPositionPotential1984,
  title = {Present {{Position}} and {{Potential Developments}}: {{Some Personal Views Statistical Theory}} the {{Prequential Approach}}},
  shorttitle = {Present {{Position}} and {{Potential Developments}}},
  author = {Dawid, A. P.},
  year = {1984},
  journal = {Journal of the Royal Statistical Society: Series A (General)},
  volume = {147},
  number = {2},
  pages = {278--290},
  issn = {2397-2327},
  doi = {10.2307/2981683},
  urldate = {2020-08-12},
  abstract = {The prequential approach is founded on the premiss that the purpose of statistical inference is to make sequential probability forecasts for future observations, rather than to express information about parameters. Many traditional parametric concepts, such as consistency and efficiency, prove to have natural counterparts in this formulation, which sheds new light on these and suggests fruitful extensions.},
  copyright = {{\copyright} 1984 Royal Statistical Society},
  langid = {english},
  keywords = {consistency,efficiency,likelihood,prequential principle,probability forecasting},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/PX9RNJBW/Dawid - 1984 - Present Position and Potential Developments Some .pdf;/Users/nikos/github-synced/zotero-nikos/storage/UXRWFPAE/2981683.html}
}

@article{dawidProperLocalScoring2012,
  title = {Proper Local Scoring Rules on Discrete Sample Spaces},
  author = {Dawid, A. Philip and Lauritzen, Steffen and Parry, Matthew},
  year = {2012},
  month = feb,
  journal = {The Annals of Statistics},
  volume = {40},
  number = {1},
  eprint = {1104.2224},
  issn = {0090-5364},
  doi = {10.1214/12-AOS972},
  urldate = {2021-12-17},
  abstract = {A scoring rule is a loss function measuring the quality of a quoted probability distribution \$Q\$ for a random variable \$X\$, in the light of the realized outcome \$x\$ of \$X\$; it is proper if the expected score, under any distribution \$P\$ for \$X\$, is minimized by quoting \$Q=P\$. Using the fact that any differentiable proper scoring rule on a finite sample space \$\{{\textbackslash}mathcal\{X\}\}\$ is the gradient of a concave homogeneous function, we consider when such a rule can be local in the sense of depending only on the probabilities quoted for points in a nominated neighborhood of \$x\$. Under mild conditions, we characterize such a proper local scoring rule in terms of a collection of homogeneous functions on the cliques of an undirected graph on the space \$\{{\textbackslash}mathcal\{X\}\}\$. A useful property of such rules is that the quoted distribution \$Q\$ need only be known up to a scale factor. Examples of the use of such scoring rules include Besag's pseudo-likelihood and Hyv{\textbackslash}"\{a\}rinen's method of ratio matching.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Mathematics - Statistics Theory},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/M7CJGNKN/Dawid et al. - 2012 - Proper local scoring rules on discrete sample spac.pdf}
}

@article{degrootComparisonEvaluationForecasters1983,
  title = {The {{Comparison}} and {{Evaluation}} of {{Forecasters}}},
  author = {DeGroot, Morris H. and Fienberg, Stephen E.},
  year = {1983},
  journal = {Journal of the Royal Statistical Society. Series D (The Statistician)},
  volume = {32},
  number = {1/2},
  eprint = {2987588},
  eprinttype = {jstor},
  pages = {12--22},
  publisher = {[Royal Statistical Society, Wiley]},
  issn = {0039-0526},
  doi = {10.2307/2987588},
  urldate = {2023-09-23},
  abstract = {In this paper we present methods for comparing and evaluating forecasters whose predictions are presented as their subjective probability distributions of various random variables that will be observed in the future, e.g. weather forecasters who each day must specify their own probabilities that it will rain in a particular location. We begin by reviewing the concepts of calibration and refinement, and describing the relationship between this notion of refinement and the notion of sufficiency in the comparison of statistical experiments. We also consider the question of interrelationships among forecasters and discuss methods by which an observer should combine the predictions from two or more different forecasters. Then we turn our attention to the concept of a proper scoring rule for evaluating forecasters, relating it to the concepts of calibration and refinement. Finally, we discuss conditions under which one forecaster can exploit the predictions of another forecaster to obtain a better score.},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/P8UTHAXS/DeGroot and Fienberg - 1983 - The Comparison and Evaluation of Forecasters.pdf}
}

@article{dehningImpactEuro20202023,
  title = {Impact of the {{Euro}} 2020 Championship on the Spread of {{COVID-19}}},
  author = {Dehning, Jonas and Mohr, Sebastian B. and Contreras, Sebastian and D{\"o}nges, Philipp and Iftekhar, Emil N. and Schulz, Oliver and Bechtle, Philip and Priesemann, Viola},
  year = {2023},
  month = jan,
  journal = {Nature Communications},
  volume = {14},
  number = {1},
  pages = {122},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-022-35512-x},
  urldate = {2023-03-21},
  abstract = {Large-scale events like the UEFA Euro 2020 football (soccer) championship offer a unique opportunity to quantify the impact of gatherings on the spread of COVID-19, as the number and dates of matches played by participating countries resembles a randomized study. Using Bayesian modeling and the gender imbalance in COVID-19 data, we attribute 840,000 (95\% CI: [0.39M, 1.26M]) COVID-19 cases across 12 countries to the championship. The impact depends non-linearly on the initial incidence, the reproduction number R, and the number of matches played. The strongest effects are seen in Scotland and England, where as much as 10,000 primary cases per million inhabitants occur from championship-related gatherings. The average match-induced increase in R was 0.46 [0.18, 0.75] on match days, but important matches caused an increase as large as +3. Altogether, our results provide quantitative insights that help judge and mitigate the impact of large-scale events on pandemic spread.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Epidemiology,Infectious diseases,Nonlinear phenomena,SARS-CoV-2,Scientific data},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/BYAMILKC/Dehning et al. - 2023 - Impact of the Euro 2020 championship on the spread.pdf}
}

@article{delvalleSummaryResults201420152018,
  title = {Summary Results of the 2014-2015 {{DARPA Chikungunya}} Challenge},
  author = {Del Valle, Sara Y. and McMahon, Benjamin H. and Asher, Jason and Hatchett, Richard and Lega, Joceline C. and Brown, Heidi E. and Leany, Mark E. and Pantazis, Yannis and Roberts, David J. and Moore, Sean and Peterson, A Townsend and Escobar, Luis E. and Qiao, Huijie and Hengartner, Nicholas W. and Mukundan, Harshini},
  year = {2018},
  month = may,
  journal = {BMC Infectious Diseases},
  volume = {18},
  number = {1},
  pages = {245},
  issn = {1471-2334},
  doi = {10.1186/s12879-018-3124-7},
  urldate = {2021-10-13},
  abstract = {Background: Emerging pathogens such as Zika, chikungunya, Ebola, and dengue viruses are serious threats to national and global health security. Accurate forecasts of emerging epidemics and their severity are critical to minimizing subsequent mortality, morbidity, and economic loss. The recent introduction of chikungunya and Zika virus to the Americas underscores the need for better methods for disease surveillance and forecasting.},
  keywords = {Chikungunya,Forecasting,Mechanistic models,Morphological models},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/T4H98UMU/Del Valle et al. - 2018 - Summary results of the 2014-2015 DARPA Chikungunya.pdf;/Users/nikos/github-synced/zotero-nikos/storage/CJQ92RQD/s12879-018-3124-7.html}
}

@misc{deutschewelleCoronavirusGermanyImpose2020,
  title = {Coronavirus: {{Germany}} to Impose One-Month Partial Lockdown {\textbar} {{DW}} {\textbar} 28.10.2020},
  shorttitle = {Coronavirus},
  author = {{Deutsche Welle}},
  year = {2020},
  urldate = {2021-06-29},
  abstract = {German Chancellor Angela Merkel has announced tough new measures from Monday, November 2, in an attempt to curb the spread of the coronavirus pandemic. But will the German people be compliant?},
  howpublished = {https://www.dw.com/en/coronavirus-germany-to-impose-one-month-partial-lockdown/a-55421241},
  langid = {british},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/PZB9PK7Z/a-55421241.html}
}

@article{dieboldComparingPredictiveAccuracy1995,
  title = {Comparing {{Predictive Accuracy}}},
  author = {Diebold, Francis X and Mariano, Roberto S},
  year = {1995},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/ZGHEQDL4/Diebold and Mariano - 2023 - Comparing Predictive Accuracy.pdf}
}

@article{dieboldComparingPredictiveAccuracy2015,
  title = {Comparing {{Predictive Accuracy}}, {{Twenty Years Later}}: {{A Personal Perspective}} on the {{Use}} and {{Abuse}} of {{Diebold}}--{{Mariano Tests}}},
  shorttitle = {Comparing {{Predictive Accuracy}}, {{Twenty Years Later}}},
  author = {Diebold, Francis X.},
  year = {2015},
  month = jan,
  journal = {Journal of Business \& Economic Statistics},
  volume = {33},
  number = {1},
  pages = {1--1},
  issn = {0735-0015, 1537-2707},
  doi = {10.1080/07350015.2014.983236},
  urldate = {2023-02-28},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/A4SKDFMF/Diebold - 2015 - Comparing Predictive Accuracy, Twenty Years Later.pdf}
}

@article{diksLikelihoodbasedScoringRules2011,
  title = {Likelihood-Based Scoring Rules for Comparing Density Forecasts in Tails},
  author = {Diks, Cees and Panchenko, Valentyn and {van Dijk}, Dick},
  year = {2011},
  month = aug,
  journal = {Journal of Econometrics},
  volume = {163},
  number = {2},
  pages = {215--230},
  issn = {03044076},
  doi = {10.1016/j.jeconom.2011.04.001},
  urldate = {2023-01-11},
  abstract = {We propose new scoring rules based on conditional and censored likelihood for assessing the predictive accuracy of competing density forecasts over a specific region of interest, such as the left tail in financial risk management. These scoring rules can be interpreted in terms of Kullback-Leibler divergence between weighted versions of the density forecast and the true density. Existing scoring rules based on weighted likelihood favor density forecasts with more probability mass in the given region, rendering predictive accuracy tests biased towards such densities. Using our novel likelihood-based scoring rules avoids this problem.},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/GCMRVXUV/Diks et al. - 2011 - Likelihood-based scoring rules for comparing densi.pdf}
}

@article{doi:10.2105/AJPH.2022.306831,
  title = {Collaborative Hubs: {{Making}} the Most of Predictive Epidemic Modeling},
  author = {Reich, Nicholas G. and Lessler, Justin and Funk, Sebastian and Viboud, Cecile and Vespignani, Alessandro and Tibshirani, Ryan J. and Shea, Katriona and Schienle, Melanie and Runge, Michael C. and Rosenfeld, Roni and Ray, Evan L. and Niehus, Rene and Johnson, Helen C. and Johansson, Michael A. and Hochheiser, Harry and Gardner, Lauren and Bracher, Johannes and Borchering, Rebecca K. and Biggerstaff, Matthew},
  year = {2022},
  journal = {American Journal of Public Health},
  volume = {112},
  number = {6},
  eprint = {https://doi.org/10.2105/AJPH.2022.306831},
  pages = {839--842},
  doi = {10.2105/AJPH.2022.306831}
}

@article{doswellWeatherForecastingHumans2004,
  title = {Weather {{Forecasting}} by {{Humans}}---{{Heuristics}} and {{Decision Making}}},
  author = {Doswell, Charles A.},
  year = {2004},
  month = dec,
  journal = {Weather and Forecasting},
  volume = {19},
  number = {6},
  pages = {1115--1126},
  publisher = {American Meteorological Society},
  issn = {1520-0434, 0882-8156},
  doi = {10.1175/WAF-821.1},
  urldate = {2021-07-29},
  abstract = {{$<$}section class="abstract"{$><$}h2 class="abstractTitle text-title my-1" id="d2593115e58"{$>$}Abstract{$<$}/h2{$><$}p{$>$}The decision-making literature contains considerable information about how humans approach tasks involving uncertainty using heuristics. Although there is some reason to believe that weather forecasters are not identical in all respects to the typical subjects used in judgment and decision-making studies, there also is evidence that weather forecasters are not so different that the existing understanding of human cognition as it relates to making decisions is entirely inapplicable to weather forecasters. Accordingly, some aspects of cognition and decision making are reviewed and considered in terms of how they apply to human weather forecasters, including biases introduced by heuristics. Considerable insight into human forecasting could be gained by applying available studies of the cognitive psychology of decision making. What few studies exist that have used weather forecasters as subjects suggest that further work might well be productive in terms of helping to guide the improvement of weather forecasts by humans. It is concluded that a multidisciplinary approach, involving disciplines outside of meteorology, needs to be developed and supported if there is to be a future role for humans in forecasting the weather.{$<$}/p{$><$}/section{$>$}},
  chapter = {Weather and Forecasting},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/TTQ8P685/Doswell - 2004 - Weather Forecasting by Humans‚ÄîHeuristics and Decis.pdf;/Users/nikos/github-synced/zotero-nikos/storage/RN8ZIK26/waf-821_1.html}
}

@article{dreberUsingPredictionMarkets2015,
  title = {Using Prediction Markets to Estimate the Reproducibility of Scientific Research},
  author = {Dreber, Anna and Pfeiffer, Thomas and Almenberg, Johan and Isaksson, Siri and Wilson, Brad and Chen, Yiling and Nosek, Brian A. and Johannesson, Magnus},
  year = {2015},
  month = dec,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {112},
  number = {50},
  pages = {15343--15347},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1516179112},
  urldate = {2023-02-22},
  abstract = {Concerns about a lack of reproducibility of statistically significant results have recently been raised in many fields, and it has been argued that this lack comes at substantial economic costs. We here report the results from prediction markets set up to quantify the reproducibility of 44 studies published in prominent psychology journals and replicated in the Reproducibility Project: Psychology. The prediction markets predict the outcomes of the replications well and outperform a survey of market participants' individual forecasts. This shows that prediction markets are a promising tool for assessing the reproducibility of published scientific results. The prediction markets also allow us to estimate probabilities for the hypotheses being true at different testing stages, which provides valuable information regarding the temporal dynamics of scientific discovery. We find that the hypotheses being tested in psychology typically have low prior probabilities of being true (median, 9\%) and that a ``statistically significant'' finding needs to be confirmed in a well-powered replication to have a high probability of being true. We argue that prediction markets could be used to obtain speedy information about reproducibility at low cost and could potentially even be used to determine which studies to replicate to optimally allocate limited resources into replications.},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/VFKRPDUU/Dreber et al. - 2015 - Using prediction markets to estimate the reproduci.pdf}
}

@book{dunnGeneralizedLinearModels2018,
  title = {Generalized {{Linear Models With Examples}} in {{R}}},
  author = {Dunn, Peter K. and Smyth, Gordon K.},
  year = {2018},
  month = nov,
  publisher = {Springer},
  abstract = {This textbook presents an introduction to generalized linear models, complete with real-world data sets and practice problems, making it applicable for both beginning and advanced students of applied statistics. Generalized linear models (GLMs) are powerful tools in applied statistics that extend the ideas of multiple linear regression and analysis of variance to include response variables that are not normally distributed. As such, GLMs can model a wide variety of data types including counts, proportions, and binary outcomes or positive quantities.The book is designed with the student in mind, making it suitable for self-study or a structured course. Beginning with an introduction to linear regression, the book also devotes time to advanced topics not typically included in introductory textbooks. It features chapter introductions and summaries, clear examples, and many practice problems, all carefully designed to balance theory and practice. The text also provides a working knowledge of applied statistical practice through the extensive use of R, which is integrated into the text. Other features include: {$\bullet$} Advanced topics such as power variance functions, saddlepoint approximations, likelihood score tests, modified profile likelihood, small-dispersion asymptotics, and randomized quantile residuals {$\bullet$} Nearly 100 data sets in the companion R package GLMsData {$\bullet$} Examples that are cross-referenced to the companion data set, allowing readers to load the data and follow the analysis in their own R session},
  googlebooks = {tBh5DwAAQBAJ},
  isbn = {978-1-4419-0118-7},
  langid = {english},
  keywords = {Computers / Mathematical & Statistical Software,Mathematics / Probability & Statistics / General,Mathematics / Probability & Statistics / Stochastic Processes}
}

@article{dunsmoreBayesianApproachCalibration1968,
  title = {A {{Bayesian Approach}} to {{Calibration}}},
  author = {Dunsmore, I. R.},
  year = {1968},
  journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
  volume = {30},
  number = {2},
  pages = {396--405},
  issn = {2517-6161},
  doi = {10.1111/j.2517-6161.1968.tb00740.x},
  urldate = {2024-06-16},
  abstract = {A Bayesian statistical decision theory model for a wide range of prediction problems in regression situations was derived in a previous paper (Dunsmore, 1966). In this paper the model is applied to problems of calibration.},
  copyright = {{\copyright} 1968 The Authors},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/AV4VN7TD/j.2517-6161.1968.tb00740.html}
}

@article{dushoffSpeedStrengthEpidemic2021,
  title = {Speed and Strength of an Epidemic Intervention},
  author = {Dushoff, Jonathan and Park, Sang Woo},
  year = {2021},
  month = mar,
  journal = {Proceedings of the Royal Society B: Biological Sciences},
  volume = {288},
  number = {1947},
  pages = {20201556},
  publisher = {Royal Society},
  doi = {10.1098/rspb.2020.1556},
  urldate = {2022-11-29},
  abstract = {An epidemic can be characterized by its strength (i.e., the reproductive number  {$R$} R ) and speed (i.e., the exponential growth rate r). Disease modellers have historically placed much more emphasis on strength, in part because the effectiveness of an intervention strategy is typically evaluated on this scale. Here, we develop a mathematical framework for the classic, strength-based paradigm and show that there is a dual speed-based paradigm which can provide complementary insights. In particular, we note that r = 0 is a threshold for disease spread, just like  {$R$}=1 R=1  [ 1], and show that we can measure the strength and speed of an intervention on the same scale as the strength and speed of an epidemic, respectively. We argue that, while the strength-based paradigm provides the clearest insight into certain questions, the speed-based paradigm provides the clearest view in other cases. As an example, we show that evaluating the prospects of `test-and-treat' interventions against the human immunodeficiency virus (HIV) can be done more clearly on the speed than strength scale, given uncertainty in the proportion of HIV spread that happens early in the course of infection. We also discuss evaluating the effects of the importance of pre-symptomatic transmission of the SARS-CoV-2 virus. We suggest that disease modellers should avoid over-emphasizing the reproductive number at the expense of the exponential growth rate, but instead look at these as complementary measures.},
  keywords = {exponential growth rate,mathematical epidemiology,reproductive number,speed and strength},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/3VZUTIGT/Dushoff and Park - 2021 - Speed and strength of an epidemic intervention.pdf}
}

@article{eatonAssessmentEpidemicProjections2015,
  title = {Assessment of Epidemic Projections Using Recent {{HIV}} Survey Data in {{South Africa}}: A Validation Analysis of Ten Mathematical Models of {{HIV}} Epidemiology in the Antiretroviral Therapy Era},
  shorttitle = {Assessment of Epidemic Projections Using Recent {{HIV}} Survey Data in {{South Africa}}},
  author = {Eaton, Jeffrey W. and Baca{\"e}r, Nicolas and Bershteyn, Anna and Cambiano, Valentina and Cori, Anne and Dorrington, Rob E. and Fraser, Christophe and Gopalappa, Chaitra and Hontelez, Jan A. C. and Johnson, Leigh F. and Klein, Daniel J. and Phillips, Andrew N. and Pretorius, Carel and Stover, John and Rehle, Thomas M. and Hallett, Timothy B.},
  year = {2015},
  month = oct,
  journal = {The Lancet Global Health},
  volume = {3},
  number = {10},
  pages = {e598-e608},
  publisher = {Elsevier},
  issn = {2214-109X},
  doi = {10.1016/S2214-109X(15)00080-7},
  urldate = {2021-07-21},
  abstract = {{$<$}h2{$>$}Summary{$<$}/h2{$><$}h3{$>$}Background{$<$}/h3{$><$}p{$>$}Mathematical models are widely used to simulate the effects of interventions to control HIV and to project future epidemiological trends and resource needs. We aimed to validate past model projections against data from a large household survey done in South Africa in 2012.{$<$}/p{$><$}h3{$>$}Methods{$<$}/h3{$><$}p{$>$}We compared ten model projections of HIV prevalence, HIV incidence, and antiretroviral therapy (ART) coverage for South Africa with estimates from national household survey data from 2012. Model projections for 2012 were made before the publication of the 2012 household survey. We compared adult (age 15--49 years) HIV prevalence in 2012, the change in prevalence between 2008 and 2012, and prevalence, incidence, and ART coverage by sex and by age groups between model projections and the 2012 household survey.{$<$}/p{$><$}h3{$>$}Findings{$<$}/h3{$><$}p{$>$}All models projected lower prevalence estimates for 2012 than the survey estimate (18{$\cdot$}8\%), with eight models' central projections being below the survey 95\% CI (17{$\cdot$}5--20{$\cdot$}3). Eight models projected that HIV prevalence would remain unchanged (n=5) or decline (n=3) between 2008 and 2012, whereas prevalence estimates from the household surveys increased from 16{$\cdot$}9\% in 2008 to 18{$\cdot$}8\% in 2012 (difference 1{$\cdot$}9, 95\% CI -0{$\cdot$}1 to 3{$\cdot$}9). Model projections accurately predicted the 1{$\cdot$}6 percentage point prevalence decline (95\% CI -0{$\cdot$}3 to 3{$\cdot$}5) in young adults aged 15--24 years, and the 2{$\cdot$}2 percentage point (0{$\cdot$}5 to 3{$\cdot$}9) increase in those aged 50 years and older. Models accurately represented the number of adults on ART in 2012; six of ten models were within the survey 95\% CI of 1{$\cdot$}54--2{$\cdot$}12 million. However, the differential ART coverage between women and men was not fully captured; all model projections of the sex ratio of women to men on ART were lower than the survey estimate of 2{$\cdot$}22 (95\% CI 1{$\cdot$}73--2{$\cdot$}71).{$<$}/p{$><$}h3{$>$}Interpretation{$<$}/h3{$><$}p{$>$}Projections for overall declines in HIV epidemics during the ART era might have been optimistic. Future treatment and HIV prevention needs might be greater than previously forecasted. Additional data about service provision for HIV care could help inform more accurate projections.{$<$}/p{$><$}h3{$>$}Funding{$<$}/h3{$><$}p{$>$}Bill \& Melinda Gates Foundation.{$<$}/p{$>$}},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/P4EAPBV4/Eaton et al. - 2015 - Assessment of epidemic projections using recent HI.pdf;/Users/nikos/github-synced/zotero-nikos/storage/L7HHV4JV/fulltext.html}
}

@article{eatonHIVTreatmentPrevention2012,
  title = {{{HIV Treatment}} as {{Prevention}}: {{Systematic Comparison}} of {{Mathematical Models}} of the {{Potential Impact}} of {{Antiretroviral Therapy}} on {{HIV Incidence}} in {{South Africa}}},
  shorttitle = {{{HIV Treatment}} as {{Prevention}}},
  author = {Eaton, Jeffrey W. and Johnson, Leigh F. and Salomon, Joshua A. and B{\"a}rnighausen, Till and Bendavid, Eran and Bershteyn, Anna and Bloom, David E. and Cambiano, Valentina and Fraser, Christophe and Hontelez, Jan A. C. and Humair, Salal and Klein, Daniel J. and Long, Elisa F. and Phillips, Andrew N. and Pretorius, Carel and Stover, John and Wenger, Edward A. and Williams, Brian G. and Hallett, Timothy B.},
  year = {2012},
  month = jul,
  journal = {PLOS Medicine},
  volume = {9},
  number = {7},
  pages = {e1001245},
  publisher = {Public Library of Science},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.1001245},
  urldate = {2021-07-21},
  abstract = {Background Many mathematical models have investigated the impact of expanding access to antiretroviral therapy (ART) on new HIV infections. Comparing results and conclusions across models is challenging because models have addressed slightly different questions and have reported different outcome metrics. This study compares the predictions of several mathematical models simulating the same ART intervention programmes to determine the extent to which models agree about the epidemiological impact of expanded ART. Methods and Findings Twelve independent mathematical models evaluated a set of standardised ART intervention scenarios in South Africa and reported a common set of outputs. Intervention scenarios systematically varied the CD4 count threshold for treatment eligibility, access to treatment, and programme retention. For a scenario in which 80\% of HIV-infected individuals start treatment on average 1 y after their CD4 count drops below 350 cells/{\textmu}l and 85\% remain on treatment after 3 y, the models projected that HIV incidence would be 35\% to 54\% lower 8 y after the introduction of ART, compared to a counterfactual scenario in which there is no ART. More variation existed in the estimated long-term (38 y) reductions in incidence. The impact of optimistic interventions including immediate ART initiation varied widely across models, maintaining substantial uncertainty about the theoretical prospect for elimination of HIV from the population using ART alone over the next four decades. The number of person-years of ART per infection averted over 8 y ranged between 5.8 and 18.7. Considering the actual scale-up of ART in South Africa, seven models estimated that current HIV incidence is 17\% to 32\% lower than it would have been in the absence of ART. Differences between model assumptions about CD4 decline and HIV transmissibility over the course of infection explained only a modest amount of the variation in model results. Conclusions Mathematical models evaluating the impact of ART vary substantially in structure, complexity, and parameter choices, but all suggest that ART, at high levels of access and with high adherence, has the potential to substantially reduce new HIV infections. There was broad agreement regarding the short-term epidemiologic impact of ambitious treatment scale-up, but more variation in longer term projections and in the efficiency with which treatment can reduce new infections. Differences between model predictions could not be explained by differences in model structure or parameterization that were hypothesized to affect intervention impact. Please see later in the article for the Editors' Summary},
  langid = {english},
  keywords = {Antiretroviral therapy,HIV,HIV epidemiology,HIV infections,HIV prevention,Mathematical models,Medical risk factors,South Africa},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/WHULJXLN/Eaton et al. - 2012 - HIV Treatment as Prevention Systematic Comparison.pdf;/Users/nikos/github-synced/zotero-nikos/storage/9WZAKPMW/article.html}
}

@misc{ecdcDownloadHistoricalData2020,
  title = {Download Historical Data (to 14 {{December}} 2020) on the Daily Number of New Reported {{COVID-19}} Cases and Deaths Worldwide},
  author = {{ECDC}},
  year = {2020},
  month = dec,
  journal = {European Centre for Disease Prevention and Control},
  urldate = {2021-05-30},
  abstract = {The downloadable data file was updated daily to 14 December 2020 using the latest available public data on COVID-19. You may use the data in line with ECDC's copyright policy.},
  howpublished = {https://www.ecdc.europa.eu/en/publications-data/download-todays-data-geographic-distribution-covid-19-cases-worldwide},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/F4F3UBWA/download-todays-data-geographic-distribution-covid-19-cases-worldwide.html}
}

@article{elliottForecastingEconomicsFinance2016,
  title = {Forecasting in {{Economics}} and {{Finance}}},
  author = {Elliott, Graham and Timmermann, Allan},
  year = {2016},
  journal = {Annual Review of Economics},
  volume = {8},
  number = {1},
  pages = {81--110},
  doi = {10.1146/annurev-economics-080315-015346},
  urldate = {2021-12-15},
  abstract = {Practices used to address economic forecasting problems have undergone substantial changes over recent years. We review how such changes have influenced the ways in which a range of forecasting questions are being addressed. We also discuss the promises and challenges arising from access to big data. Finally, we review empirical evidence and experience accumulated from the use of forecasting methods to a range of economic and financial variables.},
  keywords = {big data,forecast evaluation,forecast models,model instability,model misspecification,parameter estimation,risk},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/3AXCLA59/Elliott and Timmermann - 2016 - Forecasting in Economics and Finance.pdf}
}

@article{enayatiBiasCorrectionCapabilities2021,
  title = {Bias Correction Capabilities of Quantile Mapping Methods for Rainfall and Temperature Variables},
  author = {Enayati, Maedeh and {Bozorg-Haddad}, Omid and Bazrafshan, Javad and Hejabi, Somayeh and Chu, Xuefeng},
  year = {2021},
  month = mar,
  journal = {Journal of Water and Climate Change},
  volume = {12},
  number = {2},
  pages = {401--419},
  publisher = {IWA Publishing},
  issn = {2040-2244},
  doi = {10.2166/wcc.2020.261},
  urldate = {2021-10-28},
  abstract = {HIGHLIGHTS. Quantile mapping (QM) techniques are among the most important and popular bias correction methods. This study aims to provide a comprehensive compar},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/7TBW9UR2/Bias-correction-capabilities-of-quantile-mapping.html}
}

@misc{epiforecasts.io/covidCovid19TemporalVariation2020,
  title = {Covid-19: {{Temporal}} Variation in Transmission during the {{COVID-19}} Outbreak},
  shorttitle = {Covid-19},
  author = {{epiforecasts.io/covid}},
  year = {2020},
  journal = {Covid-19},
  urldate = {2021-05-30},
  howpublished = {https://epiforecasts.io/covid/},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/VC3R2A6X/covid.html}
}

@misc{epiforecasts.io/covidCovid19TemporalVariation2020,
  title = {Covid-19: {{Temporal}} Variation in Transmission during the {{COVID-19}} Outbreak},
  shorttitle = {Covid-19},
  author = {{epiforecasts.io/covid}},
  year = {2020},
  urldate = {2021-05-30},
  howpublished = {https://epiforecasts.io/covid/},
  organization = {Covid-19}
}

@article{epsteinScoringSystemProbability1969,
  title = {A {{Scoring System}} for {{Probability Forecasts}} of {{Ranked Categories}}},
  author = {Epstein, Edward S.},
  year = {1969},
  month = dec,
  journal = {Journal of Applied Meteorology},
  volume = {8},
  number = {6},
  pages = {985--987},
  publisher = {American Meteorological Society},
  issn = {0021-8952},
  doi = {10.1175/1520-0450(1969)008<0985:ASSFPF>2.0.CO;2},
  urldate = {2020-08-13},
  langid = {english},
  keywords = {ranked probability score,RPS},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/XAVX39GC/Epstein - 1969 - A Scoring System for Probability Forecasts of Rank.pdf;/Users/nikos/github-synced/zotero-nikos/storage/CVK2YPKP/A-Scoring-System-for-Probability-Forecasts-of.html}
}

@misc{europeancovid-19forecasthubEuropeanCovid19Forecast2021,
  title = {European {{Covid-19 Forecast Hub}}},
  author = {{European Covid-19 Forecast Hub}},
  year = {2021},
  urldate = {2021-05-30},
  howpublished = {https://covid19forecasthub.eu/},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/JRFUHRDI/covid19forecasthub.eu.html}
}

@article{europeanfoodsafetyauthorityGuidanceExpertKnowledge2014,
  title = {Guidance on {{Expert Knowledge Elicitation}} in {{Food}} and {{Feed Safety Risk Assessment}}},
  author = {{European Food Safety Authority}},
  year = {2014},
  month = jun,
  journal = {EFSA Journal},
  volume = {12},
  number = {6},
  issn = {18314732, 18314732},
  doi = {10.2903/j.efsa.2014.3734},
  urldate = {2021-11-06},
  abstract = {Abstract Quantitative risk assessments facilitate the decisions of risk managers. In the EU, risk assessment in food and feed safety is the responsibility of the European Food Safety Authority (EFSA). Quantitative risk models should be informed by systematically reviewed scientific evidence, however, in practice empirical evidence is often limited: in such cases it is necessary to turn to expert judgement. Psychological research has shown that unaided expert judgement of the quantities required for risk modelling - and particularly the uncertainty associated with such judgements - is often biased, thus limiting its value. Accordingly methods have been developed for eliciting knowledge from experts in as unbiased a manner as possible. In 2012, a working group was established to develop guidance on expert knowledge elicitation appropriate to EFSA's remit. The resulting Guidance first presents expert knowledge elicitation as a process beginning with defining the risk assessment problem, moving through preparation for elicitation (e.g. selecting the experts and the method to be used) and the elicitation itself, culminating in documentation. Those responsible for managing each of these phases are identified. Next three detailed protocols for expert knowledge elicitation are given - that can be applied to real-life questions in food and feed safety - and the pros and cons of each of these protocols are examined. This is followed by principles for overcoming the major challenges to expert knowledge elicitation: framing the question; selecting the experts; eliciting uncertainty; aggregating the results of multiple experts; and documenting the process. The results of a web search on existing guidance documents on expert elicitation are then reported, along with case studies illustrating some of the protocols of the Guidance. Finally, recommendations are made in the areas of training, organisational changes, expert identification and management, and further developments of expert knowledge elicitation methodology within EFSA.},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/7PHJCHDH/European Food Safety Authority - 2014 - Guidance on Expert Knowledge Elicitation in Food a.pdf}
}

@misc{EvaluatingUseReproduction,
  title = {Evaluating the Use of the Reproduction Number as an Epidemiological Tool, Using Spatio-Temporal Trends of the {{Covid-19}} Outbreak in {{England}} {\textbar} {{medRxiv}}},
  urldate = {2021-05-30},
  howpublished = {https://www.medrxiv.org/content/10.1101/2020.10.18.20214585v1},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/VC2FV68A/2020.10.18.html}
}

@misc{evanl.rayChallengesTrainingEnsembles,
  title = {Challenges in Training Ensembles to Forecast {{COVID-19}} Cases and Deaths in the {{United States}} - {{International Institute}} of {{Forecasters}}},
  author = {{Evan L. Ray} and {Logan C. Brooks} and {Jacob Bien} and {Johannes Bracher} and {Aaron Gerding} and {Aaron Rumack} and {Matthew Biggerstaff} and {Michael A. Johansson} and {Ryan J. Tibshirani} and {Nicholas G. Reich}},
  urldate = {2021-07-12},
  chapter = {Forecasting News},
  langid = {american},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/LQVPJLRR/challenges-in-training-ensembles-to-forecast-covid-19-cases-and-deaths-in-the-united-states.html}
}

@article{farrowHumanJudgmentApproach2017,
  title = {A Human Judgment Approach to Epidemiological Forecasting},
  author = {Farrow, David C. and Brooks, Logan C. and Hyun, Sangwon and Tibshirani, Ryan J. and Burke, Donald S. and Rosenfeld, Roni},
  year = {2017},
  month = mar,
  journal = {PLOS Computational Biology},
  volume = {13},
  number = {3},
  pages = {e1005248},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1005248},
  urldate = {2021-07-29},
  abstract = {Infectious diseases impose considerable burden on society, despite significant advances in technology and medicine over the past century. Advanced warning can be helpful in mitigating and preparing for an impending or ongoing epidemic. Historically, such a capability has lagged for many reasons, including in particular the uncertainty in the current state of the system and in the understanding of the processes that drive epidemic trajectories. Presently we have access to data, models, and computational resources that enable the development of epidemiological forecasting systems. Indeed, several recent challenges hosted by the U.S. government have fostered an open and collaborative environment for the development of these technologies. The primary focus of these challenges has been to develop statistical and computational methods for epidemiological forecasting, but here we consider a serious alternative based on collective human judgment. We created the web-based ``Epicast'' forecasting system which collects and aggregates epidemic predictions made in real-time by human participants, and with these forecasts we ask two questions: how accurate is human judgment, and how do these forecasts compare to their more computational, data-driven alternatives? To address the former, we assess by a variety of metrics how accurately humans are able to predict influenza and chikungunya trajectories. As for the latter, we show that real-time, combined human predictions of the 2014--2015 and 2015--2016 U.S. flu seasons are often more accurate than the same predictions made by several statistical systems, especially for short-term targets. We conclude that there is valuable predictive power in collective human judgment, and we discuss the benefits and drawbacks of this approach.},
  langid = {english},
  keywords = {Chikungunya infection,Epidemiological methods and statistics,Epidemiological statistics,Epidemiology,Forecasting,Infectious diseases,Influenza,Statistical methods},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/N6MZGQ7M/Farrow et al. - 2017 - A human judgment approach to epidemiological forec.pdf;/Users/nikos/github-synced/zotero-nikos/storage/VSY8MVUC/article.html}
}

@techreport{fergusonReportImpactNonpharmaceutical2020,
  type = {Report},
  title = {Report 9: {{Impact}} of Non-Pharmaceutical Interventions ({{NPIs}}) to Reduce {{COVID19}} Mortality and Healthcare Demand},
  shorttitle = {Report 9},
  author = {Ferguson, N. and Laydon, D. and Nedjati Gilani, G. and Imai, N. and Ainslie, K. and Baguelin, M. and Bhatia, S. and Boonyasiri, A. and Cucunuba Perez, Z. and {Cuomo-Dannenburg}, G. and Dighe, A. and Dorigatti, I. and Fu, H. and Gaythorpe, K. and Green, W. and Hamlet, A. and Hinsley, W. and Okell, L. and Van Elsland, S. and Thompson, H. and Verity, R. and Volz, E. and Wang, H. and Wang, Y. and Walker, P. and Walters, C. and Winskill, P. and Whittaker, C. and Donnelly, C. and Riley, S. and Ghani, A.},
  year = {2020},
  month = mar,
  journal = {20},
  doi = {10.25561/77482},
  urldate = {2021-05-29},
  abstract = {The global impact of COVID-19 has been profound, and the public health threat it represents is the most serious seen in a respiratory virus since the 1918 H1N1 influenza pandemic. Here we present the results of epidemiological modelling which has informed policymaking in the UK and other countries in recent weeks. In the absence of a COVID-19 vaccine, we assess the potential role of a number of public health measures -- so-called non-pharmaceutical interventions (NPIs) -- aimed at reducing contact rates in the population and thereby reducing transmission of the virus. In the results presented here, we apply a previously published microsimulation model to two countries: the UK (Great Britain specifically) and the US. We conclude that the effectiveness of any one intervention in isolation is likely to be limited, requiring multiple interventions to be combined to have a substantial impact on transmission. Two fundamental strategies are possible: (a) mitigation, which focuses on slowing but not necessarily stopping epidemic spread -- reducing peak healthcare demand while protecting those most at risk of severe disease from infection, and (b) suppression, which aims to reverse epidemic growth, reducing case numbers to low levels and maintaining that situation indefinitely. Each policy has major challenges. We find that that optimal mitigation policies (combining home isolation of suspect cases, home quarantine of those living in the same household as suspect cases, and social distancing of the elderly and others at most risk of severe disease) might reduce peak healthcare demand by 2/3 and deaths by half. However, the resulting mitigated epidemic would still likely result in hundreds of thousands of deaths and health systems (most notably intensive care units) being overwhelmed many times over. For countries able to achieve it, this leaves suppression as the preferred policy option. We show that in the UK and US context, suppression will minimally require a combination of social distancing of the entire population, home isolation of cases and household quarantine of their family members. This may need to be supplemented by school and university closures, though it should be recognised that such closures may have negative impacts on health systems due to increased absenteeism. The major challenge of suppression is that this type of intensive intervention package -- or something equivalently effective at reducing transmission -- will need to be maintained until a vaccine becomes available (potentially 18 months or more) -- given that we predict that transmission will quickly rebound if interventions are relaxed. We show that intermittent social distancing -- triggered by trends in disease surveillance -- may allow interventions to be relaxed temporarily in relative short time windows, but measures will need to be reintroduced if or when case numbers rebound. Last, while experience in China and now South Korea show that suppression is possible in the short term, it remains to be seen whether it is possible long-term, and whether the social and economic costs of the interventions adopted thus far can be reduced.},
  copyright = {{\copyright} 2020. This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License (https://creativecommons.org/licenses/by-nc-nd/4.0/).},
  langid = {english},
  annotation = {Accepted: 2020-03-17T09:57:15Z},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/HQC2XC9J/Ferguson et al. - 2020 - Report 9 Impact of non-pharmaceutical interventio.pdf;/Users/nikos/github-synced/zotero-nikos/storage/MEY92F4G/77482.html}
}

@misc{ForecastHub,
  title = {{{ForecastHub}}},
  urldate = {2021-11-06},
  howpublished = {https://kitmetricslab.github.io/forecasthub/forecast},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/2IEEQJR4/forecast.html}
}

@misc{ForecastVisualisation,
  title = {Forecast Visualisation},
  urldate = {2021-05-30},
  howpublished = {https://covid19forecasthub.eu/visualisation.html}
}

@misc{forsal.plRozbieznosciStatystykachKoronawirusa2020,
  title = {{Rozbie{\.z}no{\'s}ci w statystykach koronawirusa. 22 tys. przypadk{\'o}w b{\k e}d{\k a} doliczone do og{\'o}lnej liczby wynik{\'o}w}},
  author = {{Forsal.pl}},
  year = {2020},
  urldate = {2021-05-30},
  abstract = {Od wtorku narz{\k e}dzia rejestracji wynik{\'o}w test{\'o}w na koronawirusa zostan{\k a} ujednolicone. Brakuj{\k a}ce ok. 22 tys. przypadk{\'o}w, kt{\'o}re nie zosta{\l}y wcze{\'s}niej podane, trafi{\k a} we wtorek do og{\'o}lnej liczby zaka{\.z}e{\'n} -- poda{\l} GIS.},
  howpublished = {https://forsal.pl/lifestyle/zdrowie/artykuly/8017628,rozbieznosci-w-statystykach-koronawirusa-22-tys-przypadkow-beda-doliczone-do-ogolnej-liczby-wynikow.html},
  langid = {polish},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/ZXP27ZRQ/8017628,rozbieznosci-w-statystykach-koronawirusa-22-tys-przypadkow-beda-doliczone-do-ogolnej-li.html}
}

@article{fraserEstimatingIndividualHousehold2007,
  title = {Estimating {{Individual}} and {{Household Reproduction Numbers}} in an {{Emerging Epidemic}}},
  author = {Fraser, Christophe},
  year = {2007},
  month = aug,
  journal = {PLOS ONE},
  volume = {2},
  number = {8},
  pages = {e758},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0000758},
  urldate = {2021-09-29},
  abstract = {Reproduction numbers, defined as averages of the number of people infected by a typical case, play a central role in tracking infectious disease outbreaks. The aim of this paper is to develop methods for estimating reproduction numbers which are simple enough that they could be applied with limited data or in real time during an outbreak. I present a new estimator for the individual reproduction number, which describes the state of the epidemic at a point in time rather than tracking individuals over time, and discuss some potential benefits. Then, to capture more of the detail that micro-simulations have shown is important in outbreak dynamics, I analyse a model of transmission within and between households, and develop a method to estimate the household reproduction number, defined as the number of households infected by each infected household. This method is validated by numerical simulations of the spread of influenza and measles using historical data, and estimates are obtained for would-be emerging epidemics of these viruses. I argue that the household reproduction number is useful in assessing the impact of measures that target the household for isolation, quarantine, vaccination or prophylactic treatment, and measures such as social distancing and school or workplace closures which limit between-household transmission, all of which play a key role in current thinking on future infectious disease mitigation.},
  langid = {english},
  keywords = {Epidemiological methods and statistics,Epidemiology,Infectious disease epidemiology,Influenza,Measles,Measles virus,Pandemics,Respiratory infections},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/MKA528M7/Fraser - 2007 - Estimating Individual and Household Reproduction N.pdf;/Users/nikos/github-synced/zotero-nikos/storage/A795DEET/article.html}
}

@book{frauenthalMathematicalModelingEpidemiology1980,
  title = {Mathematical {{Modeling}} in {{Epidemiology}}},
  author = {Frauenthal, James C.},
  year = {1980},
  series = {Universitext},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-67795-3},
  urldate = {2024-06-16},
  copyright = {http://www.springer.com/tdm},
  isbn = {978-3-540-10328-8 978-3-642-67795-3},
  keywords = {biologisch-mathematisches Modell,Epidemiologie,epidemiology,linear optimization,mathematical modeling,mathematics,Modeling},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/BVAGZNT9/Frauenthal - 1980 - Mathematical Modeling in Epidemiology.pdf}
}

@article{fuglstadDoesNonstationarySpatial2015,
  title = {Does Non-Stationary Spatial Data Always Require Non-Stationary Random Fields?},
  author = {Fuglstad, Geir-Arne and Simpson, Daniel and Lindgren, Finn and Rue, H{\aa}vard},
  year = {2015},
  month = nov,
  journal = {Spatial Statistics},
  volume = {14},
  pages = {505--531},
  issn = {2211-6753},
  doi = {10.1016/j.spasta.2015.10.001},
  urldate = {2022-12-07},
  abstract = {A stationary spatial model is an idealization and we expect that the true dependence structures of physical phenomena are spatially varying, but how should we handle this non-stationarity in practice? We study the challenges involved in applying a flexible non-stationary model to a dataset of annual precipitation in the conterminous US, where exploratory data analysis shows strong evidence of a non-stationary covariance structure. The aim of this paper is to investigate the modelling pipeline once non-stationarity has been detected in spatial data. We show that there is a real danger of over-fitting the model and that careful modelling is necessary in order to properly account for varying second-order structure. In fact, the example shows that sometimes non-stationary Gaussian random fields are not necessary to model non-stationary spatial data.},
  langid = {english},
  keywords = {Annual precipitation,Gaussian Markov random fields,Gaussian random fields,Non-stationary spatial modelling,Penalized maximum likelihood,Stochastic partial differential equations},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/F5ZMGL6I/Fuglstad et al. - 2015 - Does non-stationary spatial data always require no.pdf;/Users/nikos/github-synced/zotero-nikos/storage/RBWNKHT6/S2211675315000780.html}
}

@article{funkAssessingPerformanceRealtime2019,
  title = {Assessing the Performance of Real-Time Epidemic Forecasts: {{A}} Case Study of {{Ebola}} in the {{Western Area}} Region of {{Sierra Leone}}, 2014-15},
  shorttitle = {Assessing the Performance of Real-Time Epidemic Forecasts},
  author = {Funk, Sebastian and Camacho, Anton and Kucharski, Adam J. and Lowe, Rachel and Eggo, Rosalind M. and Edmunds, W. John},
  year = {2019},
  month = feb,
  journal = {PLOS Computational Biology},
  volume = {15},
  number = {2},
  pages = {e1006785},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1006785},
  urldate = {2019-09-16},
  abstract = {Real-time forecasts based on mathematical models can inform critical decision-making during infectious disease outbreaks. Yet, epidemic forecasts are rarely evaluated during or after the event, and there is little guidance on the best metrics for assessment. Here, we propose an evaluation approach that disentangles different components of forecasting ability using metrics that separately assess the calibration, sharpness and bias of forecasts. This makes it possible to assess not just how close a forecast was to reality but also how well uncertainty has been quantified. We used this approach to analyse the performance of weekly forecasts we generated in real time for Western Area, Sierra Leone, during the 2013--16 Ebola epidemic in West Africa. We investigated a range of forecast model variants based on the model fits generated at the time with a semi-mechanistic model, and found that good probabilistic calibration was achievable at short time horizons of one or two weeks ahead but model predictions were increasingly unreliable at longer forecasting horizons. This suggests that forecasts may have been of good enough quality to inform decision making based on predictions a few weeks ahead of time but not longer, reflecting the high level of uncertainty in the processes driving the trajectory of the epidemic. Comparing forecasts based on the semi-mechanistic model to simpler null models showed that the best semi-mechanistic model variant performed better than the null models with respect to probabilistic calibration, and that this would have been identified from the earliest stages of the outbreak. As forecasts become a routine part of the toolkit in public health, standards for evaluation of performance will be important for assessing quality and improving credibility of mathematical models, and for elucidating difficulties and trade-offs when aiming to make the most useful and reliable forecasts.},
  langid = {english},
  keywords = {Epidemiology,Forecasting,Infectious disease epidemiology,Infectious diseases,Mathematical models,Probability distribution,Public and occupational health,Sierra Leone},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/X6Z9PIFT/Funk et al. - 2019 - Assessing the performance of real-time epidemic fo.pdf;/Users/nikos/github-synced/zotero-nikos/storage/JN28VVKF/article.html}
}

@article{funkShorttermForecastsInform2020,
  title = {Short-Term Forecasts to Inform the Response to the {{Covid-19}} Epidemic in the {{UK}}},
  author = {Funk, Sebastian and Abbott, Sam and Atkins, B. D. and Baguelin, M. and Baillie, J. K. and Birrell, P. and Blake, J. and Bosse, Nikos I. and Burton, J. and Carruthers, J. and Davies, N. G. and Angelis, D. De and Dyson, L. and Edmunds, W. J. and Eggo, R. M. and Ferguson, N. M. and Gaythorpe, K. and Gorsich, E. and {Guyver-Fletcher}, G. and Hellewell, J. and Hill, E. M. and Holmes, A. and House, T. A. and Jewell, C. and Jit, M. and Jombart, T. and Joshi, I. and Keeling, M. J. and Kendall, E. and Knock, E. S. and Kucharski, A. J. and Lythgoe, K. A. and Meakin, S. R. and Munday, J. D. and Openshaw, P. J. M. and Overton, C. E. and Pagani, F. and Pearson, J. and {Perez-Guzman}, P. N. and Pellis, L. and Scarabel, F. and Semple, M. G. and Sherratt, K. and Tang, M. and Tildesley, M. J. and {van Leeuwen}, E. and Whittles, L. K. and Group, CMMID COVID-19 Working and Team, Imperial College COVID-19 Response and Investigators, Isaric4c},
  year = {2020},
  month = nov,
  journal = {medRxiv},
  pages = {2020.11.11.20220962},
  publisher = {Cold Spring Harbor Laboratory Press},
  doi = {10.1101/2020.11.11.20220962},
  urldate = {2020-11-28},
  abstract = {{$<$}p{$>$}Background: Short-term forecasts of infectious disease can create situational awareness and inform planning for outbreak response. Here, we report on multi-model forecasts of Covid-19 in the UK that were generated at regular intervals starting at the end of March 2020, in order to monitor expected healthcare utilisation and population impacts in real time. Methods: We evaluated the performance of individual model forecasts generated between 24 March and 14 July 2020, using a variety of metrics including the weighted interval score as well as metrics that assess the calibration, sharpness, bias and absolute error of forecasts separately. We further combined the predictions from individual models to ensemble forecasts using a simple mean as well as a quantile regression average that aimed to maximise performance. We further compared model performance to a null model of no change. Results: In most cases, individual models performed better than the null model, and ensembles models were well calibrated and performed comparatively to the best individual models. The quantile regression average did not noticeably outperform the mean ensemble. Conclusions: Ensembles of multi-model forecasts can inform the policy response to the Covid-19 pandemic by assessing future resource needs and expected population impact of morbidity and mortality.{$<$}/p{$>$}},
  copyright = {{\copyright} 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/9RK57885/Funk et al. - 2020 - Short-term forecasts to inform the response to the.pdf;/Users/nikos/github-synced/zotero-nikos/storage/AKDY6PAQ/2020.11.11.20220962v1.full.html}
}

@article{ganyaniEstimatingGenerationInterval2020,
  title = {Estimating the Generation Interval for Coronavirus Disease ({{COVID-19}}) Based on Symptom Onset Data, {{March}} 2020},
  author = {Ganyani, Tapiwa and Kremer, C{\'e}cile and Chen, Dongxuan and Torneri, Andrea and Faes, Christel and Wallinga, Jacco and Hens, Niel},
  year = {2020},
  month = apr,
  journal = {Euro Surveillance: Bulletin Europeen Sur Les Maladies Transmissibles = European Communicable Disease Bulletin},
  volume = {25},
  number = {17},
  pages = {2000257},
  issn = {1560-7917},
  doi = {10.2807/1560-7917.ES.2020.25.17.2000257},
  abstract = {BackgroundEstimating key infectious disease parameters from the coronavirus disease (COVID-19) outbreak is essential for modelling studies and guiding intervention strategies.AimWe estimate the generation interval, serial interval, proportion of pre-symptomatic transmission and effective reproduction number of COVID-19. We illustrate that reproduction numbers calculated based on serial interval estimates can be biased.MethodsWe used outbreak data from clusters in Singapore and Tianjin, China to estimate the generation interval from symptom onset data while acknowledging uncertainty about the incubation period distribution and the underlying transmission network. From those estimates, we obtained the serial interval, proportions of pre-symptomatic transmission and reproduction numbers.ResultsThe mean generation interval was 5.20 days (95\% credible interval (CrI): 3.78-6.78) for Singapore and 3.95 days (95\% CrI: 3.01-4.91) for Tianjin. The proportion of pre-symptomatic transmission was 48\% (95\% CrI: 32-67) for Singapore and 62\% (95\% CrI: 50-76) for Tianjin. Reproduction number estimates based on the generation interval distribution were slightly higher than those based on the serial interval distribution. Sensitivity analyses showed that estimating these quantities from outbreak data requires detailed contact tracing information.ConclusionHigh estimates of the proportion of pre-symptomatic transmission imply that case finding and contact tracing need to be supplemented by physical distancing measures in order to control the COVID-19 outbreak. Notably, quarantine and other containment measures were already in place at the time of data collection, which may inflate the proportion of infections from pre-symptomatic individuals.},
  langid = {english},
  pmcid = {PMC7201952},
  pmid = {32372755},
  keywords = {Asymptomatic Infections,Betacoronavirus,China,Coronavirus,Coronavirus Infections,COVID-19,Disease Outbreaks,generation interval,Humans,incubation period,Models Theoretical,Pandemics,Pneumonia Viral,Quarantine,reproduction number,SARS-CoV-2,serial interval,Singapore,Time Factors},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/C5BSX2UJ/Ganyani et al. - 2020 - Estimating the generation interval for coronavirus.pdf}
}

@article{gebetsbergerEstimationMethodsNonhomogeneous2018,
  title = {Estimation {{Methods}} for {{Nonhomogeneous Regression Models}}: {{Minimum Continuous Ranked Probability Score}} versus {{Maximum Likelihood}}},
  shorttitle = {Estimation {{Methods}} for {{Nonhomogeneous Regression Models}}},
  author = {Gebetsberger, Manuel and Messner, Jakob W. and Mayr, Georg J. and Zeileis, Achim},
  year = {2018},
  month = dec,
  journal = {Monthly Weather Review},
  volume = {146},
  number = {12},
  pages = {4323--4338},
  publisher = {American Meteorological Society},
  issn = {1520-0493, 0027-0644},
  doi = {10.1175/MWR-D-17-0364.1},
  urldate = {2022-02-20},
  abstract = {Abstract Nonhomogeneous regression models are widely used to statistically postprocess numerical ensemble weather prediction models. Such regression models are capable of forecasting full probability distributions and correcting for ensemble errors in the mean and variance. To estimate the corresponding regression coefficients, minimization of the continuous ranked probability score (CRPS) has widely been used in meteorological postprocessing studies and has often been found to yield more calibrated forecasts compared to maximum likelihood estimation. From a theoretical perspective, both estimators are consistent and should lead to similar results, provided the correct distribution assumption about empirical data. Differences between the estimated values indicate a wrong specification of the regression model. This study compares the two estimators for probabilistic temperature forecasting with nonhomogeneous regression, where results show discrepancies for the classical Gaussian assumption. The heavy-tailed logistic and Student's t distributions can improve forecast performance in terms of sharpness and calibration, and lead to only minor differences between the estimators employed. Finally, a simulation study confirms the importance of appropriate distribution assumptions and shows that for a correctly specified model the maximum likelihood estimator is slightly more efficient than the CRPS estimator.},
  chapter = {Monthly Weather Review},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/8GUSMDXL/Gebetsberger et al. - 2018 - Estimation Methods for Nonhomogeneous Regression M.pdf;/Users/nikos/github-synced/zotero-nikos/storage/A3JXGGCX/mwr-d-17-0364.1.html}
}

@article{gebetsbergerEstimationMethodsNonhomogeneous2018a,
  title = {Estimation {{Methods}} for {{Nonhomogeneous Regression Models}}: {{Minimum Continuous Ranked Probability Score}} versus {{Maximum Likelihood}}},
  shorttitle = {Estimation {{Methods}} for {{Nonhomogeneous Regression Models}}},
  author = {Gebetsberger, Manuel and Messner, Jakob W. and Mayr, Georg J. and Zeileis, Achim},
  year = {2018},
  month = dec,
  journal = {Monthly Weather Review},
  volume = {146},
  number = {12},
  pages = {4323--4338},
  publisher = {American Meteorological Society},
  issn = {1520-0493, 0027-0644},
  doi = {10.1175/MWR-D-17-0364.1},
  urldate = {2022-03-22},
  abstract = {Abstract Nonhomogeneous regression models are widely used to statistically postprocess numerical ensemble weather prediction models. Such regression models are capable of forecasting full probability distributions and correcting for ensemble errors in the mean and variance. To estimate the corresponding regression coefficients, minimization of the continuous ranked probability score (CRPS) has widely been used in meteorological postprocessing studies and has often been found to yield more calibrated forecasts compared to maximum likelihood estimation. From a theoretical perspective, both estimators are consistent and should lead to similar results, provided the correct distribution assumption about empirical data. Differences between the estimated values indicate a wrong specification of the regression model. This study compares the two estimators for probabilistic temperature forecasting with nonhomogeneous regression, where results show discrepancies for the classical Gaussian assumption. The heavy-tailed logistic and Student's t distributions can improve forecast performance in terms of sharpness and calibration, and lead to only minor differences between the estimators employed. Finally, a simulation study confirms the importance of appropriate distribution assumptions and shows that for a correctly specified model the maximum likelihood estimator is slightly more efficient than the CRPS estimator.},
  chapter = {Monthly Weather Review},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/5P9W2VL6/Gebetsberger et al. - 2018 - Estimation Methods for Nonhomogeneous Regression M.pdf}
}

@article{gelmanUnderstandingPredictiveInformation2014,
  title = {Understanding Predictive Information Criteria for {{Bayesian}} Models},
  author = {Gelman, Andrew and Hwang, Jessica and Vehtari, Aki},
  year = {2014},
  month = nov,
  journal = {Statistics and Computing},
  volume = {24},
  number = {6},
  pages = {997--1016},
  issn = {1573-1375},
  doi = {10.1007/s11222-013-9416-2},
  urldate = {2019-02-28},
  abstract = {We review the Akaike, deviance, and Watanabe-Akaike information criteria from a Bayesian perspective, where the goal is to estimate expected out-of-sample-prediction error using a bias-corrected adjustment of within-sample error. We focus on the choices involved in setting up these measures, and we compare them in three simple examples, one theoretical and two applied. The contribution of this paper is to put all these information criteria into a Bayesian predictive context and to better understand, through small examples, how these methods can apply in practice.},
  langid = {english},
  keywords = {AIC,Bayes,Cross-validation,DIC,Prediction,WAIC},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/B2VVZDAP/Gelman et al. - 2014 - Understanding predictive information criteria for .pdf}
}

@misc{gerdingEvaluatingInfectiousDisease2024,
  title = {Evaluating Infectious Disease Forecasts with Allocation Scoring Rules},
  author = {Gerding, Aaron and Reich, Nicholas G. and Rogers, Benjamin and Ray, Evan L.},
  year = {2024},
  month = mar,
  number = {arXiv:2312.16201},
  eprint = {2312.16201},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.16201},
  urldate = {2024-03-17},
  abstract = {Recent years have seen increasing efforts to forecast infectious disease burdens, with a primary goal being to help public health workers make informed policy decisions. However, there has only been limited discussion of how predominant forecast evaluation metrics might indicate the success of policies based in part on those forecasts. We explore one possible tether between forecasts and policy: the allocation of limited medical resources so as to minimize unmet need. We use probabilistic forecasts of disease burden in each of several regions to determine optimal resource allocations, and then we score forecasts according to how much unmet need their associated allocations would have allowed. We illustrate with forecasts of COVID-19 hospitalizations in the US, and we find that the forecast skill ranking given by this allocation scoring rule can vary substantially from the ranking given by the weighted interval score. We see this as evidence that the allocation scoring rule detects forecast value that is missed by traditional accuracy measures and that the general strategy of designing scoring rules that are directly linked to policy performance is a promising direction for epidemic forecast evaluation.},
  archiveprefix = {arxiv},
  keywords = {Statistics - Applications},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/B7FEPM39/Gerding et al. - 2024 - Evaluating infectious disease forecasts with alloc.pdf;/Users/nikos/github-synced/zotero-nikos/storage/H428KTW3/2312.html}
}

@article{glahnUseModelOutput1972,
  title = {The {{Use}} of {{Model Output Statistics}} ({{MOS}}) in {{Objective Weather Forecasting}}},
  author = {Glahn, Harry R. and Lowry, Dale A.},
  year = {1972},
  month = dec,
  journal = {Journal of Applied Meteorology and Climatology},
  volume = {11},
  number = {8},
  pages = {1203--1211},
  publisher = {American Meteorological Society},
  issn = {1520-0450},
  doi = {10.1175/1520-0450(1972)011<1203:TUOMOS>2.0.CO;2},
  urldate = {2021-10-20},
  abstract = {{$<$}section class="abstract"{$><$}h2 class="abstractTitle text-title my-1" id="d10835514e59"{$>$}Abstract{$<$}/h2{$><$}p{$>$}Model Output Statistics (MOS) is an objective weather forecasting technique which consists of determining a statistical relationship between a predictand and variables forecast by a numerical model at some projection time(s). It is, in effect, the determination of the ``weather related'' statistics of a numerical model. This technique, together with screening regression, has been applied to the prediction of surface wind, probability of precipitation, maximum temperature, cloud amount, and conditional probability of frozen precipitation. Predictors used include surface observations at initial time and predictions from the Subsynoptic Advection Model (SAM) and the Primitive Equation model used operationally by the National Weather Service. Verification scores have been computed, and, where possible, compared to scores for forecasts from other objective techniques and for the official forecasts. MOS forecasts of surface wind, probability of precipitation, and conditional probability of frozen precipitation are being disseminated by the National Weather Service over teletype and facsimile. It is concluded that MOS is a useful technique in objective weather forecasting.{$<$}/p{$><$}/section{$>$}},
  chapter = {Journal of Applied Meteorology and Climatology},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/GNN64PKJ/Glahn and Lowry - 1972 - The Use of Model Output Statistics (MOS) in Object.pdf;/Users/nikos/github-synced/zotero-nikos/storage/TZJXSISZ/1520-0450_1972_011_1203_tuomos_2_0_co_2.html}
}

@misc{glownyurzadstatystycznyLudnoscStanStruktura2020,
  title = {{Ludno{\'s}{\'c}. Stan i struktura ludno{\'s}ci oraz ruch naturalny w przekroju terytorialnym (stan w dniu 30.06.2020)}},
  author = {G{\l}{\'o}wny Urz{\k a}d Statystyczny},
  year = {2020},
  month = jun,
  journal = {stat.gov.pl},
  urldate = {2021-06-10},
  abstract = {G{\l}{\'o}wny Urz{\k a}d Statystyczny - Portal Statystyki Publicznej},
  howpublished = {https://stat.gov.pl/obszary-tematyczne/ludnosc/ludnosc/ludnosc-stan-i-struktura-ludnosci-oraz-ruch-naturalny-w-przekroju-terytorialnym-stan-w-dniu-30-06-2020,6,28.html},
  langid = {polish},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/IDHSTLBK/ludnosc-stan-i-struktura-ludnosci-oraz-ruch-naturalny-w-przekroju-terytorialnym-stan-w-dniu-30-.html}
}

@article{gneitingCalibratedProbabilisticForecasting2005,
  title = {Calibrated {{Probabilistic Forecasting Using Ensemble Model Output Statistics}} and {{Minimum CRPS Estimation}}},
  author = {Gneiting, Tilmann and Raftery, Adrian E. and Westveld, Anton H. and Goldman, Tom},
  year = {2005},
  month = may,
  journal = {Monthly Weather Review},
  volume = {133},
  number = {5},
  pages = {1098--1118},
  publisher = {American Meteorological Society},
  issn = {0027-0644},
  doi = {10.1175/MWR2904.1},
  urldate = {2020-08-12},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/DWIEDUKB/Gneiting et al. - 2005 - Calibrated Probabilistic Forecasting Using Ensembl.pdf;/Users/nikos/github-synced/zotero-nikos/storage/UKQZH4WN/Calibrated-Probabilistic-Forecasting-Using.html}
}

@article{gneitingCombiningPredictiveDistributions2013,
  title = {Combining Predictive Distributions},
  author = {Gneiting, Tilmann and Ranjan, Roopesh},
  year = {2013},
  month = jan,
  journal = {Electronic Journal of Statistics},
  volume = {7},
  number = {none},
  issn = {1935-7524},
  doi = {10.1214/13-EJS823},
  urldate = {2022-11-10},
  abstract = {In probabilistic forecasting combination formulas for the aggregation of predictive distributions need to be estimated based on past experience and training data. We study combination formulas and aggregation methods for predictive cumulative distribution functions from the perspectives of calibration and dispersion, taking an original prediction space approach that applies to discrete, mixed discrete-continuous and continuous predictive distributions alike. The key idea is that aggregation methods ought to be parsimonious, yet sufficiently flexible to accommodate any type of dispersion in the component distributions. Both linear and non-linear aggregation methods are investigated, including generalized, spread-adjusted and beta-transformed linear pools. The effects and techniques are demonstrated theoretically, in simulation examples, and in case studies, where we fit combination formulas for density forecasts of S\&P 500 returns and daily maximum temperature at Seattle-Tacoma Airport.},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/KBVXU9QE/Gneiting and Ranjan - 2013 - Combining predictive distributions.pdf}
}

@article{gneitingMakingEvaluatingPoint2011,
  title = {Making and {{Evaluating Point Forecasts}}},
  author = {Gneiting, Tilmann},
  year = {2011},
  month = jun,
  journal = {Journal of the American Statistical Association},
  volume = {106},
  number = {494},
  pages = {746--762},
  publisher = {Taylor \& Francis},
  issn = {0162-1459},
  doi = {10.1198/jasa.2011.r10138},
  urldate = {2021-10-18},
  abstract = {Typically, point forecasting methods are compared and assessed by means of an error measure or scoring function, with the absolute error and the squared error being key examples. The individual scores are averaged over forecast cases, to result in a summary measure of the predictive performance, such as the mean absolute error or the mean squared error. I demonstrate that this common practice can lead to grossly misguided inferences, unless the scoring function and the forecasting task are carefully matched. Effective point forecasting requires that the scoring function be specified ex ante, or that the forecaster receives a directive in the form of a statistical functional, such as the mean or a quantile of the predictive distribution. If the scoring function is specified ex ante, the forecaster can issue the optimal point forecast, namely, the Bayes rule. If the forecaster receives a directive in the form of a functional, it is critical that the scoring function be consistent for it, in the sense that the expected score is minimized when following the directive. A functional is elicitable if there exists a scoring function that is strictly consistent for it. Expectations, ratios of expectations and quantiles are elicitable. For example, a scoring function is consistent for the mean functional if and only if it is a Bregman function. It is consistent for a quantile if and only if it is generalized piecewise linear. Similar characterizations apply to ratios of expectations and to expectiles. Weighted scoring functions are consistent for functionals that adapt to the weighting in peculiar ways. Not all functionals are elicitable; for instance, conditional value-at-risk is not, despite its popularity in quantitative finance.},
  keywords = {Bayes rule,Bregman function,Conditional value-at-risk (CVaR),Decision theory,Elicitability,Expectile,Mean,Median,Mode,Proper scoring rule,Quantile,Statistical functional},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/JZGDGGB9/Gneiting - 2011 - Making and Evaluating Point Forecasts.pdf;/Users/nikos/github-synced/zotero-nikos/storage/5CBRZY4U/jasa.2011.html}
}

@article{gneitingProbabilisticForecastsCalibration2007,
  title = {Probabilistic Forecasts, Calibration and Sharpness},
  author = {Gneiting, Tilmann and Balabdaoui, Fadoua and Raftery, Adrian E.},
  year = {2007},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {69},
  number = {2},
  pages = {243--268},
  issn = {1467-9868},
  doi = {10.1111/j.1467-9868.2007.00587.x},
  urldate = {2020-02-17},
  abstract = {Summary. Probabilistic forecasts of continuous variables take the form of predictive densities or predictive cumulative distribution functions. We propose a diagnostic approach to the evaluation of predictive performance that is based on the paradigm of maximizing the sharpness of the predictive distributions subject to calibration. Calibration refers to the statistical consistency between the distributional forecasts and the observations and is a joint property of the predictions and the events that materialize. Sharpness refers to the concentration of the predictive distributions and is a property of the forecasts only. A simple theoretical framework allows us to distinguish between probabilistic calibration, exceedance calibration and marginal calibration. We propose and study tools for checking calibration and sharpness, among them the probability integral transform histogram, marginal calibration plots, the sharpness diagram and proper scoring rules. The diagnostic approach is illustrated by an assessment and ranking of probabilistic forecasts of wind speed at the Stateline wind energy centre in the US Pacific Northwest. In combination with cross-validation or in the time series context, our proposal provides very general, nonparametric alternatives to the use of information criteria for model diagnostics and model selection.},
  langid = {english},
  keywords = {Cross-validation,Density forecast,Ensemble prediction system,Ex post evaluation,Forecast verification,Model diagnostics,Posterior predictive assessment,Predictive distribution,Prequential principle,Probability integral transform,Proper scoring rule},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/BUWD6CGT/Gneiting et al. - 2007 - Probabilistic forecasts, calibration and sharpness.pdf;/Users/nikos/github-synced/zotero-nikos/storage/EUCMSBKN/j.1467-9868.2007.00587.html}
}

@article{gneitingStrictlyProperScoring2007,
  title = {Strictly {{Proper Scoring Rules}}, {{Prediction}}, and {{Estimation}}},
  author = {Gneiting, Tilmann and Raftery, Adrian E},
  year = {2007},
  month = mar,
  journal = {Journal of the American Statistical Association},
  volume = {102},
  number = {477},
  pages = {359--378},
  issn = {0162-1459, 1537-274X},
  doi = {10.1198/016214506000001437},
  urldate = {2020-03-22},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/P599P5ZY/Gneiting and Raftery - 2007 - Strictly Proper Scoring Rules, Prediction, and Est.pdf}
}

@article{gneitingWeatherForecastingEnsemble2005,
  title = {Weather {{Forecasting}} with {{Ensemble Methods}}},
  author = {Gneiting, Tilmann and Raftery, Adrian E.},
  year = {2005},
  month = oct,
  journal = {Science},
  volume = {310},
  number = {5746},
  pages = {248--249},
  publisher = {American Association for the Advancement of Science},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1115255},
  urldate = {2021-05-30},
  abstract = {{$<$}p{$>$} Traditional weather forecasting has been built on a foundation of deterministic modeling--start with initial conditions, put them into a supercomputer model, and end up with a prediction about future weather. But as Gneiting and Raftery discuss in their Perspective, a new approach--ensemble forecasting--was introduced in the early 1990s. In this method, up to 100 different computer runs, each with slightly different starting conditions or model assumptions, are combined into a weather forecast. In concert with statistical techniques, ensembles can provide accurate statements about the uncertainty in daily and seasonal forecasting. The challenge now is to improve the modeling, statistical analysis, and visualization technologies for disseminating the ensemble results. {$<$}/p{$>$}},
  chapter = {Perspective},
  copyright = {{\copyright} 2005 American Association for the Advancement of Science},
  langid = {english},
  pmid = {16224011},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/VRJMN77J/Gneiting and Raftery - 2005 - Weather Forecasting with Ensemble Methods.pdf;/Users/nikos/github-synced/zotero-nikos/storage/8Q5UA2FU/248.html}
}

@article{goodRationalDecisions1952,
  title = {Rational {{Decisions}}},
  author = {Good, I. J.},
  year = {1952},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {14},
  number = {1},
  eprint = {2984087},
  eprinttype = {jstor},
  pages = {107--114},
  publisher = {[Royal Statistical Society, Wiley]},
  issn = {0035-9246},
  urldate = {2020-08-13},
  abstract = {This paper deals first with the relationship between the theory of probability and the theory of rational behaviour. A method is then suggested for encouraging people to make accurate probability estimates, a connection with the theory of information being mentioned. Finally Wald's theory of statistical decision functions is summarised and generalised and its relation to the theory of rational behaviour is discussed.},
  keywords = {Log Score,LogS},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/23458422/2020 - Rational Decisions.pdf}
}

@article{gordonAreReplicationRates2020,
  title = {Are Replication Rates the Same across Academic Fields? {{Community}} Forecasts from the {{DARPA SCORE}} Programme},
  shorttitle = {Are Replication Rates the Same across Academic Fields?},
  author = {Gordon, Michael and Viganola, Domenico and Bishop, Michael and Chen, Yiling and Dreber, Anna and Goldfedder, Brandon and Holzmeister, Felix and Johannesson, Magnus and Liu, Yang and Twardy, Charles and Wang, Juntao and Pfeiffer, Thomas},
  year = {2020},
  month = jul,
  journal = {Royal Society Open Science},
  volume = {7},
  number = {7},
  pages = {200566},
  issn = {2054-5703},
  doi = {10.1098/rsos.200566},
  urldate = {2023-02-22},
  abstract = {The Defense Advanced Research Projects Agency (DARPA) programme `Systematizing Confidence in Open Research and Evidence' (SCORE) aims to generate confidence scores for a large number of research claims from empirical studies in the social and behavioural sciences. The confidence scores will provide a quantitative assessment of how likely a claim will hold up in an independent replication. To create the scores, we follow earlier approaches and use prediction markets and surveys to forecast replication outcomes. Based on an initial set of forecasts for the overall replication rate in SCORE and its dependence on the academic discipline and the time of publication, we show that participants expect replication rates to increase over time. Moreover, they expect replication rates to differ between fields, with the highest replication rate in economics (average survey response 58\%), and the lowest in psychology and in education (average survey response of 42\% for both fields). These results reveal insights into the academic community's views of the replication crisis, including for research fields for which no large-scale replication studies have been undertaken yet.},
  pmcid = {PMC7428244},
  pmid = {32874648},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/WATRDBVL/Gordon et al. - 2020 - Are replication rates the same across academic fie.pdf}
}

@article{gosticPracticalConsiderationsMeasuring2020,
  title = {Practical Considerations for Measuring the Effective Reproductive Number, {{Rt}}},
  author = {Gostic, Katelyn M. and McGough, Lauren and Baskerville, Ed and Abbott, Sam and Joshi, Keya and Tedijanto, Christine and Kahn, Rebecca and Niehus, Rene and Hay, James and {de Salazar}, Pablo and Hellewell, Joel and Meakin, Sophie and Munday, James and Bosse, Nikos I. and Sherrat, Katharine and Thompson, Robin N. and White, Laura F. and Huisman, Jana S. and Scire, J{\'e}r{\'e}mie and Bonhoeffer, Sebastian and Stadler, Tanja and Wallinga, Jacco and Funk, Sebastian and Lipsitch, Marc and Cobey, Sarah},
  year = {2020},
  month = jun,
  journal = {medRxiv},
  doi = {10.1101/2020.06.18.20134858},
  urldate = {2020-08-12},
  abstract = {Estimation of the effective reproductive number, Rt, is important for detecting changes in disease transmission over time. During the COVID-19 pandemic, policymakers and public health officials are using Rt to assess the effectiveness of interventions and to inform policy. However, estimation of Rt from available data presents several challenges, with critical implications for the interpretation of the course of the pandemic. The purpose of this document is to summarize these challenges, illustrate them with examples from synthetic data, and, where possible, make methodological recommendations. For near real-time estimation of Rt, we recommend the approach of Cori et al. (2013), which uses data from before time t and empirical estimates of the distribution of time between infections. Methods that require data from after time t, such as Wallinga and Teunis (2004), are conceptually and methodologically less suited for near real-time estimation, but may be appropriate for some retrospective analyses. We advise against using methods derived from Bettencourt and Ribeiro (2008), as the resulting Rt estimates may be biased if the underlying structural assumptions are not met. A challenge common to all approaches is reconstruction of the time series of new infections from observations occurring long after the moment of transmission. Naive approaches for dealing with observation delays, such as subtracting delays sampled from a distribution, can introduce bias. We provide suggestions for how to mitigate this and other technical challenges and highlight open problems in Rt estimation.},
  pmcid = {PMC7325187},
  pmid = {32607522},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/2DQ39CTT/Gostic et al. - 2020 - Practical considerations for measuring the effecti.pdf}
}

@article{graefeCombiningForecastsApplication2014,
  title = {Combining Forecasts: {{An}} Application to Elections},
  shorttitle = {Combining Forecasts},
  author = {Graefe, Andreas and Armstrong, J. Scott and Jones, Randall J. and Cuz{\'a}n, Alfred G.},
  year = {2014},
  month = jan,
  journal = {International Journal of Forecasting},
  volume = {30},
  number = {1},
  pages = {43--54},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2013.02.005},
  urldate = {2021-07-07},
  abstract = {We summarize the literature on the effectiveness of combining forecasts by assessing the conditions under which combining is most valuable. Using data on the six US presidential elections from 1992 to 2012, we report the reductions in error obtained by averaging forecasts within and across four election forecasting methods: poll projections, expert judgment, quantitative models, and the Iowa Electronic Markets. Across the six elections, the resulting combined forecasts were more accurate than any individual component method, on average. The gains in accuracy from combining increased with the numbers of forecasts used, especially when these forecasts were based on different methods and different data, and in situations involving high levels of uncertainty. Such combining yielded error reductions of between 16\% and 59\%, compared to the average errors of the individual forecasts. This improvement is substantially greater than the 12\% reduction in error that had been reported previously for combining forecasts.},
  langid = {english},
  keywords = {Combining,Econometric models,Election forecasting,Expert judgment,Polls,Prediction markets},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/RV4HB4EK/S0169207013000423.html}
}

@article{graefeCombiningForecastsApplication2014a,
  title = {Combining Forecasts: {{An}} Application to Elections},
  shorttitle = {Combining Forecasts},
  author = {Graefe, Andreas and Armstrong, J. Scott and Jones, Randall J. and Cuz{\'a}n, Alfred G.},
  year = {2014},
  month = jan,
  journal = {International Journal of Forecasting},
  volume = {30},
  number = {1},
  pages = {43--54},
  issn = {01692070},
  doi = {10.1016/j.ijforecast.2013.02.005},
  urldate = {2021-07-07},
  abstract = {We summarize the literature on the effectiveness of combining forecasts by assessing the conditions under which combining is most valuable. Using data on the six US presidential elections from 1992 to 2012, we report the reductions in error obtained by averaging forecasts within and across four election forecasting methods: poll projections, expert judgment, quantitative models, and the Iowa Electronic Markets. Across the six elections, the resulting combined forecasts were more accurate than any individual component method, on average. The gains in accuracy from combining increased with the numbers of forecasts used, especially when these forecasts were based on different methods and different data, and in situations involving high levels of uncertainty. Such combining yielded error reductions of between 16\% and 59\%, compared to the average errors of the individual forecasts. This improvement is substantially greater than the 12\% reduction in error that had been reported previously for combining forecasts.},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/T4B8B8EI/Graefe et al. - 2014 - Combining forecasts An application to elections.pdf}
}

@article{graefeCombiningForecastsApplication2014b,
  title = {Combining {{Forecasts}}: {{An Application}} to {{Elections}}},
  shorttitle = {Combining {{Forecasts}}},
  author = {Graefe, Andreas and Armstrong, J. and Jones, Randall and Cuz{\'a}n, Alfred},
  year = {2014},
  month = mar,
  journal = {International Journal of Forecasting},
  volume = {30},
  pages = {43--54},
  doi = {10.1016/j.ijforecast.2013.02.005},
  abstract = {We summarize the literature on the effectiveness of combining forecasts by assessing the conditions under which combining is most valuable. Using data on the six US presidential elections from 1992 to 2012, we report the reductions in error obtained by averaging forecasts within and across four election forecasting methods: poll projections, expert judgment, quantitative models, and the Iowa Electronic Markets. Across the six elections, the resulting combined forecasts were more accurate than any individual component method, on average. The gains in accuracy from combining increased with the numbers of forecasts used, especially when these forecasts were based on different methods and different data, and in situations involving high levels of uncertainty. Such combining yielded error reductions of between 16\% and 59\%, compared to the average errors of the individual forecasts. This improvement is substantially greater than the 12\% reduction in error that had been reported previously for combining forecasts.},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/2K2CAK26/Graefe et al. - 2014 - Combining Forecasts An Application to Elections.pdf}
}

@article{greenbergCalibrationScoringRules2018,
  title = {Calibration {{Scoring Rules}} for {{Practical Prediction Training}}},
  author = {Greenberg, Spencer},
  year = {2018},
  month = aug,
  journal = {arXiv:1808.07501 [stat]},
  eprint = {1808.07501},
  primaryclass = {stat},
  urldate = {2022-01-22},
  abstract = {In situations where forecasters are scored on the quality of their probabilistic predictions, it is standard to use `proper' scoring rules to perform such scoring. These rules are desirable because they give forecasters no incentive to lie about their probabilistic beliefs. However, in the real world context of creating a training program designed to help people improve calibration through prediction practice, there are a variety of desirable traits for scoring rules that go beyond properness. These potentially may have a substantial impact on the user experience, usability of the program, or efficiency of learning. The space of proper scoring rules is too broad, in the sense that most proper scoring rules lack these other desirable properties. On the other hand, the space of proper scoring rules is potentially also too narrow, in the sense that we may sometimes choose to give up properness when it conflicts with other properties that are even more desirable from the point of view of usability and effective training. We introduce a class of scoring rules that we call `Practical' scoring rules, designed to be intuitive to users in the context of `right' vs. `wrong' probabilistic predictions. We also introduce two specific scoring rules for prediction intervals, the `Distance' and `Order of magnitude' rules. These rules are designed to satisfy a variety of properties that, based on user testing, we believe are desirable for applied calibration training.},
  archiveprefix = {arxiv},
  keywords = {Statistics - Methodology},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/EN9TEH26/Greenberg - 2018 - Calibration Scoring Rules for Practical Prediction.pdf;/Users/nikos/github-synced/zotero-nikos/storage/FHSCYEBZ/1808.html}
}

@article{groveClinicalMechanicalPrediction20000401,
  title = {Clinical versus Mechanical Prediction: {{A}} Meta-Analysis.},
  shorttitle = {Clinical versus Mechanical Prediction},
  author = {Grove, William M. and Zald, David H. and Lebow, Boyd S. and Snitz, Beth E. and Nelson, Chad},
  year = {20000401},
  journal = {Psychological Assessment},
  volume = {12},
  number = {1},
  pages = {19},
  publisher = {US: American Psychological Association},
  issn = {1939-134X},
  doi = {10.1037/1040-3590.12.1.19},
  urldate = {2021-07-29},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/N4NYA8PZ/Grove et al. - Clinical versus mechanical prediction A meta-anal.pdf;/Users/nikos/github-synced/zotero-nikos/storage/EUTZJ3W5/2000-07311-003.html}
}

@article{guerreroTimeseriesAnalysisSupported1993,
  title = {Time-Series Analysis Supported by Power Transformations},
  author = {Guerrero, Victor M.},
  year = {1993},
  month = jan,
  journal = {Journal of Forecasting},
  volume = {12},
  number = {1},
  pages = {37--48},
  issn = {02776693, 1099131X},
  doi = {10.1002/for.3980120104},
  urldate = {2022-12-07},
  abstract = {This paper presents some procedures aimed at helping an applied timeseries analyst in the use of power transformations. Two methods are proposed for selecting a variance-stabilizing transformation and another for bias-reduction of the forecast in the original scale. Since these methods are essentially model-independent, they can be employed with practically any type of time-series model. Some comparisons are made with other methods currently available and it is shown that those proposed here are either easier to apply or are more general, with a performance similar to or better than other competing procedures.},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/MYBS9BRA/Guerrero - 1993 - Time-series analysis supported by power transforma.pdf}
}

@article{hadleyChallengesInteractionModels2021,
  title = {Challenges on the Interaction of Models and Policy for Pandemic Control},
  author = {Hadley, Liza and Challenor, Peter and Dent, Chris and Isham, Valerie and Mollison, Denis and Robertson, Duncan A. and Swallow, Ben and Webb, Cerian R.},
  year = {2021},
  month = dec,
  journal = {Epidemics},
  volume = {37},
  pages = {100499},
  issn = {1755-4365},
  doi = {10.1016/j.epidem.2021.100499},
  urldate = {2021-11-18},
  abstract = {The COVID-19 pandemic has seen infectious disease modelling at the forefront of government decision-making. Models have been widely used throughout the pandemic to estimate pathogen spread and explore the potential impact of different intervention strategies. Infectious disease modellers and policymakers have worked effectively together, but there are many avenues for progress on this interface. In this paper, we identify and discuss seven broad challenges on the interaction of models and policy for pandemic control. We then conclude with suggestions and recommendations for the future.},
  langid = {english},
  keywords = {Communication,Cooperation,Modelling,Pandemic,Policy},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/WI3SQ2LX/Hadley et al. - 2021 - Challenges on the interaction of models and policy.pdf}
}

@article{hamillInterpretationRankHistograms2001a,
  title = {Interpretation of {{Rank Histograms}} for {{Verifying Ensemble Forecasts}}},
  author = {Hamill, Thomas M.},
  year = {2001},
  month = mar,
  journal = {Monthly Weather Review},
  volume = {129},
  number = {3},
  pages = {550--560},
  publisher = {American Meteorological Society},
  issn = {1520-0493, 0027-0644},
  doi = {10.1175/1520-0493(2001)129<0550:IORHFV>2.0.CO;2},
  urldate = {2022-01-21},
  abstract = {Abstract Rank histograms are a tool for evaluating ensemble forecasts. They are useful for determining the reliability of ensemble forecasts and for diagnosing errors in its mean and spread. Rank histograms are generated by repeatedly tallying the rank of the verification (usually an observation) relative to values from an ensemble sorted from lowest to highest. However, an uncritical use of the rank histogram can lead to misinterpretations of the qualities of that ensemble. For example, a flat rank histogram, usually taken as a sign of reliability, can still be generated from unreliable ensembles. Similarly, a U-shaped rank histogram, commonly understood as indicating a lack of variability in the ensemble, can also be a sign of conditional bias. It is also shown that flat rank histograms can be generated for some model variables if the variance of the ensemble is correctly specified, yet if covariances between model grid points are improperly specified, rank histograms for combinations of model variables may not be flat. Further, if imperfect observations are used for verification, the observational errors should be accounted for, otherwise the shape of the rank histogram may mislead the user about the characteristics of the ensemble. If a statistical hypothesis test is to be performed to determine whether the differences from uniformity of rank are statistically significant, then samples used to populate the rank histogram must be located far enough away from each other in time and space to be considered independent.},
  chapter = {Monthly Weather Review},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/FJYU9QZH/Hamill - 2001 - Interpretation of Rank Histograms for Verifying En.pdf;/Users/nikos/github-synced/zotero-nikos/storage/SH65U38N/1520-0493_2001_129_0550_iorhfv_2.0.co_2.html}
}

@article{heldProbabilisticForecastingInfectious2017,
  title = {Probabilistic Forecasting in Infectious Disease Epidemiology: The 13th {{Armitage}} Lecture},
  shorttitle = {Probabilistic Forecasting in Infectious Disease Epidemiology},
  author = {Held, Leonhard and Meyer, Sebastian and Bracher, Johannes},
  year = {2017},
  journal = {Statistics in Medicine},
  volume = {36},
  number = {22},
  pages = {3443--3460},
  issn = {1097-0258},
  doi = {10.1002/sim.7363},
  urldate = {2019-09-16},
  abstract = {Routine surveillance of notifiable infectious diseases gives rise to daily or weekly counts of reported cases stratified by region and age group. From a public health perspective, forecasts of infectious disease spread are of central importance. We argue that such forecasts need to properly incorporate the attached uncertainty, so they should be probabilistic in nature. However, forecasts also need to take into account temporal dependencies inherent to communicable diseases, spatial dynamics through human travel and social contact patterns between age groups. We describe a multivariate time series model for weekly surveillance counts on norovirus gastroenteritis from the 12 city districts of Berlin, in six age groups, from week 2011/27 to week 2015/26. The following year (2015/27 to 2016/26) is used to assess the quality of the predictions. Probabilistic forecasts of the total number of cases can be derived through Monte Carlo simulation, but first and second moments are also available analytically. Final size forecasts as well as multivariate forecasts of the total number of cases by age group, by district and by week are compared across different models of varying complexity. This leads to a more general discussion of issues regarding modelling, prediction and evaluation of public health surveillance data. Copyright {\copyright} 2017 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2017 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {age-structured contact matrix,endemic-epidemic modelling,multivariate probabilistic forecasting,proper scoring rules,spatio-temporal surveillance data},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/DIJH7TNP/Held et al. - 2017 - Probabilistic forecasting in infectious disease ep.pdf}
}

@article{heldValuesBayesFactors2018,
  title = {On {\emph{p}} -{{Values}} and {{Bayes Factors}}},
  author = {Held, Leonhard and Ott, Manuela},
  year = {2018},
  month = mar,
  journal = {Annual Review of Statistics and Its Application},
  volume = {5},
  number = {1},
  pages = {393--419},
  issn = {2326-8298, 2326-831X},
  doi = {10.1146/annurev-statistics-031017-100307},
  urldate = {2021-11-07},
  abstract = {The p-value quantifies the discrepancy between the data and a null hypothesis of interest, usually the assumption of no difference or no effect. A Bayesian approach allows the calibration of p-values by transforming them to direct measures of the evidence against the null hypothesis, so-called Bayes factors. We review the available literature in this area and consider two-sided significance tests for a point null hypothesis in more detail. We distinguish simple from local alternative hypotheses and contrast traditional Bayes factors based on the data with Bayes factors based on p-values or test statistics. A well-known finding is that the minimum Bayes factor, the smallest possible Bayes factor within a certain class of alternative hypotheses, provides less evidence against the null hypothesis than the corresponding p-value might suggest. It is less known that the relationship between p-values and minimum Bayes factors also depends on the sample size and on the dimension of the parameter of interest. We illustrate the transformation of p-values to minimum Bayes factors with two examples from clinical research.},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/U4XJRPGM/Held and Ott - 2018 - On p -Values and Bayes Factors.pdf}
}

@article{hersbachDecompositionContinuousRanked2000a,
  title = {Decomposition of the {{Continuous Ranked Probability Score}} for {{Ensemble Prediction Systems}}},
  author = {Hersbach, Hans},
  year = {2000},
  month = oct,
  journal = {Weather and Forecasting},
  volume = {15},
  number = {5},
  pages = {559--570},
  publisher = {American Meteorological Society},
  issn = {1520-0434, 0882-8156},
  doi = {10.1175/1520-0434(2000)015<0559:DOTCRP>2.0.CO;2},
  urldate = {2022-03-14},
  abstract = {Abstract Some time ago, the continuous ranked probability score (CRPS) was proposed as a new verification tool for (probabilistic) forecast systems. Its focus is on the entire permissible range of a certain (weather) parameter. The CRPS can be seen as a ranked probability score with an infinite number of classes, each of zero width. Alternatively, it can be interpreted as the integral of the Brier score over all possible threshold values for the parameter under consideration. For a deterministic forecast system the CRPS reduces to the mean absolute error. In this paper it is shown that for an ensemble prediction system the CRPS can be decomposed into a reliability part and a resolution/uncertainty part, in a way that is similar to the decomposition of the Brier score. The reliability part of the CRPS is closely connected to the rank histogram of the ensemble, while the resolution/uncertainty part can be related to the average spread within the ensemble and the behavior of its outliers. The usefulness of such a decomposition is illustrated for the ensemble prediction system running at the European Centre for Medium-Range Weather Forecasts. The evaluation of the CRPS and its decomposition proposed in this paper can be extended to systems issuing continuous probability forecasts, by realizing that these can be interpreted as the limit of ensemble forecasts with an infinite number of members.},
  chapter = {Weather and Forecasting},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/8C93QRYF/Hersbach - 2000 - Decomposition of the Continuous Ranked Probability.pdf;/Users/nikos/github-synced/zotero-nikos/storage/7PARNA8T/1520-0434_2000_015_0559_dotcrp_2_0_co_2.html}
}

@article{hinkleyBootstrapMoreStab1994,
  title = {[{{Bootstrap}}: {{More}} than a {{Stab}} in the {{Dark}}?]: {{Comment}}},
  shorttitle = {[{{Bootstrap}}},
  author = {Hinkley, David},
  year = {1994},
  month = aug,
  journal = {Statistical Science},
  volume = {9},
  number = {3},
  pages = {400--403},
  publisher = {Institute of Mathematical Statistics},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/ss/1177010387},
  urldate = {2023-10-03},
  abstract = {Statistical Science},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/99Q6QXV8/Hinkley - 1994 - [Bootstrap More than a Stab in the Dark] Commen.pdf}
}

@article{hoogeveenLaypeopleCanPredict2020,
  title = {Laypeople {{Can Predict Which Social-Science Studies Will Be Replicated Successfully}}},
  author = {Hoogeveen, Suzanne and Sarafoglou, Alexandra and Wagenmakers, Eric-Jan},
  year = {2020},
  month = sep,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {3},
  number = {3},
  pages = {267--285},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/2515245920919667},
  urldate = {2021-10-13},
  abstract = {Large-scale collaborative projects recently demonstrated that several key findings from the social-science literature could not be replicated successfully. Here, we assess the extent to which a finding's replication success relates to its intuitive plausibility. Each of 27 high-profile social-science findings was evaluated by 233 people without a Ph.D. in psychology. Results showed that these laypeople predicted replication success with above-chance accuracy (i.e., 59\%). In addition, when participants were informed about the strength of evidence from the original studies, this boosted their prediction performance to 67\%. We discuss the prediction patterns and apply signal detection theory to disentangle detection ability from response bias. Our study suggests that laypeople's predictions contain useful information for assessing the probability that a given finding will be replicated successfully.},
  langid = {english},
  keywords = {meta-science,open data,open materials,open science,prediction survey,preregistered,replication crisis},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/4E5N33J5/Hoogeveen et al. - 2020 - Laypeople Can Predict Which Social-Science Studies.pdf}
}

@misc{https://epiforecasts.io/scoringutilsUtilitiesScoringAssessing,
  title = {Utilities for {{Scoring}} and {{Assessing Predictions}}},
  author = {{https://epiforecasts.io/scoringutils}},
  urldate = {2024-02-27},
  abstract = {Provides a collection of metrics and proper scoring rules (Tilmann Gneiting \& Adrian E Raftery (2007) {$<$}doi:10.1198/016214506000001437{$>$}, Jordan, A., Kr{\"u}ger, F., \& Lerch, S. (2019) {$<$}doi:10.18637/jss.v090.i12{$>$}) within a consistent framework for evaluation, comparison and visualisation of forecasts. In addition to proper scoring rules, functions are provided to assess bias, sharpness and calibration (Sebastian Funk, Anton Camacho, Adam J. Kucharski, Rachel Lowe, Rosalind M. Eggo, W. John Edmunds (2019) {$<$}doi:10.1371/journal.pcbi.1006785{$>$}) of forecasts. Several types of predictions (e.g. binary, discrete, continuous) which may come in different formats (e.g. forecasts represented by predictive samples or by quantiles of the predictive distribution) can be evaluated. Scoring metrics can be used either through a convenient data.frame format, or can be applied as individual functions in a vector / matrix format. All functionality has been implemented with a focus on performance and is robustly tested. Find more information about the package in the accompanying paper ({$<$}doi:10.48550/arXiv.2205.07090{$>$}).},
  howpublished = {https://epiforecasts.io/scoringutils/},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/FZIQXXJW/scoringutils.html}
}

@article{hunterTaxonomyAgentBasedModels2017,
  title = {A {{Taxonomy}} for {{Agent-Based Models}} in {{Human Infectious Disease Epidemiology}}},
  author = {Hunter, Elizabeth and Mac Namee, Brian and Kelleher, John D.},
  year = {2017},
  journal = {Journal of Artificial Societies and Social Simulation},
  volume = {20},
  number = {3},
  pages = {2},
  issn = {1460-7425},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/YBU8LQUK/2.html}
}

@book{hyndmanrobjForecastingPrinciplesPractice2021,
  title = {Forecasting: {{Principles}} and {{Practice}}},
  shorttitle = {Forecasting},
  author = {{Hyndman, Rob J} and {Athanasopoulos, George}},
  year = {2021},
  urldate = {2024-06-15},
  abstract = {3rd edition},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/ZWX4B54Y/fpp3.html}
}

@misc{hypermindHypermindSupercollectiveIntelligence2021,
  title = {Hypermind {\textbar} {{Supercollective}} Intelligence for Decision Makers},
  author = {{Hypermind}},
  year = {2021},
  journal = {Hypermind},
  urldate = {2021-10-13},
  abstract = {Use the proven power of prediction markets to improve your company's performance through better forecasting and more creative innovation},
  howpublished = {https://www.hypermind.com/en/},
  langid = {american},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/CQHU7AHM/en.html}
}

@article{Johansson2019,
  title = {An Open Challenge to Advance Probabilistic Forecasting for Dengue Epidemics},
  author = {Johansson, Michael A. and Apfeldorf, Karyn M. and Dobson, Scott and Devita, Jason and Buczak, Anna L. and Baugher, Benjamin and Moniz, Linda J. and Bagley, Thomas and Babin, Steven M. and Guven, Erhan and Yamana, Teresa K. and Shaman, Jeffrey and Moschou, Terry and Lothian, Nick and Lane, Aaron and Osborne, Grant and Jiang, Gao and Brooks, Logan C. and Farrow, David C. and Hyun, Sangwon and Tibshirani, Ryan J. and Rosenfeld, Roni and Lessler, Justin and Reich, Nicholas G. and Cummings, Derek A. T. and Lauer, Stephen A. and Moore, Sean M. and Clapham, Hannah E. and Lowe, Rachel and Bailey, Trevor C. and {Garc{\'i}a-D{\'i}ez}, Markel and Carvalho, Marilia S{\'a} and Rod{\'o}, Xavier and Sardar, Tridip and Paul, Richard and Ray, Evan L. and Sakrejda, Krzysztof and Brown, Alexandria C. and Meng, Xi and Osoba, Osonde and Vardavas, Raffaele and Manheim, David and Moore, Melinda and Rao, Dhananjai M. and Porco, Travis C. and Ackley, Sarah and Liu, Fengchen and Worden, Lee and Convertino, Matteo and Liu, Yang and Reddy, Abraham and Ortiz, Eloy and Rivero, Jorge and Brito, Humberto and Juarrero, Alicia and Johnson, Leah R. and Gramacy, Robert B. and Cohen, Jeremy M. and Mordecai, Erin A. and Murdock, Courtney C. and Rohr, Jason R. and Ryan, Sadie J. and {Stewart-Ibarra}, Anna M. and Weikel, Daniel P. and Jutla, Antarpreet and Khan, Rakibul and Poultney, Marissa and Colwell, Rita R. and {Rivera-Garc{\'i}a}, Brenda and Barker, Christopher M. and Bell, Jesse E. and Biggerstaff, Matthew and Swerdlow, David and {Mier-y-Teran-Romero}, Luis and Forshey, Brett M. and Trtanj, Juli and Asher, Jason and Clay, Matt and Margolis, Harold S. and Hebbeler, Andrew M. and George, Dylan and {Jean-Paul Chretien}},
  year = {2019},
  journal = {Proceedings of the National Academy of Sciences},
  volume = {116},
  number = {48},
  eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1909865116},
  pages = {24268--24274},
  doi = {10.1073/pnas.1909865116}
}

@article{johanssonOpenChallengeAdvance2019,
  title = {An Open Challenge to Advance Probabilistic Forecasting for Dengue Epidemics},
  author = {Johansson, Michael A. and Apfeldorf, Karyn M. and Dobson, Scott and Devita, Jason and Buczak, Anna L. and Baugher, Benjamin and Moniz, Linda J. and Bagley, Thomas and Babin, Steven M. and Guven, Erhan and Yamana, Teresa K. and Shaman, Jeffrey and Moschou, Terry and Lothian, Nick and Lane, Aaron and Osborne, Grant and Jiang, Gao and Brooks, Logan C. and Farrow, David C. and Hyun, Sangwon and Tibshirani, Ryan J. and Rosenfeld, Roni and Lessler, Justin and Reich, Nicholas G. and Cummings, Derek A. T. and Lauer, Stephen A. and Moore, Sean M. and Clapham, Hannah E. and Lowe, Rachel and Bailey, Trevor C. and {Garc{\'i}a-D{\'i}ez}, Markel and Carvalho, Marilia S{\'a} and Rod{\'o}, Xavier and Sardar, Tridip and Paul, Richard and Ray, Evan L. and Sakrejda, Krzysztof and Brown, Alexandria C. and Meng, Xi and Osoba, Osonde and Vardavas, Raffaele and Manheim, David and Moore, Melinda and Rao, Dhananjai M. and Porco, Travis C. and Ackley, Sarah and Liu, Fengchen and Worden, Lee and Convertino, Matteo and Liu, Yang and Reddy, Abraham and Ortiz, Eloy and Rivero, Jorge and Brito, Humberto and Juarrero, Alicia and Johnson, Leah R. and Gramacy, Robert B. and Cohen, Jeremy M. and Mordecai, Erin A. and Murdock, Courtney C. and Rohr, Jason R. and Ryan, Sadie J. and {Stewart-Ibarra}, Anna M. and Weikel, Daniel P. and Jutla, Antarpreet and Khan, Rakibul and Poultney, Marissa and Colwell, Rita R. and {Rivera-Garc{\'i}a}, Brenda and Barker, Christopher M. and Bell, Jesse E. and Biggerstaff, Matthew and Swerdlow, David and {Mier-y-Teran-Romero}, Luis and Forshey, Brett M. and Trtanj, Juli and Asher, Jason and Clay, Matt and Margolis, Harold S. and Hebbeler, Andrew M. and George, Dylan and Chretien, Jean-Paul},
  year = {2019},
  month = nov,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {116},
  number = {48},
  pages = {24268--24274},
  publisher = {National Academy of Sciences},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1909865116},
  urldate = {2021-05-30},
  abstract = {A wide range of research has promised new tools for forecasting infectious disease dynamics, but little of that research is currently being applied in practice, because tools do not address key public health needs, do not produce probabilistic forecasts, have not been evaluated on external data, or do not provide sufficient forecast skill to be useful. We developed an open collaborative forecasting challenge to assess probabilistic forecasts for seasonal epidemics of dengue, a major global public health problem. Sixteen teams used a variety of methods and data to generate forecasts for 3 epidemiological targets (peak incidence, the week of the peak, and total incidence) over 8 dengue seasons in Iquitos, Peru and San Juan, Puerto Rico. Forecast skill was highly variable across teams and targets. While numerous forecasts showed high skill for midseason situational awareness, early season skill was low, and skill was generally lowest for high incidence seasons, those for which forecasts would be most valuable. A comparison of modeling approaches revealed that average forecast skill was lower for models including biologically meaningful data and mechanisms and that both multimodel and multiteam ensemble forecasts consistently outperformed individual model forecasts. Leveraging these insights, data, and the forecasting framework will be critical to improve forecast skill and the application of forecasts in real time for epidemic preparedness and response. Moreover, key components of this project---integration with public health needs, a common forecasting framework, shared and standardized data, and open participation---can help advance infectious disease forecasting beyond dengue.},
  chapter = {Biological Sciences},
  copyright = {Copyright {\copyright} 2019 the Author(s). Published by PNAS.. https://creativecommons.org/licenses/by/4.0/This open access article is distributed under Creative Commons Attribution License 4.0 (CC BY).},
  langid = {english},
  pmid = {31712420},
  keywords = {dengue,epidemic,forecast,Peru,Puerto Rico},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/EZME2294/Johansson et al. - 2019 - An open challenge to advance probabilistic forecas.pdf;/Users/nikos/github-synced/zotero-nikos/storage/4QECBFPX/24268.html}
}

@article{jordanEvaluatingProbabilisticForecasts2019,
  title = {Evaluating {{Probabilistic Forecasts}} with {{{\textbf{scoringRules}}}}},
  author = {Jordan, Alexander and Kr{\"u}ger, Fabian and Lerch, Sebastian},
  year = {2019},
  journal = {Journal of Statistical Software},
  volume = {90},
  number = {12},
  issn = {1548-7660},
  doi = {10.18637/jss.v090.i12},
  urldate = {2020-02-13},
  abstract = {Probabilistic forecasts in the form of probability distributions over future events have become popular in several fields including meteorology, hydrology, economics, and demography. In typical applications, many alternative statistical models and data sources can be used to produce probabilistic forecasts. Hence, evaluating and selecting among competing methods is an important task. The scoringRules package for R provides functionality for comparative evaluation of probabilistic models based on proper scoring rules, covering a wide range of situations in applied work. This paper discusses implementation and usage details, presents case studies from meteorology and economics, and points to the relevant background literature.},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/DSYW6QUF/Jordan et al. - 2019 - Evaluating Probabilistic Forecasts with bscoring.pdf}
}

@article{joseCharacterizationSphericalScoring2009,
  title = {A {{Characterization}} for the {{Spherical Scoring Rule}}},
  author = {Jose, Victor Richmond},
  year = {2009},
  month = mar,
  journal = {Theory and Decision},
  volume = {66},
  number = {3},
  pages = {263--281},
  issn = {1573-7187},
  doi = {10.1007/s11238-007-9067-x},
  urldate = {2022-03-14},
  abstract = {Strictly proper scoring rules have been studied widely in statistical decision theory and recently in experimental economics because of their ability to encourage assessors to honestly provide their true subjective probabilities. In this article, we study the spherical scoring rule by analytically examining some of its properties and providing some new geometric interpretations for this rule. Moreover, we state a theorem which provides an axiomatic characterization for the spherical scoring rule. The objective of this analysis is to provide a better understanding of one of the most commonly available scoring rules, which could aid decision makers in the selection of an appropriate tool for evaluating and assessing probabilistic forecasts.},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/TSSMRAB9/Jose - 2009 - A Characterization for the Spherical Scoring Rule.pdf}
}

@article{jurekResponseMinimumClinically2021,
  title = {Response (Minimum Clinically Relevant Change) in {{ASD}} Symptoms after an Intervention According to {{CARS-2}}: Consensus from an Expert Elicitation Procedure},
  shorttitle = {Response (Minimum Clinically Relevant Change) in {{ASD}} Symptoms after an Intervention According to {{CARS-2}}},
  author = {Jurek, Lucie and Baltazar, Matias and Gulati, Sheffali and Novakovic, Neda and N{\'u}{\~n}ez, Mar{\'i}a and Oakley, Jeremy and O'Hagan, Anthony},
  year = {2021},
  month = apr,
  journal = {European Child \& Adolescent Psychiatry},
  issn = {1018-8827, 1435-165X},
  doi = {10.1007/s00787-021-01772-z},
  urldate = {2021-11-06},
  abstract = {Abstract             The lack of consensual measures to monitor core change in Autism Spectrum Disorder (ASD) or response to interventions leads to difficulty to prove intervention efficacy on ASD core symptoms. There are no universally accepted outcome measures developed for measuring changes in core symptoms. However, the CARS (Childhood Autism Rating Scale) is one of the outcomes recommended in the EMA Guideline on the clinical development of medicinal products for the treatment of ASD. Unfortunately, there is currently no consensus on the response definition for CARS among individuals with ASD. The aim of this elicitation process was to determine an appropriate definition of a response on the CARS2 scale for interventions in patients with Autism Spectrum Disorder (ASD). An elicitation process was conducted following the Sheffield Elicitation Framework (SHELF). Five experts in the field of ASD and two experts in expert knowledge elicitation participated in an 1-day elicitation workshop. Experts in ASD were previously trained in the SHELF elicitation process and received a dossier of scientific evidence concerning the topic. The response definition was set as the mean clinically relevant improvement averaged over all patients, levels of functioning, age groups and clinicians. Based on the scientific evidence and expert judgment, a normal probability distribution was agreed to represent the state of knowledge of this response with expected value 4.03 and standard deviation 0.664. Considering the remaining uncertainty of the estimation and the available literature, a CARS-2 improvement of 4.5 points has been defined as a threshold to conclude to a response after an intervention. A CARS-2 improvement of 4.5 points could be used to evaluate interventions' meaningfulness in indivudals. This initial finding represents an important new benchmark and may aid decision makers in evaluating the efficacy of interventions in ASD.},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/SRX6AIFC/Jurek et al. - 2021 - Response (minimum clinically relevant change) in A.pdf}
}

@techreport{kargerReciprocalScoringMethod2021,
  type = {{{SSRN Scholarly Paper}}},
  title = {Reciprocal {{Scoring}}: {{A Method}} for {{Forecasting Unanswerable Questions}}},
  shorttitle = {Reciprocal {{Scoring}}},
  author = {Karger, Ezra and Monrad, Joshua and Mellers, Barb and Tetlock, Philip},
  year = {2021},
  month = oct,
  number = {ID 3954498},
  address = {Rochester, NY},
  institution = {Social Science Research Network},
  doi = {10.2139/ssrn.3954498},
  urldate = {2022-03-22},
  abstract = {We propose an elicitation method, Reciprocal Scoring (RS), that challenges forecasters to predict the forecasts of other forecasters. Two studies show how RS can generate accurate forecasts of otherwise unanswerable questions. Study 1 establishes the epistemic credibility of RS: forecasters randomly assigned to use RS were as accurate as forecasters predicting objectively resolvable outcomes using a proper scoring rule---and both groups were more accurate than a control group that felt accountable to neither intersubjective RS metrics nor objective metrics. Study 2 establishes the practical value of RS. We ask highly accurate forecasters to predict each other's forecasts of the effect of government policies on COVID-19 mortality, yielding a real-time ranking of the expected effectiveness of pandemic-containment policies. As in Study 1, RS forecasters converged but in this case on policy recommendations that stand up to scrutiny, even with the benefit of hindsight. The core contribution of RS is its power to create accountability for accuracy in policy debates that have long been stalemated by the absence of accountability.},
  langid = {english},
  keywords = {causal inference,COVID-19,elicitation,forecasting tournaments,policy evaluation},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/8FGUAICX/Karger et al. - 2021 - Reciprocal Scoring A Method for Forecasting Unans.pdf;/Users/nikos/github-synced/zotero-nikos/storage/S2SYDVRK/papers.html}
}

@article{katsagounosSuperforecastingRealityCheck2021,
  title = {Superforecasting Reality Check: {{Evidence}} from a Small Pool of Experts and Expedited Identification},
  shorttitle = {Superforecasting Reality Check},
  author = {Katsagounos, Ilias and Thomakos, Dimitrios D. and Litsiou, Konstantia and Nikolopoulos, Konstantinos},
  year = {2021},
  month = feb,
  journal = {European Journal of Operational Research},
  volume = {289},
  number = {1},
  pages = {107--117},
  issn = {0377-2217},
  doi = {10.1016/j.ejor.2020.06.042},
  urldate = {2023-03-01},
  abstract = {Superforecasting has drawn the attention of academics - despite earlier contradictory findings in the literature, arguing that humans can consistently and successfully forecast over long periods. It has also enthused practitioners, due to the major implications for improving forecast-driven decision-making. The evidence in support of the superforecasting hypothesis was provided via a 4-year project led by Tetlock and Mellers, which was based on an exhaustive experiment with more than 5000 experts across the globe, resulting in identifying 260 superforecasters. The result, however, jeopardizes the applicability of the proposition, as exciting as it may be for the academic world; if every company in the world needs to rely on the aforementioned 260 experts, then this will end up an impractical and expensive endeavor. Thus, it would make sense to test the superforecasting hypothesis in real-life conditions: when only a small pool of experts is available, and there is limited time to identify the superforecasters. If under these constrained conditions the hypothesis still holds, then many small and medium-sized organizations could identify fast and consequently utilize their own superforecasters. In this study, we provide supportive empirical evidence from an experiment with an initial (small) pool of 314 experts and an identification phase of (just) 9 months. Furthermore - and corroborating to the superforecasting literature, we also find preliminary evidence that even an additional training of just 20~min, can influence positively the number of superforecasters identified.},
  langid = {english},
  keywords = {Experts,Forecasting,Real-life conditions,Superforecasting,Training},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/DAF3YZRJ/Katsagounos et al. - 2021 - Superforecasting reality check Evidence from a sm.pdf;/Users/nikos/github-synced/zotero-nikos/storage/JFHF6ESX/S0377221720306007.html}
}

@article{kendallEpidemiologicalImpactsNHS2023,
  title = {Epidemiological Impacts of the {{NHS COVID-19}} App in {{England}} and {{Wales}} throughout Its First Year},
  author = {Kendall, Michelle and Tsallis, Daphne and Wymant, Chris and Di Francia, Andrea and Balogun, Yakubu and Didelot, Xavier and Ferretti, Luca and Fraser, Christophe},
  year = {2023},
  month = feb,
  journal = {Nature Communications},
  volume = {14},
  number = {1},
  pages = {858},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-023-36495-z},
  urldate = {2024-03-12},
  abstract = {The NHS COVID-19 app was launched in England and Wales in September 2020, with a Bluetooth-based contact tracing functionality designed to reduce transmission of SARS-CoV-2. We show that user engagement and the app's epidemiological impacts varied according to changing social and epidemic characteristics throughout the app's first year. We describe the interaction and complementarity of manual and digital contact tracing approaches. Results of our statistical analyses of anonymised, aggregated app data include that app users who were recently notified were more likely to test positive than app users who were not recently notified, by a factor that varied considerably over time. We estimate that the app's contact tracing function alone averted about 1 million cases (sensitivity analysis 450,000--1,400,000) during its first year, corresponding to 44,000 hospital cases (SA 20,000--60,000) and 9,600 deaths (SA 4600--13,000).},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Computational models,Epidemiology,Infectious diseases,SARS-CoV-2},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/U8MN7JAX/Kendall et al. - 2023 - Epidemiological impacts of the NHS COVID-19 app in.pdf}
}

@techreport{keyelProbabilisticEvaluationNull2021,
  type = {Preprint},
  title = {Probabilistic {{Evaluation}} of {{Null Models}} for {{West Nile Virus}} in the {{United States}}},
  author = {Keyel, Alexander C. and Kilpatrick, A. Marm},
  year = {2021},
  month = jul,
  institution = {Ecology},
  doi = {10.1101/2021.07.26.453866},
  urldate = {2021-08-09},
  abstract = {Abstract           Null models provide a useful baseline for the development of new models. A variety of options for null models exist. These options have become more sophisticated with the advent of probabilistic modeling approaches. Here, we evaluate 10 different null models for West Nile virus, a primarily mosquito-borne disease introduced to the United States in 1999. The Historical Null was significantly better than all models other than the Negative Binomial. We recommend the use of either of these models as a baseline when developing new models to predict spatial and temporal dynamics of West Nile virus at the county-annual scale. We expect these results to be scale-dependent, and a future direction is to examine performance of null models at finer spatial and temporal scales.},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/CIWG9IQ7/Keyel and Kilpatrick - 2021 - Probabilistic Evaluation of Null Models for West N.pdf}
}

@misc{kicimanCausalReasoningLarge2023,
  title = {Causal {{Reasoning}} and {{Large Language Models}}: {{Opening}} a {{New Frontier}} for {{Causality}}},
  shorttitle = {Causal {{Reasoning}} and {{Large Language Models}}},
  author = {K{\i}c{\i}man, Emre and Ness, Robert and Sharma, Amit and Tan, Chenhao},
  year = {2023},
  month = may,
  number = {arXiv:2305.00050},
  eprint = {2305.00050},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-06-05},
  abstract = {The causal capabilities of large language models (LLMs) is a matter of significant debate, with critical implications for the use of LLMs in societally impactful domains such as medicine, science, law, and policy. We further our understanding of LLMs and their causal implications, considering the distinctions between different types of causal reasoning tasks, as well as the entangled threats of construct and measurement validity. LLM-based methods establish new state-of-the-art accuracies on multiple causal benchmarks. Algorithms based on GPT-3.5 and 4 outperform existing algorithms on a pairwise causal discovery task (97\%, 13 points gain), counterfactual reasoning task (92\%, 20 points gain), and actual causality (86\% accuracy in determining necessary and sufficient causes in vignettes). At the same time, LLMs exhibit unpredictable failure modes and we provide some techniques to interpret their robustness. Crucially, LLMs perform these causal tasks while relying on sources of knowledge and methods distinct from and complementary to non-LLM based approaches. Specifically, LLMs bring capabilities so far understood to be restricted to humans, such as using collected knowledge to generate causal graphs or identifying background causal context from natural language. We envision LLMs to be used alongside existing causal methods, as a proxy for human domain knowledge and to reduce human effort in setting up a causal analysis, one of the biggest impediments to the widespread adoption of causal methods. We also see existing causal methods as promising tools for LLMs to formalize, validate, and communicate their reasoning especially in high-stakes scenarios. In capturing common sense and domain knowledge about causal mechanisms and supporting translation between natural language and formal methods, LLMs open new frontiers for advancing the research, practice, and adoption of causality.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Statistics - Methodology},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/7QXU2X5B/Kƒ±cƒ±man et al. - 2023 - Causal Reasoning and Large Language Models Openin.pdf;/Users/nikos/github-synced/zotero-nikos/storage/ZFXAXZRM/2305.html}
}

@article{kirkwoodFrameworkProbabilisticWeather2021,
  title = {A Framework for Probabilistic Weather Forecast Post-Processing across Models and Lead Times Using Machine Learning},
  author = {Kirkwood, Charlie and Economou, Theo and Odbert, Henry and Pugeault, Nicolas},
  year = {2021},
  month = apr,
  journal = {Philosophical transactions. Series A, Mathematical, physical, and engineering sciences},
  volume = {379},
  number = {2194},
  pages = {20200099},
  issn = {1364-503X},
  doi = {10.1098/rsta.2020.0099},
  urldate = {2021-10-19},
  abstract = {Forecasting the weather is an increasingly data-intensive exercise. Numerical weather prediction (NWP) models are becoming more complex, with higher resolutions, and there are increasing numbers of different models in operation. While the forecasting skill of NWP models continues to improve, the number and complexity of these models poses a new challenge for the operational meteorologist: how should the information from all available models, each with their own unique biases and limitations, be combined in order to provide stakeholders with well-calibrated probabilistic forecasts to use in decision making? In this paper, we use a road surface temperature example to demonstrate a three-stage framework that uses machine learning to bridge the gap between sets of separate forecasts from NWP models and the `ideal' forecast for decision support: probabilities of future weather outcomes. First, we use quantile regression forests to learn the error profile of each numerical model, and use these to apply empirically derived probability distributions to forecasts. Second, we combine these probabilistic forecasts using quantile averaging. Third, we interpolate between the aggregate quantiles in order to generate a full predictive distribution, which we demonstrate has properties suitable for decision support. Our results suggest that this approach provides an effective and operationally viable framework for the cohesive post-processing of weather forecasts across multiple models and lead times to produce a well-calibrated probabilistic output., This article is part of the theme issue `Machine learning for weather and climate modelling'.},
  pmcid = {PMC7898129},
  pmid = {33583271},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/HVBWBIGR/Kirkwood et al. - 2021 - A framework for probabilistic weather forecast pos.pdf}
}

@misc{kraemer2020epidemiological,
  title = {Epidemiological Data from the {{nCoV-2019}} Outbreak: {{Early}} Descriptions from Publicly Available Data},
  author = {Xu, Bo and Gutierrez, Bernardo and Hill, Sarah and Scarpino, Samuel and Loskill, Alyssa and Wu, Jessie and Sewalk, Kara and Mekaru, Sumiko and Zarebski, Alexander and Pybus, Oliver and Pigott, David and Kraemer, Moritz},
  year = {2020},
  howpublished = {http://virological.org/t/epidemiological-data-from-the-ncov-2019-outbreak-early-descriptions-from-publicly-available-data/337}
}

@article{kretzschmarMathematicalModelsInfectious2009,
  title = {Mathematical {{Models}} in {{Infectious Disease Epidemiology}}},
  author = {Kretzschmar, Mirjam and Wallinga, Jacco},
  year = {2009},
  month = jul,
  journal = {Modern Infectious Disease Epidemiology},
  pages = {209--221},
  doi = {10.1007/978-0-387-93835-6_12},
  urldate = {2024-06-16},
  abstract = {The idea that transmission and spread of infectious diseases follows laws that can be formulated in mathematical language is old. In 1766 Daniel Bernoulli published an article where he described the effects of smallpox variolation (a precursor of vaccination) on life expectancy using mathematical life table analysis (Dietz and Heesterbeek 2000). However, it was only in the twentieth century that the nonlinear dynamics of infectious disease transmission was really understood. In the beginning of that century there was much discussion about why an epidemic ended before all susceptibles were infected with hypotheses about changing virulence of the pathogen during the epidemic.},
  pmcid = {PMC7178885},
  pmid = {null},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/DFS5LEQK/Kretzschmar and Wallinga - 2009 - Mathematical Models in Infectious Disease Epidemio.pdf}
}

@article{kukkonenReviewOperationalRegionalscale2012,
  title = {A Review of Operational, Regional-Scale, Chemical Weather Forecasting Models in {{Europe}}},
  author = {Kukkonen, J. and Olsson, T. and Schultz, D. M. and Baklanov, A. and Klein, T. and Miranda, A. I. and Monteiro, A. and Hirtl, M. and Tarvainen, V. and Boy, M. and Peuch, V.-H. and Poupkou, A. and Kioutsioukis, I. and Finardi, S. and Sofiev, M. and Sokhi, R. and Lehtinen, K. E. J. and Karatzas, K. and San Jos{\'e}, R. and Astitha, M. and Kallos, G. and Schaap, M. and Reimer, E. and Jakobs, H. and Eben, K.},
  year = {2012},
  month = jan,
  journal = {Atmospheric Chemistry and Physics},
  volume = {12},
  number = {1},
  pages = {1--87},
  publisher = {Copernicus GmbH},
  issn = {1680-7316},
  doi = {10.5194/acp-12-1-2012},
  urldate = {2021-12-15},
  abstract = {{$<$}p{$><$}strong class="journal-contentHeaderColor"{$>$}Abstract.{$<$}/strong{$>$} Numerical models that combine weather forecasting and atmospheric chemistry are here referred to as chemical weather forecasting models. Eighteen operational chemical weather forecasting models on regional and continental scales in Europe are described and compared in this article. Topics discussed in this article include how weather forecasting and atmospheric chemistry models are integrated into chemical weather forecasting systems, how physical processes are incorporated into the models through parameterization schemes, how the model architecture affects the predicted variables, and how air chemistry and aerosol processes are formulated. In addition, we discuss sensitivity analysis and evaluation of the models, user operational requirements, such as model availability and documentation, and output availability and dissemination. In this manner, this article allows for the evaluation of the relative strengths and weaknesses of the various modelling systems and modelling approaches. Finally, this article highlights the most prominent gaps of knowledge for chemical weather forecasting models and suggests potential priorities for future research directions, for the following selected focus areas: emission inventories, the integration of numerical weather prediction and atmospheric chemical transport models, boundary conditions and nesting of models, data assimilation of the various chemical species, improved understanding and parameterization of physical processes, better evaluation of models against data and the construction of model ensembles.{$<$}/p{$>$}},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/X3N7D4HE/Kukkonen et al. - 2012 - A review of operational, regional-scale, chemical .pdf;/Users/nikos/github-synced/zotero-nikos/storage/XWR2S6F8/2012.html}
}

@article{laiEvaluatingProbabilityForecasts2011,
  title = {Evaluating Probability Forecasts},
  author = {Lai, Tze Leung and Gross, Shulamith T. and Shen, David Bo},
  year = {2011},
  month = oct,
  journal = {The Annals of Statistics},
  volume = {39},
  number = {5},
  pages = {2356--2382},
  publisher = {Institute of Mathematical Statistics},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/11-AOS902},
  urldate = {2023-09-24},
  abstract = {Probability forecasts of events are routinely used in climate predictions, in forecasting default probabilities on bank loans or in estimating the probability of a patient's positive response to treatment. Scoring rules have long been used to assess the efficacy of the forecast probabilities after observing the occurrence, or nonoccurrence, of the predicted events. We develop herein a statistical theory for scoring rules and propose an alternative approach to the evaluation of probability forecasts. This approach uses loss functions relating the predicted to the actual probabilities of the events and applies martingale theory to exploit the temporal structure between the forecast and the subsequent occurrence or nonoccurrence of the event.},
  keywords = {60G42,62P05,62P99,forecasting,loss functions,Martingales,scoring rules},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/DNW5BQMC/Lai et al. - 2011 - Evaluating probability forecasts.pdf}
}

@article{lauerIncubationPeriodCoronavirus2020,
  title = {The {{Incubation Period}} of {{Coronavirus Disease}} 2019 ({{COVID-19}}) {{From Publicly Reported Confirmed Cases}}: {{Estimation}} and {{Application}}},
  shorttitle = {The {{Incubation Period}} of {{Coronavirus Disease}} 2019 ({{COVID-19}}) {{From Publicly Reported Confirmed Cases}}},
  author = {Lauer, Stephen A. and Grantz, Kyra H. and Bi, Qifang and Jones, Forrest K. and Zheng, Qulu and Meredith, Hannah R. and Azman, Andrew S. and Reich, Nicholas G. and Lessler, Justin},
  year = {2020},
  month = mar,
  journal = {Annals of Internal Medicine},
  pages = {M20-0504},
  issn = {0003-4819},
  doi = {10.7326/M20-0504},
  urldate = {2023-04-10},
  abstract = {Visual Abstract. The Incubation Period of COVID-19 From Publicly Reported Confirmed Cases Using news reports and press releases from provinces, regions, and countries outside Wuhan, Hubei province, China, this analysis estimates the length of the incubation period of coronavirus disease 2019 (COVID-19) and its public health implications. , Using news reports and press releases from provinces, regions, and countries outside Wuhan, Hubei province, China, this analysis estimates the length of the incubation period of coronavirus disease 2019 (COVID-19) and its public health implications.},
  pmcid = {PMC7081172},
  pmid = {32150748},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/P8297N64/Lauer et al. - 2020 - The Incubation Period of Coronavirus Disease 2019 .pdf}
}

@misc{lerchForecasterDilemmaExtreme2015,
  title = {Forecaster's {{Dilemma}}: {{Extreme Events}} and {{Forecast Evaluation}}},
  shorttitle = {Forecaster's {{Dilemma}}},
  author = {Lerch, Sebastian and Thorarinsdottir, Thordis L. and Ravazzolo, Francesco and Gneiting, Tilmann},
  year = {2015},
  month = dec,
  number = {arXiv:1512.09244},
  eprint = {1512.09244},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1512.09244},
  urldate = {2023-06-21},
  abstract = {In public discussions of the quality of forecasts, attention typically focuses on the predictive performance in cases of extreme events. However, the restriction of conventional forecast evaluation methods to subsets of extreme observations has unexpected and undesired effects, and is bound to discredit skillful forecasts when the signal-to-noise ratio in the data generating process is low. Conditioning on outcomes is incompatible with the theoretical assumptions of established forecast evaluation methods, thereby confronting forecasters with what we refer to as the forecaster's dilemma. For probabilistic forecasts, proper weighted scoring rules have been proposed as decision theoretically justifiable alternatives for forecast evaluation with an emphasis on extreme events. Using theoretical arguments, simulation experiments, and a real data study on probabilistic forecasts of U.S. inflation and gross domestic product growth, we illustrate and discuss the forecaster's dilemma along with potential remedies.},
  archiveprefix = {arxiv},
  keywords = {Statistics - Methodology},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/5KYJ2TJ8/Lerch et al. - 2015 - Forecaster's Dilemma Extreme Events and Forecast .pdf;/Users/nikos/github-synced/zotero-nikos/storage/MV8SFZSY/1512.html}
}

@article{lerchForecasterDilemmaExtreme2017,
  title = {Forecaster's {{Dilemma}}: {{Extreme Events}} and {{Forecast Evaluation}}},
  shorttitle = {Forecaster's {{Dilemma}}},
  author = {Lerch, Sebastian and Thorarinsdottir, Thordis L. and Ravazzolo, Francesco and Gneiting, Tilmann},
  year = {2017},
  month = feb,
  journal = {Statistical Science},
  volume = {32},
  number = {1},
  pages = {106--127},
  publisher = {Institute of Mathematical Statistics},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/16-STS588},
  urldate = {2020-08-15},
  abstract = {In public discussions of the quality of forecasts, attention typically focuses on the predictive performance in cases of extreme events. However, the restriction of conventional forecast evaluation methods to subsets of extreme observations has unexpected and undesired effects, and is bound to discredit skillful forecasts when the signal-to-noise ratio in the data generating process is low. Conditioning on outcomes is incompatible with the theoretical assumptions of established forecast evaluation methods, thereby confronting forecasters with what we refer to as the forecaster's dilemma. For probabilistic forecasts, proper weighted scoring rules have been proposed as decision-theoretically justifiable alternatives for forecast evaluation with an emphasis on extreme events. Using theoretical arguments, simulation experiments and a real data study on probabilistic forecasts of U.S. inflation and gross domestic product (GDP) growth, we illustrate and discuss the forecaster's dilemma along with potential remedies.},
  langid = {english},
  mrnumber = {MR3634309},
  zmnumber = {06946266},
  keywords = {Diebold-Mariano test,hindsight bias,likelihood ratio test,Neyman-Pearson lemma,predictive performance,probabilistic forecast,proper weighted scoring rule,rare and extreme events},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/6T79RWK7/Lerch et al. - 2017 - Forecaster‚Äôs Dilemma Extreme Events and Forecast .pdf;/Users/nikos/github-synced/zotero-nikos/storage/GCBLD2K7/1491465630.html}
}

@article{leutbecherEnsembleSizeHow2019,
  title = {Ensemble Size: {{How}} Suboptimal Is Less than Infinity?},
  shorttitle = {Ensemble Size},
  author = {Leutbecher, Martin},
  year = {2019},
  journal = {Quarterly Journal of the Royal Meteorological Society},
  volume = {145},
  number = {S1},
  pages = {107--128},
  issn = {1477-870X},
  doi = {10.1002/qj.3387},
  urldate = {2021-07-05},
  abstract = {Ensemble forecasts are the method of choice in numerical weather prediction (NWP) to generate probabilistic forecasts. The number of members in an ensemble is an important factor in determining how well a probability distribution of a weather-related variable can be estimated. Having only a finite number of members reduces the average skill such a probabilistic forecast can have. Increasing ensemble size is therefore desirable; however, ensemble size is also proportional to the computational cost. Having a small ensemble size limits the cost and makes other improvements, such as increases in spatial resolution, feasible. This article examines how average skill measures with metrics such as the continuous ranked probability score, the quantile score, and the Dawid--Sebastiani score converge with ensemble size. A numerical experiment with a 200 member ensemble using the European Centre for Medium-Range Weather Forecasts (ECMWF) Integrated Forecasting System (IFS) model at a resolution of 29 km and a forecast range of 15 days provides data to compare the convergence of probabilistic skill in a current NWP system with theoretical expectations derived for perfectly reliable ensembles with exchangeable members. Results in the first part of the article can help users of operational NWP ensemble forecasts formulate their minimum requirement in terms of ensemble size. In the second part, requirements for scientists who test changes to NWP systems are examined. Using proper scores and fair scores, it is explored whether testing changes in the ensemble forecasts can be meaningful with fewer members than in the operational configuration. Results are based on medium-range numerical experiments with 50 members. Two experiments test the activation of a representation of model uncertainty and three other experiments test changes in horizontal resolution from 29 to 18 km and from 29 to 45 km.},
  langid = {english},
  keywords = {ensemble size,ensembles,fair score,forecast verification,forecasting,numerical weather prediction,proper scoring rule},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/5S7BT4E4/Leutbecher - 2019 - Ensemble size How suboptimal is less than infinit.pdf;/Users/nikos/github-synced/zotero-nikos/storage/GNK8ZM9M/qj.html}
}

@article{leutbecherUnderstandingChangesContinuous2021,
  title = {Understanding Changes of the Continuous Ranked Probability Score Using a Homogeneous {{Gaussian}} Approximation},
  author = {Leutbecher, Martin and Haiden, Thomas},
  year = {2021},
  journal = {Quarterly Journal of the Royal Meteorological Society},
  volume = {147},
  number = {734},
  pages = {425--442},
  issn = {1477-870X},
  doi = {10.1002/qj.3926},
  urldate = {2023-12-12},
  abstract = {Improving ensemble forecasts is a complex process which involves proper scores such as the continuous ranked probability score (CRPS). A homogeneous Gaussian (hoG) model is introduced in order to better understand the characteristics of the CRPS. An analytical formula is derived for the expected CRPS of an ensemble in the hoG model. The score is a function of the variance of the error of the ensemble mean, the mean error of the ensemble mean and the ensemble variance. The hoG model also provides a score decomposition into reliability and resolution components. We examine whether the hoG model provides a useful approximation of the CRPS when applied to operational ECMWF medium-range ensemble forecasts. The hoG approximation describes the spatial variations of the CRPS well while moderately overestimating the mean score. Seasonal averages over large domains are within 10\% of the actual CRPS. Furthermore, the ability to approximate score changes is evaluated by (a) comparing raw ensemble forecasts with postprocessed ensemble forecasts, and (b) by examining score changes associated with a recent upgrade of the IFS. Overall, the hoG approximation predicts the actual CRPS changes well. One of the main anticipated applications of the hoG approximation are new diagnostics in verification software used by NWP developers routinely. The purpose of the diagnostics is to help developers explain impacts of forecast system changes on the CRPS in terms of the changes in mean error, changes in error variance and changes in ensemble variance. The diagnostics require little additional computational resources compared to the alternative of verifying postprocessed versions of the ensemble forecasts. Therefore, it will be feasible to apply the diagnostics easily to all variables that are examined as part of the model development process.},
  copyright = {{\copyright} 2020 Royal Meteorological Society},
  langid = {english},
  keywords = {bias,decomposition,ensemble forecast,postprocessing,proper score,spread-error relationship,verification},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/GUTQ9NUX/qj.html}
}

@article{liboschikTscountPackageAnalysis2017,
  title = {Tscount: {{An R Package}} for {{Analysis}} of {{Count Time Series Following Generalized Linear Models}}},
  shorttitle = {Tscount},
  author = {Liboschik, Tobias and Fokianos, Konstantinos and Fried, Roland},
  year = {2017},
  month = nov,
  journal = {Journal of Statistical Software},
  volume = {82},
  pages = {1--51},
  issn = {1548-7660},
  doi = {10.18637/jss.v082.i05},
  urldate = {2022-01-21},
  abstract = {The R package tscount provides likelihood-based estimation methods for analysis and modeling of count time series following generalized linear models. This is a flexible class of models which can describe serial correlation in a parsimonious way. The conditional mean of the process is linked to its past values, to past observations and to potential covariate effects. The package allows for models with the identity and with the logarithmic link function. The conditional distribution can be Poisson or negative binomial. An important special case of this class is the so-called INGARCH model and its log-linear extension. The package includes methods for model fitting and assessment, prediction and intervention analysis. This paper summarizes the theoretical background of these methods. It gives details on the implementation of the package and provides simulation results for models which have not been studied theoretically before. The usage of the package is illustrated by two data examples. Additionally, we provide a review of R packages which can be used for count time series analysis. This includes a detailed comparison of tscount to those packages.},
  copyright = {Copyright (c) 2017 Tobias Liboschik, Konstantinos Fokianos, Roland Fried},
  langid = {english},
  keywords = {serial correlation},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/NAKUKRYZ/Liboschik et al. - 2017 - tscount An R Package for Analysis of Count Time S.pdf}
}

@article{linDiseaseSeverityClinical2021,
  title = {The {{Disease Severity}} and {{Clinical Outcomes}} of the {{SARS-CoV-2 Variants}} of {{Concern}}},
  author = {Lin, Lixin and Liu, Ying and Tang, Xiujuan and He, Daihai},
  year = {2021},
  journal = {Frontiers in Public Health},
  volume = {9},
  issn = {2296-2565},
  urldate = {2023-02-16},
  abstract = {With the continuation of the pandemic, many severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) variants have appeared around the world. Owing to a possible risk of increasing the transmissibility of the virus, severity of the infected individuals, and the ability to escape the antibody produced by the vaccines, the four SARS-CoV-2 variants of Alpha (B.1.1.7), Beta (B.1.351), Gamma (P.1), and Delta (B.1.617.2) have attracted the most widespread attention. At present, there is a unified conclusion that these four variants have increased the transmissibility of SARS-CoV-2, but the severity of the disease caused by them has not yet been determined. Studies from June 1, 2020 to October 15, 2021 were considered, and a meta-analysis was carried out to process the data. Alpha, Beta, Gamma, and Delta variants are all more serious than the wild-type virus in terms of hospitalization, ICU admission, and mortality, and the Beta and Delta variants have a higher risk than the Alpha and Gamma variants. Notably, the random effects of Beta variant to the wild-type virus with respect to hospitalization rate, severe illness rate, and mortality rate are 2.16 (95\% CI: 1.19--3.14), 2.23 (95\% CI: 1.31--3.15), and 1.50 (95\% CI: 1.26--1.74), respectively, and the random effects of Delta variant to the wild-type virus are 2.08 (95\% CI: 1.77--2.39), 3.35 (95\% CI: 2.5--4.2), and 2.33 (95\% CI: 1.45--3.21), respectively. Although, the emergence of vaccines may reduce the threat posed by SARS-CoV-2 variants, these are still very important, especially the Beta and Delta variants.},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/3WE3QDYT/Lin et al. - 2021 - The Disease Severity and Clinical Outcomes of the .pdf}
}

@misc{loganc.brooksComparingEnsembleApproaches,
  title = {Comparing Ensemble Approaches for Short-Term Probabilistic {{COVID-19}} Forecasts in the {{U}}.{{S}}. - {{International Institute}} of {{Forecasters}}},
  author = {{Logan C. Brooks} and {Evan L. Ray} and {Jacob Bien} and {Johannes Bracher} and {Aaron Rumack} and {Ryan J. Tibshirani} and {Nicholas G. Reich}},
  urldate = {2021-07-12},
  chapter = {Forecasting News},
  langid = {american},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/UCPVDFPE/comparing-ensemble-approaches-for-short-term-probabilistic-covid-19-forecasts-in-the-u-s.html}
}

@article{loweStochasticRainfallrunoffForecasting2014,
  title = {Stochastic Rainfall-Runoff Forecasting: Parameter Estimation, Multi-Step Prediction, and Evaluation of Overflow Risk},
  shorttitle = {Stochastic Rainfall-Runoff Forecasting},
  author = {L{\"o}we, Roland and Mikkelsen, Peter Steen and Madsen, Henrik},
  year = {2014},
  month = mar,
  journal = {Stochastic Environmental Research and Risk Assessment},
  volume = {28},
  number = {3},
  pages = {505--516},
  issn = {1436-3259},
  doi = {10.1007/s00477-013-0768-0},
  urldate = {2022-12-07},
  abstract = {Probabilistic runoff forecasts generated by stochastic greybox models can be notably useful for the improvement of the decision-making process in real-time control setups for urban drainage systems because the prediction risk relationships in these systems are often highly nonlinear. To date, research has primarily focused on one-step-ahead flow predictions for identifying, estimating, and evaluating greybox models. For control purposes, however, stochastic predictions are required for longer forecast horizons and for the prediction of runoff volumes, rather than flows. This article therefore analyzes the quality of multistep ahead forecasts of runoff volume and considers new estimation methods based on scoring rules for k-step-ahead predictions. The study shows that the score-based methods are, in principle, suitable for the estimation of model parameters and can therefore help the identification of models for cases with noisy in-sewer observations. For the prediction of the overflow risk, no improvement was demonstrated through the application of stochastic forecasts instead of point predictions, although this result is thought to be caused by the notably simplified setup used in this analysis. In conclusion, further research must focus on the development of model structures that allow the proper separation of dry and wet weather uncertainties and simulate runoff uncertainties depending on the rainfall input.},
  langid = {english},
  keywords = {Multistep prediction,Online forecasting,Real-time control,Skill score,Stochastic greybox model,Urban drainage},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/EK2FRDJY/L√∂we et al. - 2014 - Stochastic rainfall-runoff forecasting parameter .pdf}
}

@article{macheteContrastingProbabilisticScoring2012,
  title = {Contrasting {{Probabilistic Scoring Rules}}},
  author = {Machete, Reason Lesego},
  year = {2012},
  month = jul,
  journal = {arXiv:1112.4530 [math, stat]},
  eprint = {1112.4530},
  primaryclass = {math, stat},
  urldate = {2020-03-21},
  abstract = {There are several scoring rules that one can choose from in order to score probabilistic forecasting models or estimate model parameters. Whilst it is generally agreed that proper scoring rules are preferable, there is no clear criterion for preferring one proper scoring rule above another. This manuscript compares and contrasts some commonly used proper scoring rules and provides guidance on scoring rule selection. In particular, it is shown that the logarithmic scoring rule prefers erring with more uncertainty, the spherical scoring rule prefers erring with lower uncertainty, whereas the other scoring rules are indifferent to either option.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {62B10 62C05 62G05 62G07 62F99 62P05 62P12 62P20,Mathematics - Statistics Theory},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/8FYPC3Y4/Machete - 2012 - Contrasting Probabilistic Scoring Rules.pdf}
}

@article{mannTestWhetherOne1947,
  title = {On a {{Test}} of {{Whether}} One of {{Two Random Variables}} Is {{Stochastically Larger}} than the {{Other}}},
  author = {Mann, H. B. and Whitney, D. R.},
  year = {1947},
  month = mar,
  journal = {The Annals of Mathematical Statistics},
  volume = {18},
  number = {1},
  pages = {50--60},
  publisher = {Institute of Mathematical Statistics},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177730491},
  urldate = {2022-01-21},
  abstract = {Let \$x\$ and \$y\$ be two random variables with continuous cumulative distribution functions \$f\$ and \$g\$. A statistic \$U\$ depending on the relative ranks of the \$x\$'s and \$y\$'s is proposed for testing the hypothesis \$f = g\$. Wilcoxon proposed an equivalent test in the Biometrics Bulletin, December, 1945, but gave only a few points of the distribution of his statistic. Under the hypothesis \$f = g\$ the probability of obtaining a given \$U\$ in a sample of \$n x's\$ and \$m y's\$ is the solution of a certain recurrence relation involving \$n\$ and \$m\$. Using this recurrence relation tables have been computed giving the probability of \$U\$ for samples up to \$n = m = 8\$. At this point the distribution is almost normal. From the recurrence relation explicit expressions for the mean, variance, and fourth moment are obtained. The 2rth moment is shown to have a certain form which enabled us to prove that the limit distribution is normal if \$m, n\$ go to infinity in any arbitrary manner. The test is shown to be consistent with respect to the class of alternatives \$f(x) {$>$} g(x)\$ for every \$x\$.},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/YTWX67GQ/Mann and Whitney - 1947 - On a Test of Whether one of Two Random Variables i.pdf;/Users/nikos/github-synced/zotero-nikos/storage/8DA4G3H8/1177730491.html}
}

@article{mathesonScoringRulesContinuous1976,
  title = {Scoring {{Rules}} for {{Continuous Probability Distributions}}},
  author = {Matheson, James E. and Winkler, Robert L.},
  year = {1976},
  month = jun,
  journal = {Management Science},
  volume = {22},
  number = {10},
  pages = {1087--1096},
  publisher = {INFORMS},
  issn = {0025-1909},
  doi = {10.1287/mnsc.22.10.1087},
  urldate = {2020-08-13},
  abstract = {Personal, or subjective, probabilities are used as inputs to many inferential and decision-making models, and various procedures have been developed for the elicitation of such probabilities. Included among these elicitation procedures are scoring rules, which involve the computation of a score based on the assessor's stated probabilities and on the event that actually occurs. The development of scoring rules has, in general, been restricted to the elicitation of discrete probability distributions. In this paper, families of scoring rules for the elicitation of continuous probability distributions are developed and discussed.},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/SVJ7YPP7/Matheson and Winkler - 1976 - Scoring Rules for Continuous Probability Distribut.pdf;/Users/nikos/github-synced/zotero-nikos/storage/H5CNZS4U/mnsc.22.10.html}
}

@article{mayerImprovingPowerDiebold2017,
  title = {Improving the Power of the {{Diebold}}--{{Mariano}}--{{West}} Test for Least Squares Predictions},
  author = {Mayer, Walter J. and Liu, Feng and Dang, Xin},
  year = {2017},
  month = jul,
  journal = {International Journal of Forecasting},
  volume = {33},
  number = {3},
  pages = {618--626},
  issn = {01692070},
  doi = {10.1016/j.ijforecast.2017.01.008},
  urldate = {2023-09-24},
  abstract = {We propose a more powerful version of the test of Diebold and Mariano (1995) and West (1996) for comparing least squares predictors based on non-nested models when the parameter being tested is the expected difference between the squared prediction errors. The proposed test improves the asymptotic power by using a more efficient estimator of the parameter being tested than that used in the literature. The estimator used by the standard version of the test depends on the individual predictions and realizations only through the observations on the prediction errors. However, the parameter being tested can also be expressed in terms of moments of the predictors and the predicted variable, some of which cannot be identified separately by the observations on the prediction errors alone. Parameterizing these moments in a GMM framework and drawing on the theory of West (1996), we devise more powerful versions of the test by exploiting a restriction that is maintained routinely under the null hypothesis by West (1996, Assumption 2b) and later studies. This restriction requires only finite second-order moments and covariance stationarity in order to ensure that the population linear projection exists. Simulation experiments show that the potential gains in power can be substantial.},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/RHXFH6P3/Mayer et al. - 2017 - Improving the power of the Diebold‚ÄìMariano‚ÄìWest te.pdf}
}

@article{mayrLogLevelVAR2015,
  title = {Log versus Level in {{VAR}} Forecasting: 42 Million Empirical Answers---{{Expect}} the Unexpected},
  shorttitle = {Log versus Level in {{VAR}} Forecasting},
  author = {Mayr, Johannes and Ulbricht, Dirk},
  year = {2015},
  month = jan,
  journal = {Economics Letters},
  volume = {126},
  pages = {40--42},
  issn = {0165-1765},
  doi = {10.1016/j.econlet.2014.11.008},
  urldate = {2022-12-07},
  abstract = {The use of log-transformed data has become standard in macroeconomic forecasting with VAR models. However, its appropriateness in the context of out-of-sample forecasts has not yet been exposed to a thorough empirical investigation. With the aim of filling this void, a broad sample of VAR models is employed in a multi-country set up and approximately 42 million pseudo-out-of-sample forecasts of GDP are evaluated. The results show that, on average, the knee-jerk transformation of the data is at best harmless.},
  langid = {english},
  keywords = {Logarithmic transformation,Out-of-sample experiment,VAR-forecasting},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/UITFBGW3/Mayr and Ulbricht - 2015 - Log versus level in VAR forecasting 42 million em.pdf;/Users/nikos/github-synced/zotero-nikos/storage/IA4IHTS5/S0165176514004273.html}
}

@article{mcandrewAggregatingHumanJudgment2022,
  title = {Aggregating Human Judgment Probabilistic Predictions of the Safety, Efficacy, and Timing of a {{COVID-19}} Vaccine},
  author = {McAndrew, Thomas and Cambeiro, Juan and Besiroglu, Tamay},
  year = {2022},
  month = apr,
  journal = {Vaccine},
  volume = {40},
  number = {15},
  pages = {2331--2341},
  issn = {0264-410X},
  doi = {10.1016/j.vaccine.2022.02.054},
  urldate = {2023-02-01},
  abstract = {Safe, efficacious vaccines were developed to reduce the transmission of SARS-CoV-2 during the COVID-19 pandemic. But in the middle of 2020, vaccine effectiveness, safety, and the timeline for when a vaccine would be approved and distributed to the public was uncertain. To support public health decision making, we solicited trained forecasters and experts in vaccinology and infectious disease to provide monthly probabilistic predictions from July to September of 2020 of the efficacy, safety, timing, and delivery of a COVID-19 vaccine. We found, that despite sparse historical data, a linear pool---a combination of human judgment probabilistic predictions---can quantify the uncertainty in clinical significance and timing of a potential vaccine. The linear pool underestimated how fast a therapy would show a survival benefit and the high efficacy of approved COVID-19 vaccines. However, the linear pool did make an accurate prediction for when a vaccine would be approved by the FDA. Compared to individual forecasters, the linear pool was consistently above the median of the most accurate forecasts. A linear pool is a fast and versatile method to build probabilistic predictions of a developing vaccine that is robust to poor individual predictions. Though experts and trained forecasters did underestimate the speed of development and the high efficacy of a SARS-CoV-2 vaccine, linear pool predictions can improve situational awareness for public health officials and for the public make clearer the risks, rewards, and timing of a vaccine.},
  langid = {english},
  keywords = {COVID-19,Forecasting,Human judgement,Vaccine},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/7QDAJ9C4/McAndrew et al. - 2022 - Aggregating human judgment probabilistic predictio.pdf;/Users/nikos/github-synced/zotero-nikos/storage/ZK26BETC/S0264410X22002006.html}
}

@article{mcandrewAggregatingPredictionsExperts2021,
  title = {Aggregating Predictions from Experts: {{A}} Review of Statistical Methods, Experiments, and Applications},
  shorttitle = {Aggregating Predictions from Experts},
  author = {McAndrew, Thomas and Wattanachit, Nutcha and Gibson, Graham C. and Reich, Nicholas G.},
  year = {2021},
  journal = {WIREs Computational Statistics},
  volume = {13},
  number = {2},
  pages = {e1514},
  issn = {1939-0068},
  doi = {10.1002/wics.1514},
  urldate = {2021-05-30},
  abstract = {Forecasts support decision making in a variety of applications. Statistical models can produce accurate forecasts given abundant training data, but when data is sparse or rapidly changing, statistical models may not be able to make accurate predictions. Expert judgmental forecasts---models that combine expert-generated predictions into a single forecast---can make predictions when training data is limited by relying on human intuition. Researchers have proposed a wide array of algorithms to combine expert predictions into a single forecast, but there is no consensus on an optimal aggregation model. This review surveyed recent literature on aggregating expert-elicited predictions. We gathered common terminology, aggregation methods, and forecasting performance metrics, and offer guidance to strengthen future work that is growing at an accelerated pace. This article is categorized under: Statistical Learning and Exploratory Methods of the Data Sciences {$>$} Clustering and Classification Statistical Learning and Exploratory Methods of the Data Sciences {$>$} Exploratory Data Analysis Statistical Learning and Exploratory Methods of the Data Sciences {$>$} Modeling Methods Statistical and Graphical Methods of Data Analysis {$>$} Multivariate Analysis},
  copyright = {{\copyright} 2020 Wiley Periodicals LLC.},
  langid = {english},
  keywords = {consensus,expert judgment,forecast aggregation,forecast combination,judgmental forecasting},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/W29GXEKL/McAndrew et al. - 2021 - Aggregating predictions from experts A review of .pdf;/Users/nikos/github-synced/zotero-nikos/storage/4LD7PJYF/wics.html}
}

@article{mcandrewChimericForecastingCombining2022,
  title = {Chimeric Forecasting: Combining Probabilistic Predictions from Computational Models and Human Judgment},
  shorttitle = {Chimeric Forecasting},
  author = {McAndrew, Thomas and Codi, Allison and Cambeiro, Juan and Besiroglu, Tamay and Braun, David and Chen, Eva and De C{\`e}saris, Luis Enrique Urtubey and Luk, Damon},
  year = {2022},
  month = nov,
  journal = {BMC infectious diseases},
  volume = {22},
  number = {1},
  pages = {833},
  issn = {1471-2334},
  doi = {10.1186/s12879-022-07794-5},
  urldate = {2023-02-01},
  abstract = {Forecasts of the trajectory of an infectious agent can help guide public health decision making. A traditional approach to forecasting fits a computational model to structured data and generates a predictive distribution. However, human judgment has access to the same data as computational models plus experience, intuition, and subjective data. We propose a chimeric ensemble-a combination of computational and human judgment forecasts-as a novel approach to predicting the trajectory of an infectious agent. Each month from January, 2021 to June, 2021 we asked two generalist crowds, using the same criteria as the COVID-19 Forecast Hub, to submit a predictive distribution over incident cases and deaths at the US national level either two or three weeks into the future and combined these human judgment forecasts with forecasts from computational models submitted to the COVID-19 Forecasthub into a chimeric ensemble. We find a chimeric ensemble compared to an ensemble including only computational models improves predictions of incident cases and shows similar performance for predictions of incident deaths. A chimeric ensemble is a flexible, supportive public health tool and shows promising results for predictions of the spread of an infectious agent.},
  copyright = {cc by},
  langid = {english},
  pmcid = {PMC9648897},
  pmid = {36357829},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/MECUBIMY/12879_2022_7794_MOESM1_ESM.pdf;/Users/nikos/github-synced/zotero-nikos/storage/ULVZRCFS/McAndrew et al. - 2022 - Chimeric forecasting combining probabilistic pred.pdf}
}

@article{mcandrewEarlyHumanJudgment2022,
  title = {Early Human Judgment Forecasts of Human Monkeypox, {{May}} 2022},
  author = {McAndrew, Thomas and Majumder, Maimuna S. and Lover, Andrew A. and Venkatramanan, Srini and Bocchini, Paolo and Besiroglu, Tamay and Codi, Allison and Braun, David and Dempsey, Gaia and Abbott, Sam and Chevalier, Sylvain and Bosse, Nikos I. and Cambeiro, Juan},
  year = {2022},
  month = aug,
  journal = {The Lancet Digital Health},
  volume = {4},
  number = {8},
  pages = {e569-e571},
  publisher = {Elsevier},
  issn = {2589-7500},
  doi = {10.1016/S2589-7500(22)00127-3},
  urldate = {2023-02-01},
  langid = {english},
  pmid = {35811294},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/I8DWB4VK/McAndrew et al. - 2022 - Early human judgment forecasts of human monkeypox,.pdf}
}

@article{mcandrewExpertJudgmentModel2020,
  title = {An Expert Judgment Model to Predict Early Stages of the {{COVID-19}} Outbreak in the {{United States}}},
  author = {McAndrew, Thomas Charles and Reich, Nicholas G.},
  year = {2020},
  month = sep,
  journal = {medRxiv},
  pages = {2020.09.21.20196725},
  publisher = {Cold Spring Harbor Laboratory Press},
  issn = {2019-6725},
  doi = {10.1101/2020.09.21.20196725},
  urldate = {2020-09-23},
  abstract = {{$<$}p{$>$}During early stages of the COVID-19 pandemic, forecasts provided actionable information about disease transmission to public health decision-makers. Between February and May 2020, experts in infectious disease modeling made weekly predictions about the impact of the pandemic in the U.S. We aggregated these predictions into consensus predictions. In March and April 2020, experts predicted that the number of COVID-19 related deaths in the U.S. by the end of 2020 would be in the range of 150,000 to 250,000, with scenarios of near 1m deaths considered plausible. The wide range of possible future outcomes underscored the uncertainty surrounding the outbreak9s trajectory. Experts9 predictions of measurable short-term outcomes had varying levels of accuracy over the surveys but showed appropriate levels of uncertainty when aggregated. An expert consensus model can provide important insight early on in an emerging global catastrophe.{$<$}/p{$>$}},
  copyright = {{\copyright} 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/3M97Z8QX/McAndrew and Reich - 2020 - An expert judgment model to predict early stages o.pdf;/Users/nikos/github-synced/zotero-nikos/storage/XEAHR8XR/2020.09.21.html}
}

@article{mcandrewExpertJudgmentModel2022,
  title = {An Expert Judgment Model to Predict Early Stages of the {{COVID-19}} Pandemic in the {{United States}}},
  author = {McAndrew, Thomas and Reich, Nicholas G.},
  year = {2022},
  month = sep,
  journal = {PLOS Computational Biology},
  volume = {18},
  number = {9},
  pages = {e1010485},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1010485},
  urldate = {2023-02-01},
  abstract = {From February to May 2020, experts in the modeling of infectious disease provided quantitative predictions and estimates of trends in the emerging COVID-19 pandemic in a series of 13 surveys. Data on existing transmission patterns were sparse when the pandemic began, but experts synthesized information available to them to provide quantitative, judgment-based assessments of the current and future state of the pandemic. We aggregated expert predictions into a single ``linear pool'' by taking an equally weighted average of their probabilistic statements. At a time when few computational models made public estimates or predictions about the pandemic, expert judgment provided (a) falsifiable predictions of short- and long-term pandemic outcomes related to reported COVID-19 cases, hospitalizations, and deaths, (b) estimates of latent viral transmission, and (c) counterfactual assessments of pandemic trajectories under different scenarios. The linear pool approach of aggregating expert predictions provided more consistently accurate predictions than any individual expert, although the predictive accuracy of a linear pool rarely provided the most accurate prediction. This work highlights the importance that an expert linear pool could play in flexibly assessing a wide array of risks early in future emerging outbreaks, especially in settings where available data cannot yet support data-driven computational modeling.},
  langid = {english},
  keywords = {Body weight,COVID 19,Forecasting,Pandemics,Probability distribution,Public and occupational health,SARS CoV 2,Surveys},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/MX23NF3S/McAndrew and Reich - 2022 - An expert judgment model to predict early stages o.pdf}
}

@article{mcandrewExpertJudgmentModel2022a,
  title = {An Expert Judgment Model to Predict Early Stages of the {{COVID-19}} Pandemic in the {{United States}}},
  author = {McAndrew, Thomas and Reich, Nicholas G.},
  year = {2022},
  month = sep,
  journal = {PLOS Computational Biology},
  volume = {18},
  number = {9},
  pages = {e1010485},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1010485},
  urldate = {2024-02-10},
  abstract = {From February to May 2020, experts in the modeling of infectious disease provided quantitative predictions and estimates of trends in the emerging COVID-19 pandemic in a series of 13 surveys. Data on existing transmission patterns were sparse when the pandemic began, but experts synthesized information available to them to provide quantitative, judgment-based assessments of the current and future state of the pandemic. We aggregated expert predictions into a single ``linear pool'' by taking an equally weighted average of their probabilistic statements. At a time when few computational models made public estimates or predictions about the pandemic, expert judgment provided (a) falsifiable predictions of short- and long-term pandemic outcomes related to reported COVID-19 cases, hospitalizations, and deaths, (b) estimates of latent viral transmission, and (c) counterfactual assessments of pandemic trajectories under different scenarios. The linear pool approach of aggregating expert predictions provided more consistently accurate predictions than any individual expert, although the predictive accuracy of a linear pool rarely provided the most accurate prediction. This work highlights the importance that an expert linear pool could play in flexibly assessing a wide array of risks early in future emerging outbreaks, especially in settings where available data cannot yet support data-driven computational modeling.},
  langid = {english},
  keywords = {Body weight,COVID 19,Forecasting,Pandemics,Probability distribution,Public and occupational health,SARS CoV 2,Surveys},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/4MU38IQN/McAndrew and Reich - 2022 - An expert judgment model to predict early stages o.pdf}
}

@article{mcgowanCollaborativeEffortsForecast2019,
  title = {Collaborative Efforts to Forecast Seasonal Influenza in the {{United States}}, 2015--2016},
  author = {McGowan, Craig J. and Biggerstaff, Matthew and Johansson, Michael and Apfeldorf, Karyn M. and {Ben-Nun}, Michal and Brooks, Logan and Convertino, Matteo and Erraguntla, Madhav and Farrow, David C. and Freeze, John and Ghosh, Saurav and Hyun, Sangwon and Kandula, Sasikiran and Lega, Joceline and Liu, Yang and Michaud, Nicholas and Morita, Haruka and Niemi, Jarad and Ramakrishnan, Naren and Ray, Evan L. and Reich, Nicholas G. and Riley, Pete and Shaman, Jeffrey and Tibshirani, Ryan and Vespignani, Alessandro and Zhang, Qian and Reed, Carrie},
  year = {2019},
  month = jan,
  journal = {Scientific Reports},
  volume = {9},
  number = {1},
  pages = {683},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-018-36361-9},
  urldate = {2021-05-30},
  abstract = {Since 2013, the Centers for Disease Control and Prevention (CDC) has hosted an annual influenza season forecasting challenge. The 2015--2016 challenge consisted of weekly probabilistic forecasts of multiple targets, including fourteen models submitted by eleven teams. Forecast skill was evaluated using a modified logarithmic score. We averaged submitted forecasts into a mean ensemble model and compared them against predictions based on historical trends. Forecast skill was highest for seasonal peak intensity and short-term forecasts, while forecast skill for timing of season onset and peak week was generally low. Higher forecast skill was associated with team participation in previous influenza forecasting challenges and utilization of ensemble forecasting techniques. The mean ensemble consistently performed well and outperformed historical trend predictions. CDC and contributing teams will continue to advance influenza forecasting and work to improve the accuracy and reliability of forecasts to facilitate increased incorporation into public health response efforts.},
  copyright = {2019 The Author(s)},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/GHDYHNAE/McGowan et al. - 2019 - Collaborative efforts to forecast seasonal influen.pdf;/Users/nikos/github-synced/zotero-nikos/storage/9ES4FDUU/s41598-018-36361-9.html}
}

@article{meakinComparativeAssessmentMethods2022,
  title = {Comparative Assessment of Methods for Short-Term Forecasts of {{COVID-19}} Hospital Admissions in {{England}} at the Local Level},
  author = {Meakin, Sophie and Abbott, Sam and Bosse, Nikos and Munday, James and Gruson, Hugo and Hellewell, Joel and Sherratt, Katharine and Chapman, Lloyd A. C. and Prem, Kiesha and Klepac, Petra and Jombart, Thibaut and Knight, Gwenan M. and Jafari, Yalda and Flasche, Stefan and Waites, William and Jit, Mark and Eggo, Rosalind M. and {Villabona-Arenas}, C. Julian and Russell, Timothy W. and Medley, Graham and Edmunds, W. John and Davies, Nicholas G. and Liu, Yang and Hu{\'e}, St{\'e}phane and Brady, Oliver and Pung, Rachael and Abbas, Kaja and Gimma, Amy and Mee, Paul and Endo, Akira and Clifford, Samuel and Sun, Fiona Yueqian and McCarthy, Ciara V. and Quilty, Billy J. and Rosello, Alicia and Sandmann, Frank G. and Barnard, Rosanna C. and Kucharski, Adam J. and Procter, Simon R. and Jarvis, Christopher I. and Gibbs, Hamish P. and Hodgson, David and Lowe, Rachel and Atkins, Katherine E. and Koltai, Mihaly and Pearson, Carl A. B. and Finch, Emilie and Wong, Kerry L. M. and Quaife, Matthew and O'Reilly, Kathleen and Tully, Damien C. and Funk, Sebastian and {CMMID COVID-19 Working Group}},
  year = {2022},
  month = feb,
  journal = {BMC Medicine},
  volume = {20},
  number = {1},
  pages = {86},
  issn = {1741-7015},
  doi = {10.1186/s12916-022-02271-x},
  urldate = {2023-03-19},
  abstract = {Forecasting healthcare demand is essential in epidemic settings, both to inform situational awareness and facilitate resource planning. Ideally, forecasts should be robust across time and locations. During the COVID-19 pandemic in England, it is an ongoing concern that demand for hospital care for COVID-19 patients in England will exceed available resources.},
  keywords = {COVID-19,Ensemble,Forecasting,Healthcare demand,Infectious disease,Outbreak,Real-time},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/UTZ7643I/Meakin et al. - 2022 - Comparative assessment of methods for short-term f.pdf;/Users/nikos/github-synced/zotero-nikos/storage/BNF77GSK/s12916-022-02271-x.html}
}

@article{medinaComparisonProbabilisticPostprocessing2020,
  title = {Comparison of Probabilistic Post-Processing Approaches for Improving Numerical Weather Prediction-Based Daily and Weekly Reference Evapotranspiration Forecasts},
  author = {Medina, Hanoi and Tian, Di},
  year = {2020},
  month = mar,
  journal = {Hydrology and Earth System Sciences},
  volume = {24},
  number = {2},
  pages = {1011--1030},
  publisher = {Copernicus GmbH},
  issn = {1027-5606},
  doi = {10.5194/hess-24-1011-2020},
  urldate = {2021-10-19},
  abstract = {{$<$}p{$><$}strong class="journal-contentHeaderColor"{$>$}Abstract.{$<$}/strong{$>$} Reference evapotranspiration (ET\textsubscript{0}) forecasts play an important role in agricultural, environmental, and water management. This study evaluated probabilistic post-processing approaches, including the nonhomogeneous Gaussian regression (NGR), affine kernel dressing (AKD), and Bayesian model averaging (BMA) techniques, for improving daily and weekly ET\textsubscript{0} forecasting based on single or multiple numerical weather predictions (NWPs) from the THORPEX Interactive Grand Global Ensemble (TIGGE), which includes the European Centre for Medium-Range Weather Forecasts (ECMWF), the National Centers for Environmental Prediction (NCEP) Global Forecast System (GFS), and the United Kingdom Meteorological Office (UKMO) forecasts. The approaches were examined for the forecasting of summer ET\textsubscript{0} at 101 US Regional Climate Reference Network stations distributed all over the contiguous United States (CONUS). We found that the NGR, AKD, and BMA methods greatly improved the skill and reliability of the ET\textsubscript{0} forecasts compared with a linear regression bias correction method, due to the considerable adjustments in the spread of ensemble forecasts. The methods were especially effective when applied over the raw NCEP forecasts, followed by the raw UKMO forecasts, because of their low skill compared with that of the raw ECMWF forecasts. The post-processed weekly forecasts had much lower rRMSE values (between 8\&thinsp;\% and 11\&thinsp;\%) than the persistence-based weekly forecasts (22\&thinsp;\%) and the post-processed daily forecasts (between 13\&thinsp;\% and 20\&thinsp;\%). Compared with the single-model ensemble, ET\textsubscript{0} forecasts based on ECMWF multi-model ensemble ET\textsubscript{0} forecasts showed higher skill at shorter lead times (1 or 2\&thinsp;d) and over the southern and western regions of the US. The improvement was higher at a daily timescale than at a weekly timescale. The NGR and AKD methods showed the best performance; however, unlike the AKD method, the NGR method can post-process multi-model forecasts and is easier to interpret than the other methods. In summary, this study demonstrated that the three probabilistic approaches generally outperform conventional procedures based on the simple bias correction of single-model forecasts, with the NGR post-processing of the ECMWF and ECMWF--UKMO forecasts providing the most cost-effective ET\textsubscript{0} forecasting.{$<$}/p{$>$}},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/HUPUNWKQ/Medina and Tian - 2020 - Comparison of probabilistic post-processing approa.pdf;/Users/nikos/github-synced/zotero-nikos/storage/KNCH9X7D/2020.html}
}

@misc{metaculusPreliminaryLookMetaculus2020,
  title = {A {{Preliminary Look}} at {{Metaculus}} and {{Expert Forecasts}}},
  author = {{Metaculus}},
  year = {2020},
  month = jun,
  urldate = {2021-05-30},
  howpublished = {https://www.metaculus.com/news/2020/06/02/LRT/},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/FVCKCU7K/LRT.html}
}

@article{morrisWebbasedToolEliciting2014,
  title = {A Web-Based Tool for Eliciting Probability Distributions from Experts},
  author = {Morris, David E. and Oakley, Jeremy E. and Crowe, John A.},
  year = {2014},
  month = feb,
  journal = {Environmental Modelling \& Software},
  volume = {52},
  pages = {1--4},
  issn = {13648152},
  doi = {10.1016/j.envsoft.2013.10.010},
  urldate = {2021-11-06},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/JAW2WHGB/Morris et al. - 2014 - A web-based tool for eliciting probability distrib.pdf}
}

@article{murphyHedgingSkillScores1973,
  title = {Hedging and {{Skill Scores}} for {{Probability Forecasts}}},
  author = {Murphy, Allan H.},
  year = {1973},
  journal = {Journal of Applied Meteorology (1962-1982)},
  volume = {12},
  number = {1},
  eprint = {26176819},
  eprinttype = {jstor},
  pages = {215--223},
  publisher = {American Meteorological Society},
  issn = {0021-8952},
  urldate = {2023-10-13},
  abstract = {An individual skill score (SS) and a collective skill score (CSS) are examined to determine whether these scoring rules are strictly proper or improper. The SS and the CSS are both standardized versions of the Brier, or probability, score (PS) and have been used to measure the "skill" of probability forecasts. The SS is defined in terms of individual forecasts, while the CSS is defined in terms of collections of forecasts. The SS and the CSS are shown to be improper scoring rules, and, as a result, both the SS and the CSS encourage hedging on the part of forecasters. The results of a preliminary investigation of the nature of the hedging produced by the SS and the CSS indicate that, while the SS may encourage a considerable amount of hedging, the CSS, in general, encourages only a modest amount of hedging, and even this hedging decreases as the sample size K of the collection forecasts increases. In fact, the CSS is approximately strictly proper for large collections of forecasts (K{$\geq$}100). Finally, we briefly consider two questions related to the standardization of scoring rules: 1) the use of different scoring rules in the assessment and evaluation tasks, and 2) the transformation of strictly proper scoring rules. With regard to the latter, we identify standardized versions of the PS which are strictly proper scoring rules and which, as a result, appear to be appropriate scoring rules to use to measure the "skill" of probability forecasts.},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/PZTT4DUQ/Murphy - 1973 - Hedging and Skill Scores for Probability Forecasts.pdf}
}

@article{murphyImpactEnsembleForecasts1988,
  title = {The Impact of Ensemble Forecasts on Predictability},
  author = {Murphy, J. M.},
  year = {1988},
  journal = {Quarterly Journal of the Royal Meteorological Society},
  volume = {114},
  number = {480},
  pages = {463--493},
  issn = {1477-870X},
  doi = {10.1002/qj.49711448010},
  urldate = {2021-06-01},
  abstract = {An estimate of the mean effect of ensemble-averaging on forecast skill, under idealized `perfect model' conditions, is obtained from a set of eight independent 50-day winter ensemble forecast experiments made with a hemispheric version of the Meteorological Office (UKMO) 5-level general circulation model. Each ensemble forecast consisted of seven individual integrations. Initial conditions for these were obtained by adding spatially correlated perturbations to a given wintertime analysis, and a further integration created in the same manner was used to represent nature, giving the perfect model approach. The ensemble-mean forecast shows a clear improvement in amplitude and phase skill compared with individual forecasts, the period of significant predictability for daily fields being increased by 50\%. The improvement in skill is consistent with simple theoretical estimates based on the perfect model assumption. These calculations are used to deduce how ensemble-mean forecast skill should vary with the size of ensemble. The superiority of the ensemble-mean is maintained when forecasts are spatially smoothed or time-averaged. The spread of an ensemble distribution can in principle give an a priori indication of forecast skill. A moderate level of correlation between ensemble spread and the forecast skill of the ensemble-mean is found on the hemispheric scale. The extent to which the potential benefits of ensemble forecasting may be achieved in reality depends on the model's practical forecast skill. Since the practical skill of the 5-level model is rather low, an ensemble-mean forecast is on average no better than an individual forecast up to the normal limit of deterministic predictability. However, in four experiments where the individual forecasts show skill beyond this point, the ensemble-mean forecast does give increased skill. Spatial variations in both the practical and perfect model skills of an ensemble-mean anomaly field are found to be related to corresponding variations in the statistical significance of the anomaly field. For example, the average perfect model skill, in regions where the ensemble-mean anomaly is significantly different from zero, exceeds the full field skill in all experiments for forecast days 1--15, and in all but two cases for days 16--30.},
  copyright = {Copyright {\copyright} 1988 Royal Meteorological Society},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/7MTXUGZU/Murphy - 1988 - The impact of ensemble forecasts on predictability.pdf;/Users/nikos/github-synced/zotero-nikos/storage/JYU9TEKU/qj.html}
}

@article{murphyNoteRankedProbability1971,
  title = {A {{Note}} on the {{Ranked Probability Score}}},
  author = {Murphy, Allan H.},
  year = {1971},
  month = feb,
  journal = {Journal of Applied Meteorology},
  volume = {10},
  number = {1},
  pages = {155--156},
  publisher = {American Meteorological Society},
  issn = {0021-8952},
  doi = {10.1175/1520-0450(1971)010<0155:ANOTRP>2.0.CO;2},
  urldate = {2020-08-13},
  langid = {english},
  keywords = {ranked probability score,RPS},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/K7IF4UNP/Murphy - 1971 - A Note on the Ranked Probability Score.pdf;/Users/nikos/github-synced/zotero-nikos/storage/PRWJQBMK/A-Note-on-the-Ranked-Probability-Score.html}
}

@article{murphyNoteRankedProbability1971a,
  title = {A {{Note}} on the {{Ranked Probability Score}}},
  author = {Murphy, Allan H.},
  year = {1971},
  month = feb,
  journal = {Journal of Applied Meteorology and Climatology},
  volume = {10},
  number = {1},
  pages = {155--156},
  publisher = {American Meteorological Society},
  issn = {1520-0450},
  doi = {10.1175/1520-0450(1971)010<0155:ANOTRP>2.0.CO;2},
  urldate = {2022-03-14},
  abstract = {Abstract},
  chapter = {Journal of Applied Meteorology and Climatology},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/G4ZUDF6B/Murphy - 1971 - A Note on the Ranked Probability Score.pdf;/Users/nikos/github-synced/zotero-nikos/storage/DA47KK7K/1520-0450_1971_010_0155_anotrp_2_0_co_2.html}
}

@article{oidtmanTradeoffsIndividualEnsemble2021,
  title = {Trade-Offs between Individual and Ensemble Forecasts of an Emerging Infectious Disease},
  author = {Oidtman, Rachel J. and Omodei, Elisa and Kraemer, Moritz U. G. and {Casta{\~n}eda-Orjuela}, Carlos A. and {Cruz-Rivera}, Erica and {Misnaza-Castrill{\'o}n}, Sandra and Cifuentes, Myriam Patricia and Rincon, Luz Emilse and Ca{\~n}on, Viviana and de Alarcon, Pedro and Espa{\~n}a, Guido and Huber, John H. and Hill, Sarah C. and Barker, Christopher M. and Johansson, Michael A. and Manore, Carrie A. and Reiner, Robert C. and {Rodriguez-Barraquer}, Isabel and Siraj, Amir S. and {Frias-Martinez}, Enrique and {Garc{\'i}a-Herranz}, Manuel and Perkins, T. Alex},
  year = {2021},
  month = mar,
  journal = {medRxiv},
  pages = {2021.02.25.21252363},
  publisher = {Cold Spring Harbor Laboratory Press},
  doi = {10.1101/2021.02.25.21252363},
  urldate = {2021-03-10},
  abstract = {{$<$}p{$>$}When new pathogens emerge, numerous questions arise about their future spread, some of which can be addressed with probabilistic forecasts. The many uncertainties about the epidemiology of emerging pathogens can make it difficult to choose among model structures and assumptions, however. To assess the potential for uncertainties about emerging pathogens to affect forecasts of their spread, we evaluated the performance of a suite of 16 forecasting models in the context of the 2015-2016 Zika epidemic in Colombia. Each model featured a different combination of assumptions about the role of human mobility in driving transmission, spatiotemporal variation in transmission potential, and the number of times the virus was introduced. All models used the same core transmission model and the same iterative data assimilation algorithm to generate forecasts. By assessing forecast performance through time using logarithmic scoring with ensemble weighting, we found that which model assumptions had the most ensemble weight changed through time. In particular, spatially coupled models had higher ensemble weights in the early and late phases of the epidemic, whereas non-spatial models had higher ensemble weights at the peak of the epidemic. We compared forecast performance of the equally-weighted ensemble model to each individual model and identified a trade-off whereby certain individual models outperformed the ensemble model early in the epidemic but the ensemble model outperformed all individual models on average. On balance, our results suggest that suites of models that span uncertainty across alternative assumptions are necessary to obtain robust forecasts in the context of emerging infectious diseases.{$<$}/p{$>$}},
  copyright = {{\copyright} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/4P9NALWV/Oidtman et al. - 2021 - Trade-offs between individual and ensemble forecas.pdf;/Users/nikos/github-synced/zotero-nikos/storage/E2FPME9E/2021.02.25.html;/Users/nikos/github-synced/zotero-nikos/storage/UAULNVQ4/2021.02.25.html}
}

@misc{ourworldindataCOVID19DataExplorer2020,
  title = {{{COVID-19 Data Explorer}}},
  author = {{Our World in Data}},
  year = {2020},
  journal = {Our World in Data},
  urldate = {2021-05-30},
  abstract = {Research and data to make progress against the world's largest problems},
  howpublished = {https://ourworldindata.org/coronavirus-data-explorer},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/LUGP5UBF/coronavirus-data-explorer.html}
}

@article{pagePrescribingAustraliansLiving2015,
  title = {Prescribing for {{Australians}} Living with Dementia: Study Protocol Using the {{Delphi}} Technique},
  shorttitle = {Prescribing for {{Australians}} Living with Dementia},
  author = {Page, Amy and Potter, Kathleen and Clifford, Rhonda and McLachlan, Andrew and {Etherton-Beer}, Christopher},
  year = {2015},
  month = aug,
  journal = {BMJ Open},
  volume = {5},
  number = {8},
  pages = {e008048},
  issn = {2044-6055, 2044-6055},
  doi = {10.1136/bmjopen-2015-008048},
  urldate = {2023-02-07},
  abstract = {Introduction: Prescribing is complicated for people living with dementia, and careful consideration should be given to continuing and initiating all medicines. This study aims to elicit opinion and gain consensus on appropriate medicine use for people living with dementia in Australia to create a consensus-based list of explicit prescribing criteria. Methods and analysis: A Delphi technique will be used to develop explicit criteria of medication use in adults aged 65 years and above. An interdisciplinary panel of Australian experts in geriatric therapeutics will be convened that will consist of a minimum of 10 participants. To develop the consensus-based criteria, this study will use an iterative, anonymous, multistaged approach with controlled feedback. Round 1 questionnaire will be administered, and subsequently qualitatively analysed. The round 1 results will be fed back to the panel members, and a round 2 questionnaire developed using questions on a five-point Likert scale. This process will repeat until consensus is developed, or diminishing returns are noted. Ethics and dissemination: All participants will be provided with a participant information sheet, and sign a written consent form. Ethical approval has been granted from the University of Western Australia's Human Research Ethics Committee (HREC) (reference: RA/4/1/7172). We expect that data from this study will result in a paper published in a peerreviewed clinical journal and will also present the results at conferences.},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/G524RNFH/Page et al. - 2015 - Prescribing for Australians living with dementia .pdf}
}

@misc{paragAreEpidemicGrowth2021,
  title = {Are Epidemic Growth Rates More Informative than Reproduction Numbers?},
  author = {Parag, Kris V. and Thompson, Robin N. and Donnelly, Christl A.},
  year = {2021},
  month = apr,
  pages = {2021.04.15.21255565},
  institution = {medRxiv},
  doi = {10.1101/2021.04.15.21255565},
  urldate = {2022-01-25},
  abstract = {Summary statistics, often derived from simplified models of epidemic spread, inform public health policy in real time. The instantaneous reproduction number, Rt, is predominant among these statistics, measuring the average ability of an infection to multiply. However, Rt encodes no temporal information and is sensitive to modelling assumptions. Consequently, some have proposed the epidemic growth rate, rt, i.e., the rate of change of the log-transformed case incidence, as a more temporally meaningful and model-agnostic policy guide. We examine this assertion, identifying if and when estimates of rt are more informative than those of Rt. We assess their relative strengths both for learning about pathogen transmission mechanisms and for guiding epidemic interventions in real time.},
  copyright = {{\copyright} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/X95L2ZIY/Parag et al. - 2021 - Are epidemic growth rates more informative than re.pdf;/Users/nikos/github-synced/zotero-nikos/storage/G7NSADFA/2021.04.15.html}
}

@article{paviaElectionForecastsUsing2008,
  title = {Election {{Forecasts Using Spatiotemporal Models}}},
  author = {Pav{\'i}a, Jose Manuel and Larraz, Beatriz and Montero, Jose Mar{\'i}},
  year = {2008},
  month = sep,
  journal = {Journal of the American Statistical Association},
  volume = {103},
  number = {483},
  pages = {1050--1059},
  issn = {0162-1459, 1537-274X},
  doi = {10.1198/016214507000001427},
  urldate = {2021-10-21},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/KZE7UJJY/Pav√≠a et al. - 2008 - Election Forecasts Using Spatiotemporal Models.pdf}
}

@article{pellisChallengesControlCOVID192021,
  title = {Challenges in Control of {{COVID-19}}: Short Doubling Time and Long Delay to Effect of Interventions},
  shorttitle = {Challenges in Control of {{COVID-19}}},
  author = {Pellis, Lorenzo and Scarabel, Francesca and Stage, Helena B. and Overton, Christopher E. and Chappell, Lauren H. K. and Fearon, Elizabeth and Bennett, Emma and Lythgoe, Katrina A. and House, Thomas A. and Hall, Ian and {null}, null},
  year = {2021},
  month = jul,
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {376},
  number = {1829},
  pages = {20200264},
  publisher = {Royal Society},
  doi = {10.1098/rstb.2020.0264},
  urldate = {2022-11-29},
  abstract = {Early assessments of the growth rate of COVID-19 were subject to significant uncertainty, as expected with limited data and difficulties in case ascertainment, but as cases were recorded in multiple countries, more robust inferences could be made. Using multiple countries, data streams and methods, we estimated that, when unconstrained, European COVID-19 confirmed cases doubled on average every 3 days (range 2.2--4.3 days) and Italian hospital and intensive care unit admissions every 2--3 days; values that are significantly lower than the 5--7 days dominating the early published literature. Furthermore, we showed that the impact of physical distancing interventions was typically not seen until at least 9 days after implementation, during which time confirmed cases could grow eightfold. We argue that such temporal patterns are more critical than precise estimates of the time-insensitive basic reproduction number R0 for initiating interventions, and that the combination of fast growth and long detection delays explains the struggle in countries' outbreak response better than large values of R0 alone. One year on from first reporting these results, reproduction numbers continue to dominate the media and public discourse, but robust estimates of unconstrained growth remain essential for planning worst-case scenarios, and detection delays are still key in informing the relaxation and re-implementation of interventions. This article is part of the theme issue `Modelling that shaped the early COVID-19 pandemic response in the UK'.},
  keywords = {early growth rate,incubation period,non-pharmaceutical interventions,onset-to-hospitalization delay,reproduction number,unconstrained epidemic},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/FLM9YWNA/Pellis et al. - 2021 - Challenges in control of COVID-19 short doubling .pdf}
}

@article{perez-guzmanEpidemiologicalDriversTransmissibility2023,
  title = {Epidemiological Drivers of Transmissibility and Severity of {{SARS-CoV-2}} in {{England}}},
  author = {{Perez-Guzman}, Pablo N. and Knock, Edward and Imai, Natsuko and Rawson, Thomas and Elmaci, Yasin and Alcada, Joana and Whittles, Lilith K. and Thekke Kanapram, Divya and Sonabend, Raphael and Gaythorpe, Katy A. M. and Hinsley, Wes and FitzJohn, Richard G. and Volz, Erik and Verity, Robert and Ferguson, Neil M. and Cori, Anne and Baguelin, Marc},
  year = {2023},
  month = jul,
  journal = {Nature Communications},
  volume = {14},
  number = {1},
  pages = {4279},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-023-39661-5},
  urldate = {2024-02-14},
  abstract = {As the SARS-CoV-2 pandemic progressed, distinct variants emerged and dominated in England. These variants, Wildtype, Alpha, Delta, and Omicron were characterized by variations in transmissibility and severity. We used a robust mathematical model and Bayesian inference framework to analyse epidemiological surveillance data from England. We quantified the impact of non-pharmaceutical interventions (NPIs), therapeutics, and vaccination on virus transmission and severity. Each successive variant had a higher intrinsic transmissibility. Omicron (BA.1) had the highest basic reproduction number at 8.4 (95\% credible interval (CrI) 7.8-9.1). Varying levels of NPIs were crucial in controlling virus transmission until population immunity accumulated. Immune escape properties of Omicron decreased effective levels of immunity in the population by a third. Furthermore, in contrast to previous studies, we found Alpha had the highest basic infection fatality ratio (3.0\%, 95\% CrI 2.8-3.2), followed by Delta (2.1\%, 95\% CrI 1.9--2.4), Wildtype (1.2\%, 95\% CrI 1.1--1.2), and Omicron (0.7\%, 95\% CrI 0.6-0.8). Our findings highlight the importance of continued surveillance. Long-term strategies for monitoring and maintaining effective immunity against SARS-CoV-2 are critical to inform the role of NPIs to effectively manage future variants with potentially higher intrinsic transmissibility and severe outcomes.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Computational models,Epidemiology,Infectious diseases,SARS-CoV-2},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/89LKSD5S/Perez-Guzman et al. - 2023 - Epidemiological drivers of transmissibility and se.pdf}
}

@article{petropoulosWisdomDataGetting2021,
  title = {The {{Wisdom}} of the {{Data}}: {{Getting}} the {{Most Out}} of {{Univariate Time Series Forecasting}}},
  shorttitle = {The {{Wisdom}} of the {{Data}}},
  author = {Petropoulos, Fotios and Spiliotis, Evangelos},
  year = {2021},
  month = sep,
  journal = {Forecasting},
  volume = {3},
  number = {3},
  pages = {478--497},
  publisher = {Multidisciplinary Digital Publishing Institute},
  doi = {10.3390/forecast3030029},
  urldate = {2021-07-11},
  abstract = {Forecasting is a challenging task that typically requires making assumptions about the observed data but also the future conditions. Inevitably, any forecasting process will result in some degree of inaccuracy. The forecasting performance will further deteriorate as the uncertainty increases. In this article, we focus on univariate time series forecasting and we review five approaches that one can use to enhance the performance of standard extrapolation methods. Much has been written about the ``wisdom of the crowds'' and how collective opinions will outperform individual ones. We present the concept of the ``wisdom of the data'' and how data manipulation can result in information extraction which, in turn, translates to improved forecast accuracy by aggregating (combining) forecasts computed on different perspectives of the same data. We describe and discuss approaches that are based on the manipulation of local curvatures (theta method), temporal aggregation, bootstrapping, sub-seasonal and incomplete time series. We compare these approaches with regards to how they extract information from the data, their computational cost, and their performance.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {bagging,combination,information,sub-seasonal series,temporal aggregation,theta,uncertainty},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/NP44WTN5/Petropoulos and Spiliotis - 2021 - The Wisdom of the Data Getting the Most Out of Un.pdf;/Users/nikos/github-synced/zotero-nikos/storage/D7XRHTHB/29.html}
}

@techreport{powellSkewAdjustedExtremizedMeanSimple2022,
  type = {{{SSRN Scholarly Paper}}},
  title = {Skew-{{Adjusted Extremized-Mean}}: {{A Simple Method}} for {{Identifying}} and {{Learning From Contrarian Minorities}} in {{Groups}} of {{Forecasters}}},
  shorttitle = {Skew-{{Adjusted Extremized-Mean}}},
  author = {Powell, Ben and Satop{\"a}{\"a}, Ville and MacKay, Niall J. and Tetlock, Philip},
  year = {2022},
  month = jan,
  number = {ID 4004029},
  address = {Rochester, NY},
  institution = {Social Science Research Network},
  urldate = {2022-02-09},
  abstract = {Recent work in forecast aggregation has demonstrated that paying attention to contrarian minorities among larger groups of forecasters can improve aggregated probabilistic forecasts. In those papers, the minorities are identified using `meta-questions' that ask forecasters about their forecasting abilities or those of others. In the current paper, we explain how contrarian minorities can be identified without the meta-questions by inspecting the skewness of the distribution of the forecasts. Inspired by this observation, we introduce a new forecast aggregation tool called Skew-Adjusted Extremized-Mean and demonstrate its superior predictive power on a large set of geopolitical and general knowledge forecasting data.},
  langid = {english},
  keywords = {Ben Powell,Niall J. MacKay,Philip Tetlock,Skew-Adjusted Extremized-Mean: A Simple Method for Identifying and Learning From Contrarian Minorities in Groups of Forecasters,SSRN,Ville Satopaa},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/FJFLZPUR/Powell et al. - 2022 - Skew-Adjusted Extremized-Mean A Simple Method for.pdf;/Users/nikos/github-synced/zotero-nikos/storage/HG7BHZML/papers.html}
}

@misc{predictitPredictIt2021,
  title = {{{PredictIt}}},
  author = {{PredictIt}},
  year = {2021},
  urldate = {2021-10-13},
  howpublished = {https://www.predictit.org/},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/SRL8B8N4/www.predictit.org.html}
}

@misc{PsycNETRecordAccess,
  title = {{{PsycNET Record Access}} - {{PsycNET}}},
  urldate = {2021-07-29},
  abstract = {PsycNET Record Access page},
  howpublished = {https://doi.apa.org/recordAccess/institutional/2000-07311-003?returnUrl=https\%253A\%252F\%252Fdoi.apa.org\%252FdoiLanding\%253Fdoi\%253D10.1037\%25252F1040-3590.12.1.19},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/4BBTL6NT/2000-07311-003.html}
}

@article{ramosProbabilisticForecastsLead2013,
  title = {Do Probabilistic Forecasts Lead to Better Decisions?},
  author = {Ramos, M. H. and {van Andel}, S. J. and Pappenberger, F.},
  year = {2013},
  month = jun,
  journal = {Hydrology and Earth System Sciences},
  volume = {17},
  number = {6},
  pages = {2219--2232},
  publisher = {Copernicus GmbH},
  issn = {1027-5606},
  doi = {10.5194/hess-17-2219-2013},
  urldate = {2024-03-17},
  abstract = {The last decade has seen growing research in producing probabilistic hydro-meteorological forecasts and increasing their reliability. This followed the promise that, supplied with information about uncertainty, people would take better risk-based decisions. In recent years, therefore, research and operational developments have also started focusing attention on ways of communicating the probabilistic forecasts to decision-makers. Communicating probabilistic forecasts includes preparing tools and products for visualisation, but also requires understanding how decision-makers perceive and use uncertainty information in real time. At the EGU General Assembly 2012, we conducted a laboratory-style experiment in which several cases of flood forecasts and a choice of actions to take were presented as part of a game to participants, who acted as decision-makers. Answers were collected and analysed. In this paper, we present the results of this exercise and discuss if we indeed make better decisions on the basis of probabilistic forecasts.},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/KIJGZLRR/Ramos et al. - 2013 - Do probabilistic forecasts lead to better decision.pdf}
}

@article{rayComparingTrainedUntrained2022,
  title = {Comparing Trained and Untrained Probabilistic Ensemble Forecasts of {{COVID-19}} Cases and Deaths in the {{United States}}},
  author = {Ray, Evan L. and Brooks, Logan C. and Bien, Jacob and Biggerstaff, Matthew and Bosse, Nikos I. and Bracher, Johannes and Cramer, Estee Y. and Funk, Sebastian and Gerding, Aaron and Johansson, Michael A. and Rumack, Aaron and Wang, Yijin and Zorn, Martha and Tibshirani, Ryan J. and Reich, Nicholas G.},
  year = {2022},
  month = jul,
  journal = {International Journal of Forecasting},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2022.06.005},
  urldate = {2023-04-09},
  abstract = {The U.S. COVID-19 Forecast Hub aggregates forecasts of the short-term burden of COVID-19 in the United States from many contributing teams. We study methods for building an ensemble that combines forecasts from these teams. These experiments have informed the ensemble methods used by the Hub. To be most useful to policymakers, ensemble forecasts must have stable performance in the presence of two key characteristics of the component forecasts: (1) occasional misalignment with the reported data, and (2) instability in the relative performance of component forecasters over time. Our results indicate that in the presence of these challenges, an untrained and robust approach to ensembling using an equally weighted median of all component forecasts is a good choice to support public health decision-makers. In settings where some contributing forecasters have a stable record of good performance, trained ensembles that give those forecasters higher weight can also be helpful.},
  langid = {english},
  keywords = {COVID-19,Ensemble,Epidemiology,Health forecasting,Quantile combination},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/ECEBZVLE/Ray et al. - 2022 - Comparing trained and untrained probabilistic ense.pdf;/Users/nikos/github-synced/zotero-nikos/storage/D5VAGPM4/S0169207022000966.html}
}

@article{recchiaHowWellDid2021,
  title = {How Well Did Experts and Laypeople Forecast the Size of the {{COVID-19}} Pandemic?},
  author = {Recchia, Gabriel and Freeman, Alexandra L. J. and Spiegelhalter, David},
  year = {2021},
  month = may,
  journal = {PLOS ONE},
  volume = {16},
  number = {5},
  pages = {e0250935},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0250935},
  urldate = {2021-06-02},
  abstract = {Throughout the COVID-19 pandemic, social and traditional media have disseminated predictions from experts and nonexperts about its expected magnitude. How accurate were the predictions of `experts'---individuals holding occupations or roles in subject-relevant fields, such as epidemiologists and statisticians---compared with those of the public? We conducted a survey in April 2020 of 140 UK experts and 2,086 UK laypersons; all were asked to make four quantitative predictions about the impact of COVID-19 by 31 Dec 2020. In addition to soliciting point estimates, we asked participants for lower and higher bounds of a range that they felt had a 75\% chance of containing the true answer. Experts exhibited greater accuracy and calibration than laypersons, even when restricting the comparison to a subset of laypersons who scored in the top quartile on a numeracy test. Even so, experts substantially underestimated the ultimate extent of the pandemic, and the mean number of predictions for which the expert intervals contained the actual outcome was only 1.8 (out of 4), suggesting that experts should consider broadening the range of scenarios they consider plausible. Predictions of the public were even more inaccurate and poorly calibrated, suggesting that an important role remains for expert predictions as long as experts acknowledge their uncertainty.},
  langid = {english},
  keywords = {COVID 19,Epidemiological statistics,Forecasting,Numeracy,Pandemics,Probability distribution,Statistical distributions,Virus testing},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/X2FMBK56/Recchia et al. - 2021 - How well did experts and laypeople forecast the si.pdf;/Users/nikos/github-synced/zotero-nikos/storage/TT7K727A/article.html}
}

@article{reichAccuracyRealtimeMultimodel2019,
  title = {Accuracy of Real-Time Multi-Model Ensemble Forecasts for Seasonal Influenza in the {{U}}.{{S}}.},
  author = {Reich, Nicholas G. and McGowan, Craig J. and Yamana, Teresa K. and Tushar, Abhinav and Ray, Evan L. and Osthus, Dave and Kandula, Sasikiran and Brooks, Logan C. and {Crawford-Crudell}, Willow and Gibson, Graham Casey and Moore, Evan and Silva, Rebecca and Biggerstaff, Matthew and Johansson, Michael A. and Rosenfeld, Roni and Shaman, Jeffrey},
  year = {2019},
  month = nov,
  journal = {PLOS Computational Biology},
  volume = {15},
  number = {11},
  pages = {e1007486},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1007486},
  urldate = {2020-08-07},
  abstract = {Seasonal influenza results in substantial annual morbidity and mortality in the United States and worldwide. Accurate forecasts of key features of influenza epidemics, such as the timing and severity of the peak incidence in a given season, can inform public health response to outbreaks. As part of ongoing efforts to incorporate data and advanced analytical methods into public health decision-making, the United States Centers for Disease Control and Prevention (CDC) has organized seasonal influenza forecasting challenges since the 2013/2014 season. In the 2017/2018 season, 22 teams participated. A subset of four teams created a research consortium called the FluSight Network in early 2017. During the 2017/2018 season they worked together to produce a collaborative multi-model ensemble that combined 21 separate component models into a single model using a machine learning technique called stacking. This approach creates a weighted average of predictive densities where the weight for each component is determined by maximizing overall ensemble accuracy over past seasons. In the 2017/2018 influenza season, one of the largest seasonal outbreaks in the last 15 years, this multi-model ensemble performed better on average than all individual component models and placed second overall in the CDC challenge. It also outperformed the baseline multi-model ensemble created by the CDC that took a simple average of all models submitted to the forecasting challenge. This project shows that collaborative efforts between research teams to develop ensemble forecasting approaches can bring measurable improvements in forecast accuracy and important reductions in the variability of performance from year to year. Efforts such as this, that emphasize real-time testing and evaluation of forecasting models and facilitate the close collaboration between public health officials and modeling researchers, are essential to improving our understanding of how best to use forecasts to improve public health response to seasonal and emerging epidemic threats.},
  langid = {english},
  keywords = {Body weight,ensemble,Epidemiology,forecast,Forecasting,Infectious disease surveillance,Infectious diseases,Influenza,prediction,Public and occupational health,Seasons},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/IJEAL83T/Reich et al. - 2019 - Accuracy of real-time multi-model ensemble forecas.pdf;/Users/nikos/github-synced/zotero-nikos/storage/A2M4D7R7/article.html}
}

@article{reichCollaborativeMultiyearMultimodel2019,
  title = {A Collaborative Multiyear, Multimodel Assessment of Seasonal Influenza Forecasting in the {{United States}}},
  author = {Reich, Nicholas G. and Brooks, Logan C. and Fox, Spencer J. and Kandula, Sasikiran and McGowan, Craig J. and Moore, Evan and Osthus, Dave and Ray, Evan L. and Tushar, Abhinav and Yamana, Teresa K. and Biggerstaff, Matthew and Johansson, Michael A. and Rosenfeld, Roni and Shaman, Jeffrey},
  year = {2019},
  month = feb,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {116},
  number = {8},
  pages = {3146--3154},
  publisher = {National Academy of Sciences},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1812594116},
  urldate = {2021-10-13},
  abstract = {Influenza infects an estimated 9--35 million individuals each year in the United States and is a contributing cause for between 12,000 and 56,000 deaths annually. Seasonal outbreaks of influenza are common in temperate regions of the world, with highest incidence typically occurring in colder and drier months of the year. Real-time forecasts of influenza transmission can inform public health response to outbreaks. We present the results of a multiinstitution collaborative effort to standardize the collection and evaluation of forecasting models for influenza in the United States for the 2010/2011 through 2016/2017 influenza seasons. For these seven seasons, we assembled weekly real-time forecasts of seven targets of public health interest from 22 different models. We compared forecast accuracy of each model relative to a historical baseline seasonal average. Across all regions of the United States, over half of the models showed consistently better performance than the historical baseline when forecasting incidence of influenza-like illness 1 wk, 2 wk, and 3 wk ahead of available data and when forecasting the timing and magnitude of the seasonal peak. In some regions, delays in data reporting were strongly and negatively associated with forecast accuracy. More timely reporting and an improved overall accessibility to novel and traditional data sources are needed to improve forecasting accuracy and its integration with real-time public health decision making.},
  chapter = {PNAS Plus},
  copyright = {Copyright {\copyright} 2019 the Author(s). Published by PNAS.. https://creativecommons.org/licenses/by-nc-nd/4.0/This open access article is distributed under Creative Commons Attribution-NonCommercial-NoDerivatives License 4.0 (CC BY-NC-ND).},
  langid = {english},
  pmid = {30647115},
  keywords = {forecasting,infectious disease,influenza,public health,statistics},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/XEKLR37W/Reich et al. - 2019 - A collaborative multiyear, multimodel assessment o.pdf;/Users/nikos/github-synced/zotero-nikos/storage/QID3PLW4/3146.html}
}

@article{renIncorporatingGenuinePrior2018,
  title = {Incorporating {{Genuine Prior Information}} about {{Between-Study Heterogeneity}} in {{Random Effects Pairwise}} and {{Network Meta-analyses}}},
  author = {Ren, Shijie and Oakley, Jeremy E. and Stevens, John W.},
  year = {2018},
  month = may,
  journal = {Medical Decision Making},
  volume = {38},
  number = {4},
  pages = {531--542},
  issn = {0272-989X, 1552-681X},
  doi = {10.1177/0272989X18759488},
  urldate = {2021-11-06},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/8KT44KZ9/Ren et al. - 2018 - Incorporating Genuine Prior Information about Betw.pdf}
}

@misc{replicationmarketsReplicationMarketsReliable2020,
  title = {Replication {{Markets}} -- {{Reliable}} Research Replicates{\dots}you Can Bet on It.},
  author = {{ReplicationMarkets}},
  year = {2020},
  urldate = {2021-10-13},
  langid = {american},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/SFC8YA78/www.replicationmarkets.com.html}
}

@article{riutort-mayolPracticalHilbertSpace2022,
  title = {Practical {{Hilbert}} Space Approximate {{Bayesian Gaussian}} Processes for Probabilistic Programming},
  author = {{Riutort-Mayol}, Gabriel and B{\"u}rkner, Paul-Christian and Andersen, Michael R. and Solin, Arno and Vehtari, Aki},
  year = {2022},
  month = dec,
  journal = {Statistics and Computing},
  volume = {33},
  number = {1},
  pages = {17},
  issn = {1573-1375},
  doi = {10.1007/s11222-022-10167-2},
  urldate = {2023-04-10},
  abstract = {Gaussian processes are powerful non-parametric probabilistic models for stochastic functions. However, the direct implementation entails a complexity that is computationally intractable when the number of observations is large, especially when estimated with fully Bayesian methods such as Markov chain Monte Carlo. In this paper, we focus on a low-rank approximate Bayesian Gaussian processes, based on a basis function approximation via Laplace eigenfunctions for stationary covariance functions. The main contribution of this paper is a detailed analysis of the performance, and practical recommendations for how to select the number of basis functions and the boundary factor. Intuitive visualizations and recommendations, make it easier for users to improve approximation accuracy and computational performance. We also propose diagnostics for checking that the number of basis functions and the boundary factor are adequate given the data. The approach is simple and exhibits an attractive computational complexity due to its linear structure, and it is easy to implement in probabilistic programming frameworks. Several illustrative examples of the performance and applicability of the method in the probabilistic programming language Stan are presented together with the underlying Stan model code.},
  langid = {english},
  keywords = {Bayesian statistics,Gaussian process,Hilbert space methods,Low-rank Gaussian process,Sparse Gaussian process,Stan},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/JBR38DMU/Riutort-Mayol et al. - 2022 - Practical Hilbert space approximate Bayesian Gauss.pdf}
}

@article{rizzoEnergyDistance2016,
  title = {Energy Distance},
  author = {Rizzo, Maria L. and Sz{\'e}kely, G{\'a}bor J.},
  year = {2016},
  journal = {WIREs Computational Statistics},
  volume = {8},
  number = {1},
  pages = {27--38},
  issn = {1939-0068},
  doi = {10.1002/wics.1375},
  urldate = {2021-07-21},
  abstract = {Energy distance is a metric that measures the distance between the distributions of random vectors. Energy distance is zero if and only if the distributions are identical, thus it characterizes equality of distributions and provides a theoretical foundation for statistical inference and analysis. Energy statistics are functions of distances between observations in metric spaces. As a statistic, energy distance can be applied to measure the difference between a sample and a hypothesized distribution or the difference between two or more samples in arbitrary, not necessarily equal dimensions. The name energy is inspired by the close analogy with Newton's gravitational potential energy. Applications include testing independence by distance covariance, goodness-of-fit, nonparametric tests for equality of distributions and extension of analysis of variance, generalizations of clustering algorithms, change point analysis, feature selection, and more. WIREs Comput Stat 2016, 8:27--38. doi: 10.1002/wics.1375 This article is categorized under: Statistical and Graphical Methods of Data Analysis {$>$} Multivariate Analysis Statistical and Graphical Methods of Data Analysis {$>$} Nonparametric Methods},
  langid = {english},
  keywords = {DISCO,distance correlation,goodness-of-fit,independence,Multivariate},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/FPXAF5PX/wics.html}
}

@misc{RKICoronavirusSARSCoV2,
  title = {{{RKI}} - {{Coronavirus SARS-CoV-2}} - {{Archiv}} Der {{Situationsberichte}} Des {{Robert Koch-Instituts}} Zu {{COVID-19}} (Ab 4.3.2020)},
  urldate = {2021-05-30},
  howpublished = {https://www.rki.de/DE/Content/InfAZ/N/Neuartiges\_Coronavirus/Situationsberichte/Archiv.html},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/9YXYCWXU/Archiv.html}
}

@misc{RKICoronavirusSARSCoV2b,
  title = {{{RKI}} - {{Coronavirus SARS-CoV-2}} - {{November}} 2020: {{Archiv}} Der {{Situationsberichte}} Des {{Robert Koch-Instituts}} Zu {{COVID-19}}},
  urldate = {2021-05-30},
  howpublished = {https://www.rki.de/DE/Content/InfAZ/N/Neuartiges\_Coronavirus/Situationsberichte/Nov\_2020/Archiv\_November.html}
}

@misc{rkiRKICoronavirusSARSCoV22021,
  title = {{{RKI}} - {{Coronavirus SARS-CoV-2}} - {{Aktueller Lage-}}/{{Situationsbericht}} Des {{RKI}} Zu {{COVID-19}}},
  author = {{RKI}},
  year = {2021},
  urldate = {2021-05-30},
  howpublished = {https://www.rki.de/DE/Content/InfAZ/N/Neuartiges\_Coronavirus/Situationsberichte/Gesamt.html}
}

@article{RoadmapOutLockdown,
  title = {Roadmap out of Lockdown},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/JNPF2W66/Roadmap out of lockdown.pdf}
}

@article{romanoConformalizedQuantileRegression,
  title = {Conformalized {{Quantile Regression}}},
  author = {Romano, Yaniv and Patterson, Evan and Candes, Emmanuel},
  pages = {11},
  abstract = {Conformal prediction is a technique for constructing prediction intervals that attain valid coverage in finite samples, without making distributional assumptions. Despite this appeal, existing conformal methods can be unnecessarily conservative because they form intervals of constant or weakly varying length across the input space. In this paper we propose a new method that is fully adaptive to heteroscedasticity. It combines conformal prediction with classical quantile regression, inheriting the advantages of both. We establish a theoretical guarantee of valid coverage, supplemented by extensive experiments on popular regression datasets. We compare the efficiency of conformalized quantile regression to other conformal methods, showing that our method tends to produce shorter intervals.},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/RIAT42FV/Romano et al. - Conformalized Quantile Regression.pdf}
}

@article{romanoMaliceNoneAssessing2020,
  title = {With {{Malice Toward None}}: {{Assessing Uncertainty}} via {{Equalized Coverage}}},
  shorttitle = {With {{Malice Toward None}}},
  author = {Romano, Yaniv and Barber, Rina Foygel and Sabatti, Chiara and Cand{\`e}s, Emmanuel},
  year = {2020},
  month = apr,
  journal = {Harvard Data Science Review},
  volume = {2},
  number = {2},
  doi = {10.1162/99608f92.03f00592},
  urldate = {2021-10-28},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/L7Q5GIJI/Romano et al. - 2020 - With Malice Toward None Assessing Uncertainty via.pdf}
}

@article{romanoMaliceNoneAssessing2020a,
  title = {With {{Malice Toward None}}: {{Assessing Uncertainty}} via {{Equalized Coverage}}},
  shorttitle = {With {{Malice Toward None}}},
  author = {Romano, Yaniv and Barber, Rina Foygel and Sabatti, Chiara and Cand{\`e}s, Emmanuel},
  year = {2020},
  month = apr,
  journal = {Harvard Data Science Review},
  volume = {2},
  number = {2},
  doi = {10.1162/99608f92.03f00592},
  urldate = {2021-11-07},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/528S74VU/Romano et al. - 2020 - With Malice Toward None Assessing Uncertainty via.pdf}
}

@misc{rumackRecalibratingProbabilisticForecasts,
  title = {Recalibrating Probabilistic Forecasts of Epidemics},
  author = {Rumack, Aaron and Rosenfeld, Roni and Tibshirani, Ryan J.},
  doi = {10.1016/j.ijforecast.2021.11.001},
  urldate = {2022-01-28},
  howpublished = {https://reader.elsevier.com/reader/sd/pii/S0169207021001758?token=48FCE643427BEB3660FD0C16D61220C2D14E2079DE5A6E2B01154980653F65BD509977855816923463738722ACFD1B20\&originRegion=eu-west-1\&originCreation=20220128175248},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/HKGR7ZQW/Elsevier Enhanced Reader.pdf;/Users/nikos/github-synced/zotero-nikos/storage/YK3UTB95/S0169207021001758.html}
}

@article{rumackRecalibratingProbabilisticForecasts2021,
  title = {Recalibrating Probabilistic Forecasts of Epidemics},
  author = {Rumack, Aaron and Tibshirani, Ryan J. and Rosenfeld, Roni},
  year = {2021},
  month = dec,
  journal = {arXiv:2112.06305 [cs]},
  eprint = {2112.06305},
  primaryclass = {cs},
  urldate = {2022-04-13},
  abstract = {Distributional forecasts are important for a wide variety of applications, including forecasting epidemics. Often, forecasts are miscalibrated, or unreliable in assigning uncertainty to future events. We present a recalibration method that can be applied to a black-box forecaster given retrospective forecasts and observations, as well as an extension to make this method more effective in recalibrating epidemic forecasts. This method is guaranteed to improve calibration and log score performance when trained and measured in-sample. We also prove that the increase in expected log score of a recalibrated forecaster is equal to the entropy of the PIT distribution. We apply this recalibration method to the 27 influenza forecasters in the FluSight Network and show that recalibration reliably improves forecast accuracy and calibration. This method is effective, robust, and easy to use as a post-processing tool to improve epidemic forecasts.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/HPQK5IF2/Rumack et al. - 2021 - Recalibrating probabilistic forecasts of epidemics.pdf;/Users/nikos/github-synced/zotero-nikos/storage/YLG4EF4S/2112.html}
}

@article{sailynojaGraphicalTestDiscrete2021,
  title = {Graphical {{Test}} for {{Discrete Uniformity}} and Its {{Applications}} in {{Goodness}} of {{Fit Evaluation}} and {{Multiple Sample Comparison}}},
  author = {S{\"a}ilynoja, Teemu and B{\"u}rkner, Paul-Christian and Vehtari, Aki},
  year = {2021},
  month = nov,
  journal = {arXiv:2103.10522 [stat]},
  eprint = {2103.10522},
  primaryclass = {stat},
  urldate = {2022-01-25},
  abstract = {Assessing goodness of fit to a given distribution plays an important role in computational statistics. The Probability integral transformation (PIT) can be used to convert the question of whether a given sample originates from a reference distribution into a problem of testing for uniformity. We present new simulation and optimization based methods to obtain simultaneous confidence bands for the whole empirical cumulative distribution function (ECDF) of the PIT values under the assumption of uniformity. Simultaneous confidence bands correspond to such confidence intervals at each point that jointly satisfy a desired coverage. These methods can also be applied in cases where the reference distribution is represented only by a finite sample. The confidence bands provide an intuitive ECDF-based graphical test for uniformity, which also provides useful information on the quality of the discrepancy. We further extend the simulation and optimization methods to determine simultaneous confidence bands for testing whether multiple samples come from the same underlying distribution. This multiple sample comparison test is especially useful in Markov chain Monte Carlo convergence diagnostics. We provide numerical experiments to assess the properties of the tests using both simulated and real world data and give recommendations on their practical application in computational statistics workflows.},
  archiveprefix = {arxiv},
  keywords = {Statistics - Methodology},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/85F4KCSM/S√§ilynoja et al. - 2021 - Graphical Test for Discrete Uniformity and its App.pdf;/Users/nikos/github-synced/zotero-nikos/storage/6EJBQ7TH/2103.html}
}

@misc{ScoringEpidemiologicalForecasts,
  title = {Scoring Epidemiological Forecasts on Transformed Scales {\textbar} {{PLOS Computational Biology}}},
  urldate = {2023-11-16},
  howpublished = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1011393},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/ZF63XZW2/article.html}
}

@article{seoStatisticalPostprocessorAccounting2006,
  title = {A Statistical Post-Processor for Accounting of Hydrologic Uncertainty in Short-Range Ensemble Streamflow Prediction},
  author = {Seo, D.-J. and Herr, H. D. and Schaake, J. C.},
  year = {2006},
  month = aug,
  journal = {Hydrology and Earth System Sciences Discussions},
  volume = {3},
  number = {4},
  pages = {1987--2035},
  publisher = {Copernicus GmbH},
  issn = {1027-5606},
  doi = {10.5194/hessd-3-1987-2006},
  urldate = {2021-10-20},
  abstract = {{$<$}p{$><$}strong class="journal-contentHeaderColor"{$>$}Abstract.{$<$}/strong{$>$} In addition to the uncertainty in future boundary conditions of precipitation and temperature (i.e.\&nbsp;the meteorological uncertainty), parametric and structural uncertainties in the hydrologic models and uncertainty in the model initial conditions (i.e.\&nbsp;the hydrologic uncertainties) constitute a major source of error in hydrologic prediction. As such, accurate accounting of both meteorological and hydrologic uncertainties is critical to producing reliable probabilistic hydrologic prediction. In this paper, we describe and evaluate a statistical procedure that accounts for hydrologic uncertainty in short-range (1 to 5 days ahead) ensemble streamflow prediction (ESP). Referred to as the ESP post-processor, the procedure operates on ensemble traces of model-predicted streamflow that reflect only the meteorological uncertainty and produces post-processed ensemble traces that reflect both the meteorological and hydrologic uncertainties. A combination of probability matching and regression, the procedure is simple, parsimonious and robust. For a critical evaluation of the procedure, independent validation is carried out for five basins of the Juniata River in Pennsylvania, USA, under a very stringent setting. The results indicate that the post-processor is fully capable of producing ensemble traces that are unbiased in the mean and in the probabilistic sense. Due primarily to the uncertainties in the cumulative probability distributions (CDF) of observed and simulated flows, however, the unbiasedness may be compromised to a varying degree in real world situations. It is also shown, however, that the uncertainties in the CDF's do not significantly diminish the value of post-processed ensemble traces for decision making, and that probabilistic prediction based on post-processed ensemble traces significantly improves the value of single-value prediction at all ranges of flow.{$<$}/p{$>$}},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/BZTLYRI4/Seo et al. - 2006 - A statistical post-processor for accounting of hyd.pdf;/Users/nikos/github-synced/zotero-nikos/storage/5WQF3CUT/hessd-2006-0022.html}
}

@article{servan-schreiberPredictionMarketsDoes2004,
  title = {Prediction {{Markets}}: {{Does Money Matter}}?},
  shorttitle = {Prediction {{Markets}}},
  author = {{Servan-Schreiber}, Emile and Wolfers, Justin and Pennock, David M. and Galebach, Brian},
  year = {2004},
  month = sep,
  journal = {Electronic Markets},
  volume = {14},
  number = {3},
  pages = {243--251},
  issn = {1019-6781, 1422-8890},
  doi = {10.1080/1019678042000245254},
  urldate = {2021-10-13},
  abstract = {The accuracy of prediction markets has been documented both for markets based on real money and those based on play money. To test how much extra accuracy can be obtained by using real money versus play money, we set up a real-world online experiment pitting the predictions of TradeSports.com (real money) against those of NewsFutures.com (play money) regarding American Football outcomes during the 2003--2004 NFL season. As expected, both types of markets exhibited significant predictive powers, and remarkable performance compared to individual humans. But, perhaps surprisingly, the play-money markets performed as well as the real-money markets. We speculate that this result reflects two opposing forces: real-money markets may better motivate information discovery while play-money markets may yield more efficient information aggregation.},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/ET8A3PTR/Servan-Schreiber et al. - 2004 - Prediction Markets Does Money Matter.pdf}
}

@article{shaferTutorialConformalPrediction,
  title = {A {{Tutorial}} on {{Conformal Prediction}}},
  author = {Shafer, Glenn and Vovk, Vladimir},
  pages = {51},
  abstract = {Conformal prediction uses past experience to determine precise levels of confidence in new predictions. Given an error probability {$\varepsilon$}, together with a method that makes a prediction y{\textasciicircum} of a label y, it produces a set of labels, typically containing y{\textasciicircum}, that also contains y with probability 1 - {$\varepsilon$}. Conformal prediction can be applied to any method for producing y{\textasciicircum}: a nearest-neighbor method, a support-vector machine, ridge regression, etc.},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/D37CKLUP/Shafer and Vovk - A Tutorial on Conformal Prediction.pdf}
}

@inproceedings{shahIntroductionCompartmentalModels2021,
  title = {Introduction to {{Compartmental Models}} in {{Epidemiology}}},
  booktitle = {Mathematical {{Analysis}} for {{Transmission}} of {{COVID-19}}},
  author = {Shah, Nita H. and Mittal, Mandeep},
  editor = {Shah, Nita H. and Mittal, Mandeep},
  year = {2021},
  pages = {1--20},
  publisher = {Springer},
  address = {Singapore},
  doi = {10.1007/978-981-33-6264-2_1},
  abstract = {In this chapter, we discuss the basics of compartmental models in epidemiology and requisite analysis.},
  isbn = {978-981-336-264-2},
  langid = {english}
}

@article{shamanForecastingSeasonalOutbreaks2012,
  title = {Forecasting Seasonal Outbreaks of Influenza},
  author = {Shaman, Jeffrey and Karspeck, Alicia},
  year = {2012},
  month = dec,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {109},
  number = {50},
  pages = {20425--20430},
  publisher = {National Academy of Sciences},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1208772109},
  urldate = {2021-10-13},
  abstract = {Influenza recurs seasonally in temperate regions of the world; however, our ability to predict the timing, duration, and magnitude of local seasonal outbreaks of influenza remains limited. Here we develop a framework for initializing real-time forecasts of seasonal influenza outbreaks, using a data assimilation technique commonly applied in numerical weather prediction. The availability of real-time, web-based estimates of local influenza infection rates makes this type of quantitative forecasting possible. Retrospective ensemble forecasts are generated on a weekly basis following assimilation of these web-based estimates for the 2003--2008 influenza seasons in New York City. The findings indicate that real-time skillful predictions of peak timing can be made more than 7 wk in advance of the actual peak. In addition, confidence in those predictions can be inferred from the spread of the forecast ensemble. This work represents an initial step in the development of a statistically rigorous system for real-time forecast of seasonal influenza.},
  chapter = {Biological Sciences},
  langid = {english},
  pmid = {23184969},
  keywords = {absolute humidity,Kalman filter},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/D7479BD6/Shaman and Karspeck - 2012 - Forecasting seasonal outbreaks of influenza.pdf;/Users/nikos/github-synced/zotero-nikos/storage/627KTBNX/20425.html}
}

@article{sherrattEvaluatingUseReproduction2020,
  title = {Evaluating the Use of the Reproduction Number as an Epidemiological Tool, Using Spatio-Temporal Trends of the {{Covid-19}} Outbreak in {{England}}},
  author = {Sherratt, Katharine and Abbott, Sam and Meakin, Sophie R. and Hellewell, Joel and Munday, James D. and Bosse, Nikos and working Group, CMMID Covid-19 and Jit, Mark and Funk, Sebastian},
  year = {2020},
  month = oct,
  journal = {medRxiv},
  pages = {2020.10.18.20214585},
  publisher = {Cold Spring Harbor Laboratory Press},
  doi = {10.1101/2020.10.18.20214585},
  urldate = {2021-05-30},
  abstract = {{$<$}h3{$>$}Abstract{$<$}/h3{$>$} {$<$}p{$>$}The time-varying reproduction number (R\textsubscript{t}: the average number secondary infections caused by each infected person) may be used to assess changes in transmission potential during an epidemic. Since new infections usually are not observed directly, it can only be estimated from delayed and potentially biased data. We estimated R\textsubscript{t} using a model that mapped unobserved infections to observed test-positive cases, hospital admissions, and deaths with confirmed Covid-19, in seven regions of England over March through August 2020. We explored the sensitivity of R\textsubscript{t} estimates of Covid-19 in England to different data sources, and investigated the potential of using differences in the estimates to track epidemic dynamics in population sub-groups.{$<$}/p{$><$}p{$>$}Our estimates of transmission potential varied for each data source. The divergence between estimates from each source was not consistent within or across regions over time, although estimates based on hospital admissions and deaths were more spatio-temporally synchronous than compared to estimates from all test-positives. We compared differences in R\textsubscript{t} with the demographic and social context of transmission, and found the differences between R\textsubscript{t} may be linked to biased representations of sub-populations in each data source: from uneven testing rates, or increasing severity of disease with age, seen via outbreaks in care home populations and changing age distributions of cases.{$<$}/p{$><$}p{$>$}We highlight that policy makers should consider the source populations of R\textsubscript{t} estimates. Further work should clarify the best way to combine and interpret R\textsubscript{t} estimates from different data sources based on the desired use.{$<$}/p{$>$}},
  copyright = {{\copyright} 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/9Z8LVWSS/Sherratt et al. - 2020 - Evaluating the use of the reproduction number as a.pdf}
}

@article{sherrattExploringSurveillanceData2021,
  title = {Exploring Surveillance Data Biases When Estimating the Reproduction Number: With Insights into Subpopulation Transmission of {{Covid-19}} in {{England}}},
  shorttitle = {Exploring Surveillance Data Biases When Estimating the Reproduction Number},
  author = {Sherratt, Katharine and Abbott, Sam and Meakin, Sophie R. and Hellewell, Joel and Munday, James D. and Bosse, Nikos and working Group, CMMID Covid-19 and Jit, Mark and Funk, Sebastian},
  year = {2021},
  month = mar,
  pages = {2020.10.18.20214585},
  publisher = {Cold Spring Harbor Laboratory Press},
  doi = {10.1101/2020.10.18.20214585},
  urldate = {2021-10-14},
  abstract = {The time-varying reproduction number (Rt: the average number secondary infections caused by each infected person) may be used to assess changes in transmission potential during an epidemic. While new infections are not usually observed directly, they can be estimated from data. However, data may be delayed and potentially biased. We investigated the sensitivity of Rt estimates to different data sources representing Covid-19 in England, and we explored how this sensitivity could track epidemic dynamics in population sub-groups. We sourced public data on test-positive cases, hospital admissions, and deaths with confirmed Covid-19 in seven regions of England over March through August 2020. We estimated Rt using a model that mapped unobserved infections to each data source. We then compared differences in Rt with the demographic and social context of surveillance data over time. Our estimates of transmission potential varied for each data source, with the relative inconsistency of estimates varying across regions and over time. Rt estimates based on hospital admissions and deaths were more spatio-temporally synchronous than when compared to estimates from all test-positives. We found these differences may be linked to biased representations of subpopulations in each data source. These included spatially clustered testing, and where outbreaks in hospitals, care homes, and young age groups reflected the link between age and severity of disease. We highlight that policy makers could better target interventions by considering the source populations of Rt estimates. Further work should clarify the best way to combine and interpret Rt estimates from different data sources based on the desired use.},
  copyright = {{\copyright} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/Y8CPF3U3/Sherratt et al. - 2021 - Exploring surveillance data biases when estimating.pdf;/Users/nikos/github-synced/zotero-nikos/storage/HGFVXLM7/2020.10.18.html}
}

@article{sherrattPredictivePerformanceMultimodel2022,
  title = {Predictive Performance of Multi-Model Ensemble Forecasts of {{COVID-19}} across  {{European}} Nation},
  author = {Sherratt, K. and Gruson, H. and Grah, R. and Johnson, H. and Niehus, R. and Prasse, B. and Sandman, F. and Deuschel, J. and Wolffram, D. and Abbott, S. and Ullrich, A. and Gibson, G. and Ray, {\relax EL}. and Reich, {\relax NG}. and Sheldon, D. and Wang, Y. and Wattanachit, N. and Wang, L. and Trnka, J. and Obozinski, G. and Sun, T. and Thanou, D. and Pottier, L. and Krymova, E. and Barbarossa, {\relax MV}. and Leith{\"a}user, N. and Mohring, J. and Schneider, J. and Wlazlo, J. and Fuhrmann, J. and Lange, B. and Rodiah, I. and Baccam, P. and Gurung, H. and Stage, S. and Suchoski, B. and Budzinski, J. and Walraven, R. and Villanueva, I. and Tucek, V. and {\v S}m{\'i}d, M. and Zaj{\'i}cek, M. and P{\'e}rez, {\'A}lvarez C. and Reina, B. and Bosse, {\relax NI}. and Meakin, S. and Di Loro, Alaimo and Maruotti, A. and Eclerov{\'a}, V. and Kraus, A. and Kraus, D. and Pribylova, L. and Dimitris, B. and Li, {\relax ML}. and Saksham, S. and Dehning, J. and Mohr, S. and Priesemann, V. and Redlarski, G. and Bejar, B. and Ardenghi, G. and Parolini, N. and Ziarelli, G. and Bock, W. and Heyder, S. and Hotz, T. and E., Singh D. and {Guzman-Merino}, M. and Aznarte, {\relax JL}. and Mori{\~n}a, D. and Alonso, S. and {\'A}lvarez, E. and L{\'o}pez, D. and Prats, C. and Burgard, {\relax JP}. and Rodloff, A. and Zimmermann, T. and Kuhlmann, A. and Zibert, J. and Pennoni, F. and Divino, F. and Catal{\`a}, M. and Lovison, G. and Giudici, P. and Tarantino, B. and Bartolucci, F. and Jona, Lasinio G. and Mingione, M. and Farcomeni, A. and Srivastava, A. and {Montero-Manso}, P. and Adiga, A. and Hurt, B. and Lewis, B. and Marathe, M. and Porebski, P. and Venkatramanan, S. and Bartczuk, R. and Dreger, F. and Gambin, A. and Gogolewski, K. and {Gruziel-Slomka}, M. and Krupa, B. and Moszynski, A. and Niedzielewski, K. and Nowosielski, J. and Radwan, M. and Rakowski, F. and Semeniuk, M. and Szczurek, E. and Zielinski, J. and Kisielewski, J. and Pabjan, B. and Holger, K. and Kheifetz, Y. and Scholz, M. and Bodych, M. and Filinski, M. and Idzikowski, R. and Krueger, T. and Ozanski, T. and Bracher, J. and Funk, S.},
  year = {2022},
  journal = {Europe PMC},
  doi = {10.1101/2022.06.16.22276024},
  urldate = {2023-02-07},
  abstract = {Background  Short-term forecasts of infectious disease burden can contribute to  situational awareness and aid capacity planning. Based on best practice in  other fields and recent insights in infectious disease epidemiology, one can  maximise the predictive performance of such forecasts if multiple models are  combined into an ensemble. Here we report on the performance of ensembles in  predicting COVID-19 cases and deaths across Europe between 08 March 2021 and  07 March 2022. Methods  We used open-source tools to develop a public European COVID-19 Forecast  Hub. We invited groups globally to contribute weekly forecasts for COVID-19  cases and deaths reported from a standardised source over the next one to  four weeks. Teams submitted forecasts from March 2021 using standardised  quantiles of the predictive distribution. Each week we created an ensemble  forecast, where each predictive quantile was calculated as the  equally-weighted average (initially the mean and then from 26th July the  median) of all individual models' predictive quantiles. We measured the  performance of each model using the relative Weighted Interval Score (WIS),  comparing models' forecast accuracy relative to all other models. We  retrospectively explored alternative methods for ensemble forecasts,  including weighted averages based on models' past predictive  performance. Results  Over 52 weeks we collected and combined up to 28 forecast models for 32  countries. We found a weekly ensemble had a consistently strong performance  across countries over time. Across all horizons and locations, the ensemble  performed better on relative WIS than 84\% of participating models' forecasts  of incident cases (with a total N=862), and 92\% of participating models'  forecasts of deaths (N=746). Across a one to four week time horizon,  ensemble performance declined with longer forecast periods when forecasting  cases, but remained stable over four weeks for incident death forecasts. In  every forecast across 32 countries, the ensemble outperformed most  contributing models when forecasting either cases or deaths, frequently  outperforming all of its individual component models. Among several choices  of ensemble methods we found that the most influential and best choice was  to use a median average of models instead of using the mean, regardless of  methods of weighting component forecast models. Conclusions  Our results support the use of combining forecasts from individual  models into an ensemble in order to improve predictive performance across  epidemiological targets and populations during infectious disease epidemics.  Our findings further suggest that median ensemble methods yield better  predictive performance more than ones based on means. Our findings also  highlight that forecast consumers should place more weight on incident death  forecasts than incident case forecasts at forecast horizons greater than two  weeks. Code and data availability  All data and code are publicly available on Github:  covid19-forecast-hub-europe/euro-hub-ensemble.},
  copyright = {cc by},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/HK3RSINT/Sherratt et al. - 2022 - Predictive performance of multi-model ensemble for.pdf}
}

@misc{srivastavaShapebasedEvaluationEpidemic2022,
  title = {Shape-Based {{Evaluation}} of {{Epidemic Forecasts}}},
  author = {Srivastava, Ajitesh and Singh, Satwant and Lee, Fiona},
  year = {2022},
  month = nov,
  number = {arXiv:2209.04035},
  eprint = {2209.04035},
  primaryclass = {q-bio, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2209.04035},
  urldate = {2022-11-21},
  abstract = {Infectious disease forecasting for ongoing epidemics has been traditionally performed, communicated, and evaluated as numerical targets - 1, 2, 3, and 4 week ahead cases, deaths, and hospitalizations. While there is great value in predicting these numerical targets to assess the burden of the disease, we argue that there is also value in communicating the future trend (description of the shape) of the epidemic -- for instance, if the cases will remain flat or a surge is expected. To ensure what is being communicated is useful we need to be able to evaluate how well the predicted shape matches with the ground truth shape. Instead of treating this as a classification problem (one out of \$n\$ shapes), we define a transformation of the numerical forecasts into a ``shapelet''-space representation. In this representation, each dimension corresponds to the similarity of the shape with one of the shapes of interest (a shapelet). We prove that this representation satisfies the property that two shapes that one would consider similar are mapped close to each other, and vice versa. We demonstrate that our representation is able to reasonably capture the trends in COVID-19 cases and deaths time-series. With this representation, we define an evaluation measure and a measure of agreement among multiple models. We also define the shapelet-space ensemble of multiple models as the mean of their shapelet-space representations. We show that this ensemble is able to accurately predict the shape of the future trend for COVID-19 cases and trends. We also show that the agreement between models can provide a good indicator of the reliability of the forecast.},
  archiveprefix = {arxiv},
  keywords = {Quantitative Biology - Quantitative Methods,Statistics - Applications},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/NDNZDX3Z/Srivastava et al. - 2022 - Shape-based Evaluation of Epidemic Forecasts.pdf;/Users/nikos/github-synced/zotero-nikos/storage/II5XLIM6/2209.html}
}

@book{StatisticalModelsEpidemiology2001,
  title = {Statistical {{Models}} in {{Epidemiology}}},
  year = {2001},
  publisher = {Oxford University Press},
  abstract = {This book aims to give a self-contained account of the statistical basis of epidemiology. The book is intended primarily for students enrolled for a masters degree in epidemiology, clinical epidemiology, or biostatistics, and should be suitable both as the basis for a taught course and for private study. No previous knowledge is assumed, and the mathematical level has been chosen to suit readers whose basic training is in biology. The most important concept in statistics is the probability model. All statistical analysis of data is based on probability models, even though these may not be explicit. Only by fully understanding the model can one fully understand the analysis. In showing how to use models in epidemiology the authors have chosen to emphasize the role of likelihood. This is an approach to statistics which is both simple and intuitively satisfying, and has the additional advantage that it requires the model and its parameters to be made explicit, even in the simplest situations.},
  googlebooks = {b1MPnwEACAAJ},
  langid = {english}
}

@misc{statistischesbundesamtBevoelkerungNachNationalitaet2020,
  title = {{Bev{\"o}lkerung nach Nationalit{\"a}t und Bundesl{\"a}ndern}},
  author = {{Statistisches Bundesamt}},
  year = {2020},
  month = jun,
  journal = {Statistisches Bundesamt},
  urldate = {2021-06-10},
  abstract = {Diese Tabelle enth{\"a}lt: Bev{\"o}lkerung am 31.12.2019 nach Nationalit{\"a}t und Bundesl{\"a}ndern.},
  howpublished = {https://www.destatis.de/DE/Themen/Gesellschaft-Umwelt/Bevoelkerung/Bevoelkerungsstand/Tabellen/bevoelkerung-nichtdeutsch-laender.html},
  langid = {ngerman},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/BVRMKNET/bevoelkerung-nichtdeutsch-laender.html}
}

@article{swallowChallengesEstimationUncertainty2022,
  title = {Challenges in Estimation, Uncertainty Quantification and Elicitation for Pandemic Modelling},
  author = {Swallow, Ben and Birrell, Paul and Blake, Joshua and Burgman, Mark and Challenor, Peter and Coffeng, Luc E. and Dawid, Philip and De Angelis, Daniela and Goldstein, Michael and Hemming, Victoria and Marion, Glenn and McKinley, Trevelyan J. and Overton, Christopher E. and {Panovska-Griffiths}, Jasmina and Pellis, Lorenzo and Probert, Will and Shea, Katriona and Villela, Daniel and Vernon, Ian},
  year = {2022},
  month = mar,
  journal = {Epidemics},
  volume = {38},
  pages = {100547},
  issn = {1755-4365},
  doi = {10.1016/j.epidem.2022.100547},
  urldate = {2023-03-21},
  abstract = {The estimation of parameters and model structure for informing infectious disease response has become a focal point of the recent pandemic. However, it has also highlighted a plethora of challenges remaining in the fast and robust extraction of information using data and models to help inform policy. In this paper, we identify and discuss four broad challenges in the estimation paradigm relating to infectious disease modelling, namely the Uncertainty Quantification framework, data challenges in estimation, model-based inference and prediction, and expert judgement. We also postulate priorities in estimation methodology to facilitate preparation for future pandemics.},
  langid = {english},
  keywords = {Expert elicitation,Pandemic modelling,Statistical estimation,Uncertainty quantification},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/GABIPC4B/Swallow et al. - 2022 - Challenges in estimation, uncertainty quantificati.pdf;/Users/nikos/github-synced/zotero-nikos/storage/DSRZFE87/S1755436522000093.html}
}

@article{szekelyEnergyData2017,
  title = {The {{Energy}} of {{Data}}},
  author = {Sz{\'e}kely, G{\'a}bor J. and Rizzo, Maria L.},
  year = {2017},
  month = mar,
  journal = {Annual Review of Statistics and Its Application},
  volume = {4},
  number = {1},
  pages = {447--479},
  issn = {2326-8298, 2326-831X},
  doi = {10.1146/annurev-statistics-060116-054026},
  urldate = {2021-07-21},
  abstract = {The energy of data is the value of a real function of distances between data in metric spaces. The name energy derives from Newton's gravitational potential energy which is also a function of distances between physical objects. One of the advantages of working with energy functions (energy statistics) is that even if the observations/data are complex objects, like functions or graphs, we can use their real valued distances for inference. Other advantages will be illustrated and discussed in this paper. Concrete examples include energy testing for normality, energy clustering, and distance correlation. Applications include genome, brain studies, and astrophysics. The direct connection between energy and mind/observations/data in this paper is a counterpart of the equivalence of energy and matter/mass in Einstein's E = mc2.},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/M2RDGTZH/Sz√©kely and Rizzo - 2017 - The Energy of Data.pdf}
}

@article{taillardatCalibratedEnsembleForecasts2016,
  title = {Calibrated {{Ensemble Forecasts Using Quantile Regression Forests}} and {{Ensemble Model Output Statistics}}},
  author = {Taillardat, Maxime and Mestre, Olivier and Zamo, Micha{\"e}l and Naveau, Philippe},
  year = {2016},
  month = jun,
  journal = {Monthly Weather Review},
  volume = {144},
  number = {6},
  pages = {2375--2393},
  publisher = {American Meteorological Society},
  issn = {1520-0493, 0027-0644},
  doi = {10.1175/MWR-D-15-0260.1},
  urldate = {2021-10-20},
  abstract = {{$<$}section class="abstract"{$><$}h2 class="abstractTitle text-title my-1" id="d140121652e97"{$>$}Abstract{$<$}/h2{$><$}p{$>$}Ensembles used for probabilistic weather forecasting tend to be biased and underdispersive. This paper proposes a statistical method for postprocessing ensembles based on quantile regression forests (QRF), a generalization of random forests for quantile regression. This method does not fit a parametric probability density function (PDF) like in ensemble model output statistics (EMOS) but provides an estimation of desired quantiles. This is a nonparametric approach that eliminates any assumption on the variable subject to calibration. This method can estimate quantiles using not only members of the ensemble but any predictor available including statistics on other variables.{$<$}/p{$><$}p{$>$}The method is applied to the M{\'e}t{\'e}o-France 35-member ensemble forecast (PEARP) for surface temperature and wind speed for available lead times from 3 up to 54 h and compared to EMOS. All postprocessed ensembles are much better calibrated than the PEARP raw ensemble and experiments on real data also show that QRF performs better than EMOS, and can bring a real gain for human forecasters compared to EMOS. QRF provides sharp and reliable probabilistic forecasts. At last, classical scoring rules to verify predictive forecasts are completed by the introduction of entropy as a general measure of reliability.{$<$}/p{$><$}/section{$>$}},
  chapter = {Monthly Weather Review},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/K2FGHAXH/Taillardat et al. - 2016 - Calibrated Ensemble Forecasts Using Quantile Regre.pdf}
}

@article{taylorComparisonAggregationMethodsb,
  title = {A {{Comparison}} of {{Aggregation Methods}} for {{Probabilistic Forecasts}} of {{COVID-19 Mortality}} in the {{United States}}},
  author = {Taylor, Kathryn S and Taylor, James W},
  pages = {32},
  abstract = {The COVID-19 pandemic has placed forecasting models at the forefront of health policy making. Predictions of mortality and hospitalization help governments meet planning and resource allocation challenges. In this paper, we consider the weekly forecasting of the cumulative mortality due to COVID19 at the national and state level in the U.S. Optimal decision-making requires a forecast of a probability distribution, rather than just a single point forecast. Interval forecasts are also important, as they can support decision making and provide situational awareness. We consider the case where probabilistic forecasts have been provided by multiple forecasting teams, and we aggregate the forecasts to extract the wisdom of the crowd. With only limited information available regarding the historical accuracy of the forecasting teams, we consider aggregation (i.e. combining) methods that do not rely on a record of past accuracy. In this empirical paper, we evaluate the accuracy of aggregation methods that have been previously proposed for interval forecasts and predictions of probability distributions. These include the use of the simple average, the median, and trimming methods, which enable robust estimation and allow the aggregate forecast to reduce the impact of a tendency for the forecasting teams to be under- or overconfident. We use data that has been made publicly available from the COVID-19 Forecast Hub. While the simple average performed well for the high mortality series, we obtained greater accuracy using the median and certain trimming methods for the low and medium mortality series. It will be interesting to see if this remains the case as the pandemic evolves.},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/8YGC7EYJ/Taylor and Taylor - A Comparison of Aggregation Methods for Probabilis.pdf}
}

@article{taylorEvaluatingVolatilityInterval1999,
  title = {Evaluating Volatility and Interval Forecasts},
  author = {Taylor, James W.},
  year = {1999},
  journal = {Journal of Forecasting},
  volume = {18},
  number = {2},
  pages = {111--128},
  issn = {1099-131X},
  doi = {10.1002/(SICI)1099-131X(199903)18:2<111::AID-FOR713>3.0.CO;2-C},
  urldate = {2022-12-07},
  abstract = {A widely used approach to evaluating volatility forecasts uses a regression framework which measures the bias and variance of the forecast. We show that the associated test for bias is inappropriate before introducing a more suitable procedure which is based on the test for bias in a conditional mean forecast. Although volatility has been the most common measure of the variability in a financial time series, in many situations confidence interval forecasts are required. We consider the evaluation of interval forecasts and present a regression-based procedure which uses quantile regression to assess quantile estimator bias and variance. We use exchange rate data to illustrate the proposal by evaluating seven quantile estimators, one of which is a new non-parametric autoregressive conditional heteroscedasticity quantile estimator. The empirical analysis shows that the new evaluation procedure provides useful insight into the quality of quantile estimators. Copyright {\copyright} 1999 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {forecast evaluation,interval forecasting,quantile regression,volatility forecasting},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/K8NELTS8/(SICI)1099-131X(199903)182111AID-FOR7133.0.html}
}

@article{teamForecastingCOVID19Impact2020,
  title = {Forecasting {{COVID-19}} Impact on Hospital Bed-Days, {{ICU-days}}, Ventilator-Days and Deaths by {{US}} State in the next 4 Months},
  author = {health service utilization forecasting Team, IHME COVID-19 and Murray, Christopher JL},
  year = {2020},
  month = mar,
  journal = {medRxiv},
  pages = {2020.03.27.20043752},
  publisher = {Cold Spring Harbor Laboratory Press},
  doi = {10.1101/2020.03.27.20043752},
  urldate = {2021-05-29},
  abstract = {{$<$}h3{$>$}Abstract{$<$}/h3{$>$} {$<$}h3{$>$}Importance{$<$}/h3{$>$} {$<$}p{$>$}This study presents the first set of estimates of predicted health service utilization and deaths due to COVID-19 by day for the next 4 months for each state in the US.{$<$}/p{$><$}h3{$>$}Objective{$<$}/h3{$>$} {$<$}p{$>$}To determine the extent and timing of deaths and excess demand for hospital services due to COVID-19 in the US.{$<$}/p{$><$}h3{$>$}Design, Setting, and Participants{$<$}/h3{$>$} {$<$}p{$>$}This study used data on confirmed COVID-19 deaths by day from WHO websites and local and national governments; data on hospital capacity and utilization for US states; and observed COVID-19 utilization data from select locations to develop a statistical model forecasting deaths and hospital utilization against capacity by state for the US over the next 4 months.{$<$}/p{$><$}h3{$>$}Exposure(s){$<$}/h3{$>$} {$<$}p{$>$}COVID-19.{$<$}/p{$><$}h3{$>$}Main outcome(s) and measure(s){$<$}/h3{$>$} {$<$}p{$>$}Deaths, bed and ICU occupancy, and ventilator use.{$<$}/p{$><$}h3{$>$}Results{$<$}/h3{$>$} {$<$}p{$>$}Compared to licensed capacity and average annual occupancy rates, excess demand from COVID-19 at the peak of the pandemic in the second week of April is predicted to be 64,175 (95\% UI 7,977 to 251,059) total beds and 17,380 (95\% UI 2,432 to 57,955) ICU beds. At the peak of the pandemic, ventilator use is predicted to be 19,481 (95\% UI 9,767 to 39,674). The date of peak excess demand by state varies from the second week of April through May. We estimate that there will be a total of 81,114 (95\% UI 38,242 to 162,106) deaths from COVID-19 over the next 4 months in the US. Deaths from COVID-19 are estimated to drop below 10 deaths per day between May 31 and June 6.{$<$}/p{$><$}h3{$>$}Conclusions and Relevance{$<$}/h3{$>$} {$<$}p{$>$}In addition to a large number of deaths from COVID-19, the epidemic in the US will place a load well beyond the current capacity of hospitals to manage, especially for ICU care. These estimates can help inform the development and implementation of strategies to mitigate this gap, including reducing non-COVID-19 demand for services and temporarily increasing system capacity. These are urgently needed given that peak volumes are estimated to be only three weeks away. The estimated excess demand on hospital systems is predicated on the enactment of social distancing measures in all states that have not done so already within the next week and maintenance of these measures throughout the epidemic, emphasizing the importance of implementing, enforcing, and maintaining these measures to mitigate hospital system overload and prevent deaths.{$<$}/p{$><$}h3{$>$}Data availability statement{$<$}/h3{$>$} {$<$}p{$>$}A full list of data citations are available by contacting the corresponding author.{$<$}/p{$><$}h3{$>$}Funding Statement{$<$}/h3{$>$} {$<$}p{$>$}Bill \&amp; Melinda Gates Foundation and the State of Washington{$<$}/p{$><$}h3{$>$}Key Points{$<$}/h3{$>$} {$<$}h3{$>$}Question{$<$}/h3{$>$} {$<$}p{$>$}Assuming social distancing measures are maintained, what are the forecasted gaps in available health service resources and number of deaths from the COVID-19 pandemic for each state in the United States?{$<$}/p{$><$}h3{$>$}Findings{$<$}/h3{$>$} {$<$}p{$>$}Using a statistical model, we predict excess demand will be 64,175 (95\% UI 7,977 to 251,059) total beds and 17,380 (95\% UI 2,432 to 57,955) ICU beds at the peak of COVID-19. Peak ventilator use is predicted to be 19,481 (95\% UI 9,767 to 39,674) ventilators. Peak demand will be in the second week of April. We estimate 81,114 (95\% UI 38,242 to 162,106) deaths in the United States from COVID-19 over the next 4 months.{$<$}/p{$><$}h3{$>$}Meaning{$<$}/h3{$>$} {$<$}p{$>$}Even with social distancing measures enacted and sustained, the peak demand for hospital services due to the COVID-19 pandemic is likely going to exceed capacity substantially. Alongside the implementation and enforcement of social distancing measures, there is an urgent need to develop and implement plans to reduce non-COVID-19 demand for and temporarily increase capacity of health facilities.{$<$}/p{$>$}},
  copyright = {{\copyright} 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/HP5YTXXP/Team and Murray - 2020 - Forecasting COVID-19 impact on hospital bed-days, .pdf;/Users/nikos/github-synced/zotero-nikos/storage/QMBBN8NS/2020.03.27.html}
}

@article{tetlockForecastingTournamentsTools2014,
  title = {Forecasting {{Tournaments}}: {{Tools}} for {{Increasing Transparency}} and {{Improving}} the {{Quality}} of {{Debate}}},
  shorttitle = {Forecasting {{Tournaments}}},
  author = {Tetlock, Philip E. and Mellers, Barbara A. and Rohrbaugh, Nick and Chen, Eva},
  year = {2014},
  month = aug,
  journal = {Current Directions in Psychological Science},
  volume = {23},
  number = {4},
  pages = {290--295},
  publisher = {SAGE Publications Inc},
  issn = {0963-7214},
  doi = {10.1177/0963721414534257},
  urldate = {2021-05-30},
  abstract = {Forecasting tournaments are level-playing-field competitions that reveal which individuals, teams, or algorithms generate more accurate probability estimates on which topics. This article describes a massive geopolitical tournament that tested clashing views on the feasibility of improving judgmental accuracy and on the best methods of doing so. The tournament's winner, the Good Judgment Project, outperformed the simple average of the crowd by (a) designing new forms of cognitive-debiasing training, (b) incentivizing rigorous thinking in teams and prediction markets, (c) skimming top talent into elite collaborative teams of ``super forecasters,'' and (d) fine-tuning aggregation algorithms for distilling greater wisdom from crowds. Tournaments have the potential to open closed minds and increase assertion-to-evidence ratios in polarized scientific and policy debates.},
  langid = {english},
  keywords = {accuracy,forecasting,probability,tournaments}
}

@article{thoreyOnlineLearningContinuous2017a,
  title = {Online Learning with the {{Continuous Ranked Probability Score}} for Ensemble Forecasting},
  author = {Thorey, J. and Mallet, V. and Baudin, P.},
  year = {2017},
  month = jan,
  journal = {Quarterly Journal of the Royal Meteorological Society},
  volume = {143},
  number = {702},
  pages = {521--529},
  issn = {0035-9009, 1477-870X},
  doi = {10.1002/qj.2940},
  urldate = {2022-03-14},
  abstract = {Ensemble forecasting resorts to multiple individual forecasts to produce a discrete probability distribution which accurately represents the uncertainties. Before every forecast, a weighted empirical distribution function is derived from the ensemble, so as to minimize the Continuous Ranked Probability Score (CRPS). We apply online learning techniques, which have previously been used for deterministic forecasting, and we adapt them for the minimization of the CRPS. The proposed method theoretically guarantees that the aggregated forecast competes, in terms of CRPS, against the best weighted empirical distribution function with weights constant in time. This is illustrated on synthetic data. Besides, our study improves the knowledge of the CRPS expectation for model mixtures. We generalize results on the bias of the CRPS computed with ensemble forecasts, and propose a new scheme to achieve fair CRPS minimization, without any assumption on the distributions.},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/R96EA9GW/Thorey et al. - 2017 - Online learning with the Continuous Ranked Probabi.pdf}
}

@article{tibshiraniConformalPredictionCovariate,
  title = {Conformal {{Prediction Under Covariate Shift}}},
  author = {Tibshirani, Ryan J and Cand{\`e}s, Emmanuel J and Barber, Rina Foygel and Ramdas, Aaditya},
  pages = {11},
  abstract = {We extend conformal prediction methodology beyond the case of exchangeable data. In particular, we show that a weighted version of conformal prediction can be used to compute distribution-free prediction intervals for problems in which the test and training covariate distributions differ, but the likelihood ratio between the two distributions is known---or, in practice, can be estimated accurately from a set of unlabeled data (test covariate points). Our weighted extension of conformal prediction also applies more broadly, to settings in which the data satisfies a certain weighted notion of exchangeability. We discuss other potential applications of our new conformal methodology, including latent variable and missing data problems.},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/9GCFMW8F/Tibshirani et al. - Conformal Prediction Under Covariate Shift.pdf}
}

@article{timmermannForecastingMethodsFinance2018,
  title = {Forecasting {{Methods}} in {{Finance}}},
  author = {Timmermann, Allan},
  year = {2018},
  journal = {Annual Review of Financial Economics},
  volume = {10},
  number = {1},
  pages = {449--479},
  doi = {10.1146/annurev-financial-110217-022713},
  urldate = {2020-12-14},
  abstract = {Our review highlights some of the key challenges in financial forecasting problems and opportunities arising from the unique features of financial data. We analyze the difficulty of establishing predictability in an environment with a low signal-to-noise ratio, persistent predictors, and instability in predictive relations arising from competitive pressures and investors' learning. We discuss approaches for forecasting the mean, variance, and probability distribution of asset returns. Finally, we discuss how to evaluate financial forecasts while accounting for the possibility that numerous forecasting models may have been considered, leading to concerns of data mining.}
}

@article{timmermannForecastingMethodsFinance2018a,
  title = {Forecasting {{Methods}} in {{Finance}}},
  author = {Timmermann, Allan},
  year = {2018},
  month = nov,
  journal = {Annual Review of Financial Economics},
  volume = {10},
  number = {1},
  pages = {449--479},
  issn = {1941-1367, 1941-1375},
  doi = {10.1146/annurev-financial-110217-022713},
  urldate = {2021-12-15},
  abstract = {Our review highlights some of the key challenges in financial forecasting problems and opportunities arising from the unique features of financial data. We analyze the difficulty of establishing predictability in an environment with a low signal-to-noise ratio, persistent predictors, and instability in predictive relations arising from competitive pressures and investors' learning. We discuss approaches for forecasting the mean, variance, and probability distribution of asset returns. Finally, we discuss how to evaluate financial forecasts while accounting for the possibility that numerous forecasting models may have been considered, leading to concerns of data mining.},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/9FHI5V4F/Timmermann - 2018 - Forecasting Methods in Finance.pdf}
}

@article{toccaceliConformalPredictorCombination,
  title = {Conformal {{Predictor Combination}} Using {{Neyman-Pearson Lemma}}},
  author = {Toccaceli, Paolo},
  pages = {23},
  abstract = {The problem of how to combine advantageously Conformal Predictors (CP) has attracted the interest of many researchers in recent years. The challenge is to retain validity, while improving efficiency. In this article a very generic method is proposed which takes advantage of a well-established result in Classical Statistical Hypothesis Testing, the NeymanPearson Lemma, to combine CP with maximum efficiency. The merits and the limits of the method are explored on synthetic data sets under different levels of correlation between NonConformity Measures (NCM). CP Combination via Neyman-Pearson Lemma generally outperforms other combination methods when an accurate and robust density ratio estimation method, such as the V-Matrix method, is used.},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/4WKPEZI5/Toccaceli - Conformal Predictor Combination using Neyman-Pears.pdf}
}

@article{todterGeneralizationIgnoranceScore2012,
  title = {Generalization of the {{Ignorance Score}}: {{Continuous Ranked Version}} and {{Its Decomposition}}},
  shorttitle = {Generalization of the {{Ignorance Score}}},
  author = {T{\"o}dter, Julian and Ahrens, Bodo},
  year = {2012},
  month = jun,
  journal = {Monthly Weather Review},
  volume = {140},
  number = {6},
  pages = {2005--2017},
  publisher = {American Meteorological Society},
  issn = {1520-0493, 0027-0644},
  doi = {10.1175/MWR-D-11-00266.1},
  urldate = {2022-04-11},
  abstract = {Abstract The Brier score (BS) and its generalizations to the multicategory ranked probability score (RPS) and to the continuous ranked probability score (CRPS) are the prominent verification measures for probabilistic forecasts. Particularly, their decompositions into measures quantifying the reliability, resolution, and uncertainty of the forecasts are attractive. Information theory sets up the natural framework for forecast verification. Recently, it has been shown that the BS is a second-order approximation of the information-based ignorance score (IGN), which also contains easily interpretable components and can also be generalized to a ranked version (RIGN). Here, the IGN, its generalizations, and decompositions are systematically discussed in analogy to the variants of the BS. Additionally, a continuous ranked IGN (CRIGN) is introduced in analogy to the CRPS. The applicability and usefulness of the conceptually appealing CRIGN are illustrated, together with an algorithm to evaluate its components reliability, resolution, and uncertainty for ensemble-generated forecasts.},
  chapter = {Monthly Weather Review},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/KZVDJ4QI/T√∂dter and Ahrens - 2012 - Generalization of the Ignorance Score Continuous .pdf}
}

@article{twohigHospitalAdmissionEmergency2022,
  title = {Hospital Admission and Emergency Care Attendance Risk for {{SARS-CoV-2}} Delta ({{B}}.1.617.2) Compared with Alpha ({{B}}.1.1.7) Variants of Concern: A Cohort Study},
  shorttitle = {Hospital Admission and Emergency Care Attendance Risk for {{SARS-CoV-2}} Delta ({{B}}.1.617.2) Compared with Alpha ({{B}}.1.1.7) Variants of Concern},
  author = {Twohig, Katherine A. and Nyberg, Tommy and Zaidi, Asad and Thelwall, Simon and Sinnathamby, Mary A. and Aliabadi, Shirin and Seaman, Shaun R. and Harris, Ross J. and Hope, Russell and {Lopez-Bernal}, Jamie and Gallagher, Eileen and Charlett, Andre and Angelis, Daniela De and Presanis, Anne M. and Dabrera, Gavin and Koshy, Cherian and Ash, Amy and Wise, Emma and Moore, Nathan and Mori, Matilde and Cortes, Nick and Lynch, Jessica and Kidd, Stephen and Fairley, Derek and Curran, Tanya and McKenna, James and Adams, Helen and Fraser, Christophe and Golubchik, Tanya and Bonsall, David and {Hassan-Ibrahim}, Mohammed and Malone, Cassandra and Cogger, Benjamin and Wantoch, Michelle and Reynolds, Nicola and Warne, Ben and Maksimovic, Joshua and Spellman, Karla and McCluggage, Kathryn and John, Michaela and Beer, Robert and Afifi, Safiah and Morgan, Sian and Marchbank, Angela and Price, Anna and Kitchen, Christine and Gulliver, Huw and Merrick, Ian and Southgate, Joel and Guest, Martyn and Munn, Robert and Workman, Trudy and Connor, Thomas and Fuller, William and Bresner, Catherine and Snell, Luke and Patel, Amita and Charalampous, Themoula and Nebbia, Gaia and Batra, Rahul and Edgeworth, Jonathan and Robson, Samuel and Beckett, Angela and Aanensen, David and Underwood, Anthony and Yeats, Corin and Abudahab, Khalil and Taylor, Ben and Menegazzo, Mirko and Clark, Gemma and Smith, Wendy and Khakh, Manjinder and Fleming, Vicki and Lister, Michelle and {Howson-Wells}, Hannah and Berry, Louise and Boswell, Tim and Joseph, Amelia and Willingham, Iona and Jones, Carl and Holmes, Christopher and Bird, Paul and Helmer, Thomas and Fallon, Karlie and Tang, Julian and Raviprakash, Veena and Campbell, Sharon and Sheriff, Nicola and Blakey, Victoria and Williams, Lesley-Anne and Loose, Matthew and Holmes, Nadine and Moore, Christopher and Carlile, Matthew and Wright, Victoria and Sang, Fei and Debebe, Johnny and Coll, Francesc and Signell, Adrian and Betancor, Gilberto and Wilson, Harry and Eldirdiri, Sahar and Kenyon, Anita and Davis, Thomas and Pybus, Oliver and du Plessis, Louis and Zarebski, Alex and Raghwani, Jayna and Kraemer, Moritz and Francois, Sarah and Attwood, Stephen and Vasylyeva, Tetyana and Zamudio, Marina Escalera and Gutierrez, Bernardo and Torok, M. Estee and Hamilton, William and Goodfellow, Ian and Hall, Grant and Jahun, Aminu and Chaudhry, Yasmin and Hosmillo, Myra and Pinckert, Malte and Georgana, Iliana and Moses, Samuel and Lowe, Hannah and Bedford, Luke and Moore, Jonathan and Stonehouse, Susanne and Fisher, Chloe and Awan, Ali and BoYes, John and Breuer, Judith and Harris, Kathryn and Brown, Julianne and Shah, Divya and Atkinson, Laura and Lee, Jack and Storey, Nathaniel and Flaviani, Flavia and {Alcolea-Medina}, Adela and Williams, Rebecca and Vernet, Gabrielle and Chapman, Michael and Levett, Lisa and Heaney, Judith and Chatterton, Wendy and Pusok, Monika and {Xu-McCrae}, Li and Smith, Darren and Bashton, Matthew and Young, Gregory and Holmes, Alison and Randell, Paul and Cox, Alison and Madona, Pinglawathee and Bolt, Frances and Price, James and Mookerjee, Siddharth and {Ragonnet-Cronin}, Manon and Nascimento, Fabricia F. and Jorgensen, David and Siveroni, Igor and Johnson, Rob and Boyd, Olivia and Geidelberg, Lily and Volz, Erik and Rowan, Aileen and Taylor, Graham and Smollett, Katherine and Loman, Nicholas and Quick, Joshua and McMurray, Claire and Stockton, Joanne and Nicholls, Sam and Rowe, Will and Poplawski, Radoslaw and McNally, Alan and Nunez, Rocio Martinez and Mason, Jenifer and Robinson, Trevor and O'Toole, Elaine and Watts, Joanne and Breen, Cassie and Cowell, Angela and Sluga, Graciela and Machin, Nicholas and Ahmad, Shazaad and George, Ryan and Halstead, Fenella and Sivaprakasam, Venkat and Hogsden, Wendy and Illingworth, Chris and Jackson, Chris and Thomson, Emma and Shepherd, James and Asamaphan, Patawee and Niebel, Marc and Li, Kathy and Shah, Rajiv and Jesudason, Natasha and Tong, Lily and Broos, Alice and Mair, Daniel and Nichols, Jenna and Carmichael, Stephen and Nomikou, Kyriaki and {Aranday-Cortes}, Elihu and Johnson, Natasha and Starinskij, Igor and Filipe, Ana da Silva and Robertson, David and Orton, Richard and Hughes, Joseph and Vattipally, Sreenu and Singer, Joshua and Nickbakhsh, Seema and Hale, Antony and {Macfarlane-Smith}, Louissa and Harper, Katherine and Carden, Holli and Taha, Yusri and Payne, Brendan and {Burton-Fanning}, Shirelle and Waugh, Sheila and Collins, Jennifer and Eltringham, Gary and Rushton, Steven and O'Brien, Sarah and Bradley, Amanda and Maclean, Alasdair and Mollett, Guy and Blacow, Rachel and Templeton, Kate and McHugh, Martin and Dewar, Rebecca and Wastenge, Elizabeth and Dervisevic, Samir and Stanley, Rachael and Meader, Emma and Coupland, Lindsay and Smith, Louise and Graham, Clive and Barton, Edward and Padgett, Debra and Scott, Garren and Swindells, Emma and Greenaway, Jane and Nelson, Andrew and McCann, Clare and Yew, Wen and Andersson, Monique and Peto, Timothy and Justice, Anita and Eyre, David and Crook, Derrick and Sloan, Tim and Duckworth, Nichola and Walsh, Sarah and Chauhan, Anoop and Glaysher, Sharon and Bicknell, Kelly and Wyllie, Sarah and Elliott, Scott and Lloyd, Allyson and Impey, Robert and Levene, Nick and Monaghan, Lynn and Bradley, Declan and Wyatt, Tim and Allara, Elias and Pearson, Clare and Osman, Husam and Bosworth, Andrew and Robinson, Esther and Muir, Peter and Vipond, Ian and Hopes, Richard and Pymont, Hannah and Hutchings, Stephanie and Curran, Martin and Parmar, Surendra and Lackenby, Angie and Mbisa, Tamyo and Platt, Steven and Miah, Shahjahan and Bibby, David and Manso, Carmen and Hubb, Jonathan and Chand, Meera and Dabrera, Gavin and Ramsay, Mary and Bradshaw, Daniel and Thornton, Alicia and Myers, Richard and Schaefer, Ulf and Groves, Natalie and Gallagher, Eileen and Lee, David and Williams, David and Ellaby, Nicholas and Harrison, Ian and Hartman, Hassan and Manesis, Nikos and Patel, Vineet and Bishop, Chloe and Chalker, Vicki and Ledesma, Juan and Twohig, Katherine and Holden, Matthew and Shaaban, Sharif and Birchley, Alec and Adams, Alexander and Davies, Alisha and Gaskin, Amy and Plimmer, Amy and {Gatica-Wilcox}, Bree and McKerr, Caoimhe and Moore, Catherine and Williams, Chris and Heyburn, David and Lacy, Elen De and Hilvers, Ember and Downing, Fatima and Shankar, Giri and Jones, Hannah and Asad, Hibo and Coombes, Jason and Watkins, Joanne and Evans, Johnathan and Fina, Laia and Gifford, Laura and Gilbert, Lauren and Graham, Lee and Perry, Malorie and Morgan, Mari and Bull, Matthew and Cronin, Michelle and Pacchiarini, Nicole and Craine, Noel and Jones, Rachel and Howe, Robin and Corden, Sally and Rey, Sara and {Kumziene-SummerhaYes}, Sara and Taylor, Sarah and Cottrell, Simon and Jones, Sophie and Edwards, Sue and O'Grady, Justin and Page, Andrew and Mather, Alison and Baker, David and Rudder, Steven and Aydin, Alp and Kay, Gemma and Trotter, Alexander and Alikhan, Nabil-Fareed and Martins, Leonardo de Oliveira and {Le-Viet}, Thanh and Meadows, Lizzie and Casey, Anna and Ratcliffe, Liz and Simpson, David and Molnar, Zoltan and Thompson, Thomas and Acheson, Erwan and Masoli, Jane and Knight, Bridget and Ellard, Sian and Auckland, Cressida and Jones, Christopher and Mahungu, Tabitha and {Irish-Tavares}, Dianne and Haque, Tanzina and Hart, Jennifer and Witele, Eric and Fenton, Melisa and Dadrah, Ashok and Symmonds, Amanda and Saluja, Tranprit and Bourgeois, Yann and Scarlett, Garry and Loveson, Katie and Goudarzi, Salman and Fearn, Christopher and Cook, Kate and Dent, Hannah and Paul, Hannah and Partridge, David and Raza, Mohammad and Evans, Cariad and Johnson, Kate and Liggett, Steven and Baker, Paul and Bonner, Stephen and Essex, Sarah and Lyons, Ronan and Saeed, Kordo and Mahanama, Adhyana and Samaraweera, Buddhini and Silveira, Siona and Pelosi, Emanuela and {Wilson-Davies}, Eleri and Williams, Rachel and Kristiansen, Mark and Roy, Sunando and Williams, Charlotte and Cotic, Marius and Bayzid, Nadua and Westhorpe, Adam and Hartley, John and Jannoo, Riaz and Lowe, Helen and Karamani, Angeliki and Ensell, Leah and Prieto, Jacqui and Jeremiah, Sarah and Grammatopoulos, Dimitris and Pandey, Sarojini and Berry, Lisa and Jones, Katie and Richter, Alex and Beggs, Andrew and Best, Angus and Percival, Benita and Mirza, Jeremy and Megram, Oliver and Mayhew, Megan and Crawford, Liam and Ashcroft, Fiona and {Moles-Garcia}, Emma and Cumley, Nicola and Smith, Colin and Bucca, Giselda and Hesketh, Andrew and Blane, Beth and Girgis, Sophia and Leek, Danielle and Sridhar, Sushmita and Forrest, Sally and Cormie, Claire and Gill, Harmeet and Dias, Joana and Higginson, Ellen and Maes, Mailis and Young, Jamie and Kermack, Leanne and Gupta, Ravi and Ludden, Catherine and Peacock, Sharon and Palmer, Sophie and Churcher, Carol and Hadjirin, Nazreen and Carabelli, Alessandro and Brooks, Ellena and Smith, Kim and Galai, Katerina and McManus, Georgina and Ruis, Chris and Davidson, Rose and Rambaut, Andrew and Williams, Thomas and Balcazar, Carlos and Gallagher, Michael and O'Toole, {\'A}ine and Rooke, Stefan and Hill, Verity and Williamson, Kathleen and Stanton, Thomas and Michell, Stephen and Bewshea, Claire and Temperton, Ben and Michelsen, Michelle and {Warwick-Dugdale}, Joanna and Manley, Robin and Farbos, Audrey and Harrison, James and Sambles, Christine and Studholme, David and Jeffries, Aaron and Jackson, Leigh and Darby, Alistair and Hiscox, Julian and Paterson, Steve and {Iturriza-Gomara}, Miren and Jackson, Kathryn and Lucaci, Anita and Vamos, Edith and Hughes, Margaret and Rainbow, Lucille and Eccles, Richard and Nelson, Charlotte and Whitehead, Mark and Turtle, Lance and Haldenby, Sam and Gregory, Richard and Gemmell, Matthew and Wierzbicki, Claudia and Webster, Hermione and de Silva, Thushan and Smith, Nikki and Angyal, Adrienn and Lindsey, Benjamin and Groves, Danielle and Green, Luke and Wang, Dennis and Freeman, Timothy and Parker, Matthew and Keeley, Alexander and Parsons, Paul and Tucker, Rachel and Brown, Rebecca and Wyles, Matthew and Whiteley, Max and Zhang, Peijun and Gallis, Marta and Louka, Stavroula and Constantinidou, Chrystala and Unnikrishnan, Meera and Ott, Sascha and Cheng, Jeffrey and Bridgewater, Hannah and Frost, Lucy and {Taylor-Joyce}, Grace and Stark, Richard and Baxter, Laura and Alam, Mohammad and Brown, Paul and Aggarwal, Dinesh and Cerda, Alberto and Merrill, Tammy and Wilson, Rebekah and McClure, Patrick and Chappell, Joseph and Tsoleridis, Theocharis and Ball, Jonathan and Buck, David and Todd, John and Green, Angie and Trebes, Amy and {MacIntyre-Cockett}, George and de Cesare, Mariateresa and Alderton, Alex and Amato, Roberto and Ariani, Cristina and Beale, Mathew and Beaver, Charlotte and Bellis, Katherine and Betteridge, Emma and Bonfield, James and Danesh, John and Dorman, Matthew and Drury, Eleanor and Farr, Ben and Foulser, Luke and Goncalves, Sonia and Goodwin, Scott and Gourtovaia, Marina and Harrison, Ewan and Jackson, David and Jamrozy, Dorota and Johnston, Ian and Kane, Leanne and Kay, Sally and Keatley, Jon-Paul and Kwiatkowski, Dominic and Langford, Cordelia and Lawniczak, Mara and Letchford, Laura and Livett, Rich and Lo, Stephanie and Martincorena, Inigo and McGuigan, Samantha and Nelson, Rachel and Palmer, Steve and Park, Naomi and Patel, Minal and Prestwood, Liam and Puethe, Christoph and Quail, Michael and Rajatileka, Shavanthi and Scott, Carol and Shirley, Lesley and Sillitoe, John and Chapman, Michael Spencer and Thurston, Scott and {Tonkin-Hill}, Gerry and Weldon, Danni and Rajan, Diana and Bronner, Iraad and Aigrain, Louise and Redshaw, Nicholas and Lensing, Stefanie and Davies, Robert and Whitwham, Andrew and Liddle, Jennifier and Lewis, Kevin and {Tovar-Corona}, Jaime and Leonard, Steven and Durham, Jillian and Bassett, Andrew and McCarthy, Shane and Moll, Robin and James, Keith and Oliver, Karen and Makunin, Alex and Barrett, Jeff and Gunson, Rory},
  year = {2022},
  month = jan,
  journal = {The Lancet Infectious Diseases},
  volume = {22},
  number = {1},
  pages = {35--42},
  publisher = {Elsevier},
  issn = {1473-3099, 1474-4457},
  doi = {10.1016/S1473-3099(21)00475-8},
  urldate = {2023-12-11},
  langid = {english},
  pmid = {34461056},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/DBME6CQ5/Twohig et al. - 2022 - Hospital admission and emergency care attendance r.pdf}
}

@article{vannitsemStatisticalPostprocessingWeather2021a,
  title = {Statistical {{Postprocessing}} for {{Weather Forecasts}}: {{Review}}, {{Challenges}}, and {{Avenues}} in a {{Big Data World}}},
  shorttitle = {Statistical {{Postprocessing}} for {{Weather Forecasts}}},
  author = {Vannitsem, St{\'e}phane and Bremnes, John Bj{\o}rnar and Demaeyer, Jonathan and Evans, Gavin R. and Flowerdew, Jonathan and Hemri, Stephan and Lerch, Sebastian and Roberts, Nigel and Theis, Susanne and Atencia, Aitor and Bouall{\`e}gue, Zied Ben and Bhend, Jonas and Dabernig, Markus and Cruz, Lesley De and Hieta, Leila and Mestre, Olivier and Moret, Lionel and Plenkovi{\'c}, Iris Odak and Schmeits, Maurice and Taillardat, Maxime and den Bergh, Joris Van and Schaeybroeck, Bert Van and Whan, Kirien and Ylhaisi, Jussi},
  year = {2021},
  month = mar,
  journal = {Bulletin of the American Meteorological Society},
  volume = {102},
  number = {3},
  pages = {E681-E699},
  publisher = {American Meteorological Society},
  issn = {0003-0007, 1520-0477},
  doi = {10.1175/BAMS-D-19-0308.1},
  urldate = {2021-10-18},
  abstract = {{$<$}section class="abstract"{$><$}h2 class="abstractTitle text-title my-1" id="d167604301e280"{$>$}Abstract{$<$}/h2{$><$}p{$>$}Statistical postprocessing techniques are nowadays key components of the forecasting suites in many national meteorological services (NMS), with, for most of them, the objective of correcting the impact of different types of errors on the forecasts. The final aim is to provide optimal, automated, seamless forecasts for end users. Many techniques are now flourishing in the statistical, meteorological, climatological, hydrological, and engineering communities. The methods range in complexity from simple bias corrections to very sophisticated distribution-adjusting techniques that incorporate correlations among the prognostic variables. The paper is an attempt to summarize the main activities going on in this area from theoretical developments to operational applications, with a focus on the current challenges and potential avenues in the field. Among these challenges is the shift in NMS toward running ensemble numerical weather prediction (NWP) systems at the kilometer scale that produce very large datasets and require high-density high-quality observations, the necessity to preserve space--time correlation of high-dimensional corrected fields, the need to reduce the impact of model changes affecting the parameters of the corrections, the necessity for techniques to merge different types of forecasts and ensembles with different behaviors, and finally the ability to transfer research on statistical postprocessing to operations. Potential new avenues are also discussed.{$<$}/p{$><$}/section{$>$}},
  chapter = {Bulletin of the American Meteorological Society},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/EF4UJFK3/Vannitsem et al. - 2021 - Statistical Postprocessing for Weather Forecasts .pdf;/Users/nikos/github-synced/zotero-nikos/storage/TTTCQVCD/journals$002fbams$002faop$002fBAMS-D-19-0308.1$002fBAMS-D-19-0308.1.html}
}

@article{vanschaeybroeckEnsemblePostprocessingUsing2015a,
  title = {Ensemble Post-Processing Using Member-by-Member Approaches: Theoretical Aspects: {{Ensemble Post-processing}} Using {{Member-by-Member Approaches}}},
  shorttitle = {Ensemble Post-Processing Using Member-by-Member Approaches},
  author = {Van Schaeybroeck, Bert and Vannitsem, St{\'e}phane},
  year = {2015},
  month = apr,
  journal = {Quarterly Journal of the Royal Meteorological Society},
  volume = {141},
  number = {688},
  pages = {807--818},
  issn = {00359009},
  doi = {10.1002/qj.2397},
  urldate = {2021-10-18},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/QM33ICLX/Van Schaeybroeck and Vannitsem - 2015 - Ensemble post-processing using member-by-member ap.pdf}
}

@article{vanschaeybroeckPostprocessingLinearRegression2011,
  title = {Post-Processing through Linear Regression},
  author = {Van Schaeybroeck, B. and Vannitsem, S.},
  year = {2011},
  month = mar,
  journal = {Nonlinear Processes in Geophysics},
  volume = {18},
  number = {2},
  pages = {147--160},
  issn = {1607-7946},
  doi = {10.5194/npg-18-147-2011},
  urldate = {2021-10-19},
  abstract = {Various post-processing techniques are compared for both deterministic and ensemble forecasts, all based on linear regression between forecast data and observations. In order to evaluate the quality of the regression methods, three criteria are proposed, related to the effective correction of forecast error, the optimal variability of the corrected forecast and multicollinearity. The regression schemes under consideration include the ordinary leastsquare (OLS) method, a new time-dependent Tikhonov regularization (TDTR) method, the total least-square method, a new geometric-mean regression (GM), a recently introduced error-in-variables (EVMOS) method and, finally, a ``best member'' OLS method. The advantages and drawbacks of each method are clarified.},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/6BEZYI94/Van Schaeybroeck and Vannitsem - 2011 - Post-processing through linear regression.pdf}
}

@article{venkatramananUtilityHumanJudgment2022,
  title = {Utility of Human Judgment Ensembles during Times of Pandemic Uncertainty: {{A}} Case Study during the {{COVID-19 Omicron BA}}.1 Wave in the {{USA}}},
  shorttitle = {Utility of Human Judgment Ensembles during Times of Pandemic Uncertainty},
  author = {Venkatramanan, Srinivasan and Cambeiro, Juan and Liptay, Tom and Lewis, Bryan and Orr, Mark and Dempsey, Gaia and Telionis, Alex and Crow, Justin and Barrett, Chris and Marathe, Madhav},
  year = {2022},
  month = oct,
  pages = {2022.10.12.22280997},
  doi = {10.1101/2022.10.12.22280997},
  urldate = {2023-02-07},
  abstract = {Responding to a rapidly evolving pandemic like COVID-19 is challenging, and involves anticipating novel variants, vaccine uptake, and behavioral adaptations. Human judgment systems can complement computational models by providing valuable real-time forecasts. We report findings from a study conducted on Metaculus, a community forecasting platform, in partnership with the Virginia Department of Health, involving six rounds of forecasting during the Omicron BA.1 wave in the United States from November 2021 to March 2022. We received 8355 probabilistic predictions from 129 unique users across 60 questions pertaining to cases, hospitalizations, vaccine uptake, and peak/trough activity. We observed that the case forecasts performed on par with national multi-model ensembles and the vaccine uptake forecasts were more robust and accurate compared to baseline models. We also identified qualitative shifts in Omicron BA.1 wave prognosis during the surge phase, demonstrating rapid adaptation of such systems. Finally, we found that community estimates of variant characteristics such as growth rate and timing of dominance were in line with the scientific consensus. The observed accuracy, timeliness, and scope of such systems demonstrates the value of incorporating them into pandemic policymaking workflows.},
  copyright = {{\copyright} 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/HG7AGF5X/Venkatramanan et al. - 2022 - Utility of human judgment ensembles during times o.pdf}
}

@misc{venkatramananUtilityHumanJudgment2022a,
  title = {Utility of Human Judgment Ensembles during Times of Pandemic Uncertainty: {{A}} Case Study during the {{COVID-19 Omicron BA}}.1 Wave in the {{USA}}},
  shorttitle = {Utility of Human Judgment Ensembles during Times of Pandemic Uncertainty},
  author = {Venkatramanan, Srinivasan and Cambeiro, Juan and Liptay, Tom and Lewis, Bryan and Orr, Mark and Dempsey, Gaia and Telionis, Alex and Crow, Justin and Barrett, Chris and Marathe, Madhav},
  year = {2022},
  month = oct,
  pages = {2022.10.12.22280997},
  publisher = {medRxiv},
  doi = {10.1101/2022.10.12.22280997},
  urldate = {2023-04-15},
  abstract = {Responding to a rapidly evolving pandemic like COVID-19 is challenging, and involves anticipating novel variants, vaccine uptake, and behavioral adaptations. Human judgment systems can complement computational models by providing valuable real-time forecasts. We report findings from a study conducted on Metaculus, a community forecasting platform, in partnership with the Virginia Department of Health, involving six rounds of forecasting during the Omicron BA.1 wave in the United States from November 2021 to March 2022. We received 8355 probabilistic predictions from 129 unique users across 60 questions pertaining to cases, hospitalizations, vaccine uptake, and peak/trough activity. We observed that the case forecasts performed on par with national multi-model ensembles and the vaccine uptake forecasts were more robust and accurate compared to baseline models. We also identified qualitative shifts in Omicron BA.1 wave prognosis during the surge phase, demonstrating rapid adaptation of such systems. Finally, we found that community estimates of variant characteristics such as growth rate and timing of dominance were in line with the scientific consensus. The observed accuracy, timeliness, and scope of such systems demonstrates the value of incorporating them into pandemic policymaking workflows.},
  archiveprefix = {medRxiv},
  copyright = {{\copyright} 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/AGMIG33K/Venkatramanan et al. - 2022 - Utility of human judgment ensembles during times o.pdf}
}

@article{viboudRAPIDDEbolaForecasting2018,
  title = {The {{RAPIDD}} Ebola Forecasting Challenge: {{Synthesis}} and Lessons Learnt},
  shorttitle = {The {{RAPIDD}} Ebola Forecasting Challenge},
  author = {Viboud, C{\'e}cile and Sun, Kaiyuan and Gaffey, Robert and Ajelli, Marco and Fumanelli, Laura and Merler, Stefano and Zhang, Qian and Chowell, Gerardo and Simonsen, Lone and Vespignani, Alessandro},
  year = {2018},
  month = mar,
  journal = {Epidemics},
  series = {The {{RAPIDD Ebola Forecasting Challenge}}},
  volume = {22},
  pages = {13--21},
  issn = {1755-4365},
  doi = {10.1016/j.epidem.2017.08.002},
  urldate = {2021-05-30},
  abstract = {Infectious disease forecasting is gaining traction in the public health community; however, limited systematic comparisons of model performance exist. Here we present the results of a synthetic forecasting challenge inspired by the West African Ebola crisis in 2014--2015 and involving 16 international academic teams and US government agencies, and compare the predictive performance of 8 independent modeling approaches. Challenge participants were invited to predict 140 epidemiological targets across 5 different time points of 4 synthetic Ebola outbreaks, each involving different levels of interventions and ``fog of war'' in outbreak data made available for predictions. Prediction targets included 1--4 week-ahead case incidences, outbreak size, peak timing, and several natural history parameters. With respect to weekly case incidence targets, ensemble predictions based on a Bayesian average of the 8 participating models outperformed any individual model and did substantially better than a null auto-regressive model. There was no relationship between model complexity and prediction accuracy; however, the top performing models for short-term weekly incidence were reactive models with few parameters, fitted to a short and recent part of the outbreak. Individual model outputs and ensemble predictions improved with data accuracy and availability; by the second time point, just before the peak of the epidemic, estimates of final size were within 20\% of the target. The 4th challenge scenario - mirroring an uncontrolled Ebola outbreak with substantial data reporting noise - was poorly predicted by all modeling teams. Overall, this synthetic forecasting challenge provided a deep understanding of model performance under controlled data and epidemiological conditions. We recommend such ``peace time'' forecasting challenges as key elements to improve coordination and inspire collaboration between modeling groups ahead of the next pandemic threat, and to assess model forecasting accuracy for a variety of known and hypothetical pathogens.},
  langid = {english},
  keywords = {Data accuracy,Ebola epidemic,Forecasting challenge,Mathematical modeling,Model comparison,Prediction horizon,Prediction performance,Synthetic data},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/FIZA5NMQ/Viboud et al. - 2018 - The RAPIDD ebola forecasting challenge Synthesis .pdf;/Users/nikos/github-synced/zotero-nikos/storage/KA34URD6/S1755436517301275.html}
}

@article{vovkNonparametricPredictiveDistributions2019,
  title = {Nonparametric Predictive Distributions Based on Conformal Prediction},
  author = {Vovk, Vladimir and Shen, Jieli and Manokhin, Valery and Xie, Min-ge},
  year = {2019},
  month = mar,
  journal = {Machine Learning},
  volume = {108},
  number = {3},
  pages = {445--474},
  issn = {1573-0565},
  doi = {10.1007/s10994-018-5755-8},
  urldate = {2021-11-07},
  abstract = {This paper applies conformal prediction to derive predictive distributions that are valid under a nonparametric assumption. Namely, we introduce and explore predictive distribution functions that always satisfy a natural property of validity in terms of guaranteed coverage for IID observations. The focus is on a prediction algorithm that we call the Least Squares Prediction Machine (LSPM). The LSPM generalizes the classical Dempster--Hill predictive distributions to nonparametric regression problems. If the standard parametric assumptions for Least Squares linear regression hold, the LSPM is as efficient as the Dempster--Hill procedure, in a natural sense. And if those parametric assumptions fail, the LSPM is still valid, provided the observations are IID.},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/KH7BBVMI/Vovk et al. - 2019 - Nonparametric predictive distributions based on co.pdf}
}

@article{wallingaHowGenerationIntervals2006,
  title = {How Generation Intervals Shape the Relationship between Growth Rates and Reproductive Numbers},
  author = {Wallinga, J and Lipsitch, M},
  year = {2006},
  month = nov,
  journal = {Proceedings of the Royal Society B: Biological Sciences},
  volume = {274},
  number = {1609},
  pages = {599--604},
  publisher = {Royal Society},
  doi = {10.1098/rspb.2006.3754},
  urldate = {2023-02-22},
  abstract = {Mathematical models of transmission have become invaluable management tools in planning for the control of emerging infectious diseases. A key variable in such models is the reproductive number R. For new emerging infectious diseases, the value of the reproductive number can only be inferred indirectly from the observed exponential epidemic growth rate r. Such inference is ambiguous as several different equations exist that relate the reproductive number to the growth rate, and it is unclear which of these equations might apply to a new infection. Here, we show that these different equations differ only with respect to their assumed shape of the generation interval distribution. Therefore, the shape of the generation interval distribution determines which equation is appropriate for inferring the reproductive number from the observed growth rate. We show that by assuming all generation intervals to be equal to the mean, we obtain an upper bound to the range of possible values that the reproductive number may attain for a given growth rate. Furthermore, we show that by taking the generation interval distribution equal to the observed distribution, it is possible to obtain an empirical estimate of the reproductive number.},
  keywords = {basic reproduction ratio,epidemiology,influenza,Lotka-Euler equation,serial interval},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/ZY9TWRMB/Wallinga and Lipsitch - 2006 - How generation intervals shape the relationship be.pdf}
}

@article{wallingaHowGenerationIntervals2007,
  title = {How Generation Intervals Shape the Relationship between Growth Rates and Reproductive Numbers},
  author = {Wallinga, J and Lipsitch, M},
  year = {2007},
  month = feb,
  journal = {Proceedings of the Royal Society B: Biological Sciences},
  volume = {274},
  number = {1609},
  pages = {599--604},
  publisher = {Royal Society},
  doi = {10.1098/rspb.2006.3754},
  urldate = {2022-11-29},
  abstract = {Mathematical models of transmission have become invaluable management tools in planning for the control of emerging infectious diseases. A key variable in such models is the reproductive number R. For new emerging infectious diseases, the value of the reproductive number can only be inferred indirectly from the observed exponential epidemic growth rate r. Such inference is ambiguous as several different equations exist that relate the reproductive number to the growth rate, and it is unclear which of these equations might apply to a new infection. Here, we show that these different equations differ only with respect to their assumed shape of the generation interval distribution. Therefore, the shape of the generation interval distribution determines which equation is appropriate for inferring the reproductive number from the observed growth rate. We show that by assuming all generation intervals to be equal to the mean, we obtain an upper bound to the range of possible values that the reproductive number may attain for a given growth rate. Furthermore, we show that by taking the generation interval distribution equal to the observed distribution, it is possible to obtain an empirical estimate of the reproductive number.},
  keywords = {basic reproduction ratio,epidemiology,influenza,Lotka-Euler equation,serial interval},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/LHAEKWXA/Wallinga and Lipsitch - 2007 - How generation intervals shape the relationship be.pdf}
}

@article{wangScoreBasedTestsDifferential2018,
  title = {Score-{{Based Tests}} of {{Differential Item Functioning}} via {{Pairwise Maximum Likelihood Estimation}}},
  author = {Wang, Ting and Strobl, Carolin and Zeileis, Achim and Merkle, Edgar C.},
  year = {2018},
  month = mar,
  journal = {Psychometrika},
  volume = {83},
  number = {1},
  pages = {132--155},
  issn = {1860-0980},
  doi = {10.1007/s11336-017-9591-8},
  urldate = {2022-03-22},
  abstract = {Measurement invariance is a fundamental assumption in item response theory models, where the relationship between a latent construct (ability) and observed item responses is of interest. Violation of this assumption would render the scale misinterpreted or cause systematic bias against certain groups of persons. While a number of methods have been proposed to detect measurement invariance violations, they typically require advance definition of problematic item parameters and respondent grouping information. However, these pieces of information are typically unknown in practice. As an alternative, this paper focuses on a family of recently proposed tests based on stochastic processes of casewise derivatives of the likelihood function (i.e., scores). These score-based tests only require estimation of the null model (when measurement invariance is assumed to hold), and they have been previously applied in factor-analytic, continuous data contexts as well as in models of the Rasch family. In this paper, we aim to extend these tests to two-parameter item response models, with strong emphasis on pairwise maximum likelihood. The tests' theoretical background and implementation are detailed, and the tests' abilities to identify problematic item parameters are studied via simulation. An empirical example illustrating the tests' use in practice is also provided.},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/XUEG42JI/Wang et al. - 2018 - Score-Based Tests of Differential Item Functioning.pdf}
}

@article{wattanachitComparisonCombinationMethods2022,
  title = {Comparison of {{Combination Methods}} to {{Create Calibrated Ensemble Forecasts}} for {{Seasonal Influenza}} in the {{U}}.{{S}}},
  author = {Wattanachit, Nutcha and Ray, Evan L. and McAndrew, Thomas C. and Reich, Nicholas G.},
  year = {2022},
  month = feb,
  journal = {arXiv:2202.11834 [stat]},
  eprint = {2202.11834},
  primaryclass = {stat},
  urldate = {2022-03-03},
  abstract = {The characteristics of influenza seasons varies substantially from year to year, posing challenges for public health preparation and response. Influenza forecasting is used to inform seasonal outbreak response, which can in turn potentially reduce the societal impact of an epidemic. The United States Centers for Disease Control and Prevention, in collaboration with external researchers, has run an annual prospective influenza forecasting exercise, known as the FluSight challenge. A subset of participating teams has worked together to produce a collaborative multi-model ensemble, the FluSight Network ensemble. Uniting theoretical results from the forecasting literature with domain-specific forecasts from influenza outbreaks, we applied parametric forecast combination methods that simultaneously optimize individual model weights and calibrate the ensemble via a beta transformation. We used the beta-transformed linear pool and the finite beta mixture model to produce ensemble forecasts retrospectively for the 2016/2017 to 2018/2019 influenza seasons in the U.S. We compared their performance to methods currently used in the FluSight challenge, namely the equally weighted linear pool and the linear pool. Ensemble forecasts produced from methods with a beta transformation were shown to outperform those from the equally weighted linear pool and the linear pool for all week-ahead targets across in the test seasons based on average log scores. We observed improvements in overall accuracy despite the beta-transformed linear pool or beta mixture methods' modest under-prediction across all targets and seasons. Combination techniques that explicitly adjust for known calibration issues in linear pooling should be considered to improve ensemble probabilistic scores in outbreak settings.},
  archiveprefix = {arxiv},
  keywords = {Statistics - Applications},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/4RG8QA84/Wattanachit et al. - 2022 - Comparison of Combination Methods to Create Calibr.pdf;/Users/nikos/github-synced/zotero-nikos/storage/LWM3GRJT/2202.html}
}

@article{weijsKullbackLeiblerDivergence2010,
  title = {Kullback--{{Leibler Divergence}} as a {{Forecast Skill Score}} with {{Classic Reliability}}--{{Resolution}}--{{Uncertainty Decomposition}}},
  author = {Weijs, Steven V. and van Nooijen, Ronald and van de Giesen, Nick},
  year = {2010},
  month = sep,
  journal = {Monthly Weather Review},
  volume = {138},
  number = {9},
  pages = {3387--3399},
  publisher = {American Meteorological Society},
  issn = {1520-0493, 0027-0644},
  doi = {10.1175/2010MWR3229.1},
  urldate = {2023-09-25},
  abstract = {Abstract This paper presents a score that can be used for evaluating probabilistic forecasts of multicategory events. The score is a reinterpretation of the logarithmic score or ignorance score, now formulated as the relative entropy or Kullback--Leibler divergence of the forecast distribution from the observation distribution. Using the information--theoretical concepts of entropy and relative entropy, a decomposition into three components is presented, analogous to the classic decomposition of the Brier score. The information--theoretical twins of the components uncertainty, resolution, and reliability provide diagnostic information about the quality of forecasts. The overall score measures the information conveyed by the forecast. As was shown recently, information theory provides a sound framework for forecast verification. The new decomposition, which has proven to be very useful for the Brier score and is widely used, can help acceptance of the logarithmic score in meteorology.},
  chapter = {Monthly Weather Review},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/RZ8EF5RE/Weijs et al. - 2010 - Kullback‚ÄìLeibler Divergence as a Forecast Skill Sc.pdf}
}

@book{westBayesianForecastingDynamic1997,
  title = {Bayesian {{Forecasting}} and {{Dynamic Models}}},
  author = {West, Mike and Harrison, Jeff},
  year = {1997},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {Springer-Verlag},
  address = {New York},
  doi = {10.1007/b98971},
  urldate = {2024-06-16},
  copyright = {http://www.springer.com/tdm},
  isbn = {978-0-387-94725-9},
  langid = {english},
  keywords = {forecasting,modeling,Rang,regression,time series analysis,Trend},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/6ICDVJ4N/1997 - Bayesian Forecasting and Dynamic Models.pdf}
}

@article{winklerDecisionTheoreticApproachInterval1972,
  title = {A {{Decision-Theoretic Approach}} to {{Interval Estimation}}},
  author = {Winkler, Robert L.},
  year = {1972},
  month = mar,
  journal = {Journal of the American Statistical Association},
  volume = {67},
  number = {337},
  pages = {187--191},
  publisher = {Taylor \& Francis},
  issn = {0162-1459},
  doi = {10.1080/01621459.1972.10481224},
  urldate = {2024-06-16},
  abstract = {Under an appropriate loss function, interval estimation may be regarded as a Bayesian decision-making procedure in which the objective is to find an interval that minimizes expected loss. For various loss functions, the behavior of the optimal interval is investigated, a comparison is made with the usual non-decision-theoretic interval estimates, and applications and examples are discussed.}
}

@article{winklerGoodProbabilityAssessors1968,
  title = {``{{Good}}'' {{Probability Assessors}}},
  author = {Winkler, Robert L. and Murphy, Allan H.},
  year = {1968},
  month = oct,
  issn = {1520-0450},
  urldate = {2024-06-16},
  abstract = {Abstract Since a meteorologist's predictions are subjective, a framework for the evaluation of meteorological probability assessors must be consistent with the theory of subjective probability. Such a framework is described in this paper. First, two standards of ``goodness,'' one based upon normative considerations and one based upon substantive considerations, are proposed. Specific properties which a meteorologist's assessments should possess are identified for each standard. Then, several measures of ``goodness,'' or scoring rules, which indicate the extent to which such assessments possess certain properties, are described. Finally, several important uses of these scoring rules are considered.},
  chapter = {Journal of Applied Meteorology and Climatology},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/QLS5NSGN/Winkler and Murphy - 1968 - ‚ÄúGood‚Äù Probability Assessors.pdf}
}

@article{winklerScoringRulesEvaluation1996,
  title = {Scoring Rules and the Evaluation of Probabilities},
  author = {Winkler, R. L. and Mu{\~n}oz, Javier and Cervera, Jos{\'e} L. and Bernardo, Jos{\'e} M. and Blattenberger, Gail and Kadane, Joseph B. and Lindley, Dennis V. and Murphy, Allan H. and Oliver, Robert M. and {R{\'i}os-Insua}, David},
  year = {1996},
  month = jun,
  journal = {Test},
  volume = {5},
  number = {1},
  pages = {1--60},
  issn = {1863-8260},
  doi = {10.1007/BF02562681},
  urldate = {2021-03-04},
  abstract = {In Bayesian inference and decision analysis, inferences and predictions are inherently probabilistic in nature. Scoring rules, which involve the computation of a score based on probability forecasts and what actually occurs, can be used to evaluate probabilities and to provide appropriate incentives for ``good'' probabilities. This paper review scoring rules and some related measures for evaluating probabilities, including decompositions of scoring rules and attributes of ``goodness'' of probabilites, comparability of scores, and the design of scoring rules for specific inferential and decision-making problems},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/VHTQR6BK/Winkler et al. - 1996 - Scoring rules and the evaluation of probabilities.pdf}
}

@article{woodCorrectingErrorsStreamflow2008,
  title = {Correcting {{Errors}} in {{Streamflow Forecast Ensemble Mean}} and {{Spread}}},
  author = {Wood, Andrew W. and Schaake, John C.},
  year = {2008},
  month = feb,
  journal = {Journal of Hydrometeorology},
  volume = {9},
  number = {1},
  pages = {132--148},
  publisher = {American Meteorological Society},
  issn = {1525-7541, 1525-755X},
  doi = {10.1175/2007JHM862.1},
  urldate = {2021-10-28},
  abstract = {{$<$}section class="abstract"{$><$}h2 class="abstractTitle text-title my-1" id="d8151315e70"{$>$}Abstract{$<$}/h2{$><$}p{$>$}When hydrological models are used for probabilistic streamflow forecasting in the Ensemble Streamflow Prediction (ESP) framework, the deterministic components of the approach can lead to errors in the estimation of forecast uncertainty, as represented by the spread of the forecast ensemble. One avenue for correcting the resulting forecast reliability errors is to calibrate the streamflow forecast ensemble to match observed error characteristics. This paper outlines and evaluates a method for forecast calibration as applied to seasonal streamflow prediction. The approach uses the correlation of forecast ensemble means with observations to generate a conditional forecast mean and spread that lie between the climatological mean and spread (when the forecast has no skill) and the raw forecast mean with zero spread (when the forecast is perfect). Retrospective forecasts of summer period runoff in the Feather River basin, California, are used to demonstrate that the approach improves upon the performance of traditional ESP forecasts by reducing errors in forecast mean and improving spread estimates, thereby increasing forecast reliability and skill.{$<$}/p{$><$}/section{$>$}},
  chapter = {Journal of Hydrometeorology},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/HEQKNQMR/Wood and Schaake - 2008 - Correcting Errors in Streamflow Forecast Ensemble .pdf;/Users/nikos/github-synced/zotero-nikos/storage/5UWCG24J/2007jhm862_1.html}
}

@article{yamanaSuperensembleForecastsDengue2016,
  title = {Superensemble Forecasts of Dengue Outbreaks},
  author = {Yamana, Teresa K. and Kandula, Sasikiran and Shaman, Jeffrey},
  year = {2016},
  month = oct,
  journal = {Journal of The Royal Society Interface},
  volume = {13},
  number = {123},
  pages = {20160410},
  publisher = {Royal Society},
  doi = {10.1098/rsif.2016.0410},
  urldate = {2021-05-30},
  abstract = {In recent years, a number of systems capable of predicting future infectious disease incidence have been developed. As more of these systems are operationalized, it is important that the forecasts generated by these different approaches be formally reconciled so that individual forecast error and bias are reduced. Here we present a first example of such multi-system, or superensemble, forecast. We develop three distinct systems for predicting dengue, which are applied retrospectively to forecast outbreak characteristics in San Juan, Puerto Rico. We then use Bayesian averaging methods to combine the predictions from these systems and create superensemble forecasts. We demonstrate that on average, the superensemble approach produces more accurate forecasts than those made from any of the individual forecasting systems.},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/E5APZHIN/Yamana et al. - 2016 - Superensemble forecasts of dengue outbreaks.pdf;/Users/nikos/github-synced/zotero-nikos/storage/2RH9MFEI/rsif.2016.html}
}

@article{yuVarianceStabilizingTransformations2009,
  title = {Variance Stabilizing Transformations of {{Poisson}}, Binomial and Negative Binomial Distributions},
  author = {Yu, Guan},
  year = {2009},
  month = jul,
  journal = {Statistics \& Probability Letters},
  volume = {79},
  number = {14},
  pages = {1621--1629},
  issn = {01677152},
  doi = {10.1016/j.spl.2009.04.010},
  urldate = {2022-12-07},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/9WEQEI2D/Yu - 2009 - Variance stabilizing transformations of Poisson, b.pdf}
}

@article{zellnerSurveyHumanJudgement2021,
  title = {A Survey of Human Judgement and Quantitative Forecasting Methods},
  author = {Zellner, Maximilian and Abbas, Ali E. and Budescu, David V. and Galstyan, Aram},
  year = {2021},
  month = feb,
  journal = {Royal Society Open Science},
  volume = {8},
  number = {2},
  pages = {201187},
  publisher = {Royal Society},
  doi = {10.1098/rsos.201187},
  urldate = {2023-02-01},
  abstract = {This paper's top-level goal is to provide an overview of research conducted in the many academic domains concerned with forecasting. By providing a summary encompassing these domains, this survey connects them, establishing a common ground for future discussions. To this end, we survey literature on human judgement and quantitative forecasting as well as hybrid methods that involve both humans and algorithmic approaches. The survey starts with key search terms that identified more than 280 publications in the fields of computer science, operations research, risk analysis, decision science, psychology and forecasting. Results show an almost 10-fold increase in the application-focused forecasting literature between the 1990s and the current decade, with a clear rise of quantitative, data-driven forecasting models. Comparative studies of quantitative methods and human judgement show that (1) neither method is universally superior, and (2) the better method varies as a function of factors such as availability, quality, extent and format of data, suggesting that (3) the two approaches can complement each other to yield more accurate and resilient models. We also identify four research thrusts in the human/machine-forecasting literature: (i) the choice of the appropriate quantitative model, (ii) the nature of the interaction between quantitative models and human judgement, (iii) the training and incentivization of human forecasters, and (iv) the combination of multiple forecasts (both algorithmic and human) into one. This review surveys current research in all four areas and argues that future research in the field of human/machine forecasting needs to consider all of them when investigating predictive performance. We also address some of the ethical dilemmas that might arise due to the combination of quantitative models with human judgement.},
  keywords = {forecast combination,forecasting,human judgment,quantitative forecasting methods},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/WLF44NUH/Zellner et al. - 2021 - A survey of human judgement and quantitative forec.pdf}
}

@article{zhaoHowSuitableQuantile2017,
  title = {How {{Suitable}} Is {{Quantile Mapping For Postprocessing GCM Precipitation Forecasts}}?},
  author = {Zhao, Tongtiegang and Bennett, James C. and Wang, Q. J. and Schepen, Andrew and Wood, Andrew W. and Robertson, David E. and Ramos, Maria-Helena},
  year = {2017},
  month = may,
  journal = {Journal of Climate},
  volume = {30},
  number = {9},
  pages = {3185--3196},
  publisher = {American Meteorological Society},
  issn = {0894-8755, 1520-0442},
  doi = {10.1175/JCLI-D-16-0652.1},
  urldate = {2021-10-20},
  abstract = {{$<$}section class="abstract"{$><$}h2 class="abstractTitle text-title my-1" id="d85819389e126"{$>$}Abstract{$<$}/h2{$><$}p{$>$}GCMs are used by many national weather services to produce seasonal outlooks of atmospheric and oceanic conditions and fluxes. Postprocessing is often a necessary step before GCM forecasts can be applied in practice. Quantile mapping (QM) is rapidly becoming the method of choice by operational agencies to postprocess raw GCM outputs. The authors investigate whether QM is appropriate for this task. Ensemble forecast postprocessing methods should aim to 1) correct bias, 2) ensure forecasts are reliable in ensemble spread, and 3) guarantee forecasts are at least as skillful as climatology, a property called ``coherence.'' This study evaluates the effectiveness of QM in achieving these aims by applying it to precipitation forecasts from the POAMA model. It is shown that while QM is highly effective in correcting bias, it cannot ensure reliability in forecast ensemble spread or guarantee coherence. This is because QM ignores the correlation between raw ensemble forecasts and observations. When raw forecasts are not significantly positively correlated with observations, QM tends to produce negatively skillful forecasts. Even when there is significant positive correlation, QM cannot ensure reliability and coherence for postprocessed forecasts. Therefore, QM is not a fully satisfactory method for postprocessing forecasts where the issues of bias, reliability, and coherence pre-exist. Alternative postprocessing methods based on ensemble model output statistics (EMOS) are available that achieve not only unbiased but also reliable and coherent forecasts. This is shown with one such alternative, the Bayesian joint probability modeling approach.{$<$}/p{$><$}/section{$>$}},
  chapter = {Journal of Climate},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/VXLIYGAF/Zhao et al. - 2017 - How Suitable is Quantile Mapping For Postprocessin.pdf;/Users/nikos/github-synced/zotero-nikos/storage/W6ZB73W8/jcli-d-16-0652.1.html}
}

@article{zhouModifiedDieboldMariano2021,
  title = {A Modified {{Diebold}}--{{Mariano}} Test for Equal Forecast Accuracy with Clustered Dependence},
  author = {Zhou, Jin and Li, Haiqi and Zhong, Wanling},
  year = {2021},
  month = oct,
  journal = {Economics Letters},
  volume = {207},
  pages = {110029},
  issn = {01651765},
  doi = {10.1016/j.econlet.2021.110029},
  urldate = {2023-09-24},
  abstract = {This study proposes a modified Diebold--Mariano (DM) test for equal forecast accuracy with clustered dependence. A novel consistent long-run variance estimator is developed to account for the clustered dependence. The modified DM test statistic asymptotically follows a normal distribution. The moving block bootstrap is employed to improve the size and power performance of the newly proposed test. A Monte Carlo simulation shows that the modified DM test has a better finite sample performance than the conventional DM test.},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/EN7PDR88/Zhou et al. - 2021 - A modified Diebold‚ÄìMariano test for equal forecast.pdf}
}

@article{zielEnergyDistanceEnsemble2021,
  title = {The Energy Distance for Ensemble and Scenario Reduction},
  author = {Ziel, Florian},
  year = {2021},
  month = jun,
  journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {379},
  number = {2202},
  pages = {20190431},
  publisher = {Royal Society},
  doi = {10.1098/rsta.2019.0431},
  urldate = {2023-12-12},
  abstract = {Scenario reduction techniques are widely applied for solving sophisticated dynamic and stochastic programs, especially in energy and power systems, but are also used in probabilistic forecasting, clustering and estimating generative adversarial networks. We propose a new method for ensemble and scenario reduction based on the energy distance which is a special case of the maximum mean discrepancy. We discuss the choice of energy distance in detail, especially in comparison to the popular Wasserstein distance which is dominating the scenario reduction literature. The energy distance is a metric between probability measures that allows for powerful tests for equality of arbitrary multivariate distributions or independence. Thanks to the latter, it is a suitable candidate for ensemble and scenario reduction problems. The theoretical properties and considered examples indicate clearly that the reduced scenario sets tend to exhibit better statistical properties for the energy distance than a corresponding reduction with respect to the Wasserstein distance. We show applications to a Bernoulli random walk and two real data-based examples for electricity demand profiles and day-ahead electricity prices. This article is part of the theme issue `The mathematics of energy systems'.},
  keywords = {electricity load,energy score,Kontorovic distance,maximum mean discrepancy,scenario reduction,stochastic programming,Wasserstein metric},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/ZIY7UT87/Ziel - 2021 - The energy distance for ensemble and scenario redu.pdf}
}

@misc{zouForecastingFutureWorld2022,
  title = {Forecasting {{Future World Events}} with {{Neural Networks}}},
  author = {Zou, Andy and Xiao, Tristan and Jia, Ryan and Kwon, Joe and Mazeika, Mantas and Li, Richard and Song, Dawn and Steinhardt, Jacob and Evans, Owain and Hendrycks, Dan},
  year = {2022},
  month = jun,
  journal = {arXiv.org},
  urldate = {2024-03-08},
  abstract = {Forecasting future world events is a challenging but valuable task. Forecasts of climate, geopolitical conflict, pandemics and economic indicators help shape policy and decision making. In these domains, the judgment of expert humans contributes to the best forecasts. Given advances in language modeling, can these forecasts be automated? To this end, we introduce Autocast, a dataset containing thousands of forecasting questions and an accompanying news corpus. Questions are taken from forecasting tournaments, ensuring high quality, real-world importance, and diversity. The news corpus is organized by date, allowing us to precisely simulate the conditions under which humans made past forecasts (avoiding leakage from the future). Motivated by the difficulty of forecasting numbers across orders of magnitude (e.g. global cases of COVID-19 in 2022), we also curate IntervalQA, a dataset of numerical questions and metrics for calibration. We test language models on our forecasting task and find that performance is far below a human expert baseline. However, performance improves with increased model size and incorporation of relevant information from the news corpus. In sum, Autocast poses a novel challenge for large language models and improved performance could bring large practical benefits.},
  howpublished = {https://arxiv.org/abs/2206.15474v2},
  langid = {english},
  keywords = {Forecasting AI},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/8Z3VHIWT/Zou et al. - 2022 - Forecasting Future World Events with Neural Networ.pdf}
}

@article{10.12688/wellcomeopenres.19380.1,
  title = {Human Judgement Forecasting of {{COVID-19}} in the {{UK}} [Version 1; Peer Review: 1 Approved with Reservations]},
  author = {Bosse, {\relax NI} and Abbott, S and Bracher, J and {van Leeuwen}, E and Cori, A and Funk, S},
  year = {2023},
  journal = {Wellcome Open Research},
  volume = {8},
  number = {416},
  doi = {10.12688/wellcomeopenres.19380.1}
}

@article{arvanIntegratingHumanJudgement2019,
  title = {Integrating Human Judgement into Quantitative Forecasting Methods: {{A}} Review},
  shorttitle = {Integrating Human Judgement into Quantitative Forecasting Methods},
  author = {Arvan, Meysam and Fahimnia, Behnam and Reisi, Mohsen and Siemsen, Enno},
  year = {2019},
  month = jul,
  journal = {Omega},
  volume = {86},
  pages = {237--252},
  issn = {0305-0483},
  doi = {10.1016/j.omega.2018.07.012},
  urldate = {2023-02-01},
  abstract = {Product forecasts are a critical input into sourcing, procurement, production, inventory, logistics, finance and marketing decisions. Numerous quantitative models have been developed and applied to generate and improve product forecasts. The use of human judgement, either solely or in conjunction with quantitative models, has been well researched in the academic literature and is a popular forecasting approach in industry practice. In the context of judgemental forecasting, methods that integrate an expert's judgement into quantitative forecasting models are commonly referred to as ``integrating forecasting'' methods. This paper presents a systematic review of the literature of judgemental demand forecasting with a focus placed on integrating methods. We explore the role of expert opinion and contextual information and discuss the application of behaviourally informed support systems. We also provide important directions for further research in these areas.},
  langid = {english},
  keywords = {Behavioural operations,Forecasting,Integrating methods,Judgement,Review},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/J62DMSUX/Arvan et al. - 2019 - Integrating human judgement into quantitative fore.pdf;/Users/nikos/github-synced/zotero-nikos/storage/KA544AL7/S0305048317311155.html}
}

@article{atanasovDistillingWisdomCrowds2016,
  title = {Distilling the {{Wisdom}} of {{Crowds}}: {{Prediction Markets}} vs. {{Prediction Polls}}},
  shorttitle = {Distilling the {{Wisdom}} of {{Crowds}}},
  author = {Atanasov, Pavel and Rescober, Phillip and Stone, Eric and Swift, Samuel A. and {Servan-Schreiber}, Emile and Tetlock, Philip and Ungar, Lyle and Mellers, Barbara},
  year = {2016},
  month = apr,
  journal = {Management Science},
  volume = {63},
  number = {3},
  pages = {691--706},
  publisher = {{INFORMS}},
  issn = {0025-1909},
  doi = {10.1287/mnsc.2015.2374},
  urldate = {2021-05-30},
  abstract = {We report the results of the first large-scale, long-term, experimental test between two crowdsourcing methods: prediction markets and prediction polls. More than 2,400 participants made forecasts on 261 events over two seasons of a geopolitical prediction tournament. Forecasters were randomly assigned to either prediction markets (continuous double auction markets) in which they were ranked based on earnings, or prediction polls in which they submitted probability judgments, independently or in teams, and were ranked based on Brier scores. In both seasons of the tournament, prices from the prediction market were more accurate than the simple mean of forecasts from prediction polls. However, team prediction polls outperformed prediction markets when forecasts were statistically aggregated using temporal decay, differential weighting based on past performance, and recalibration. The biggest advantage of prediction polls was at the beginning of long-duration questions. Results suggest that prediction polls with proper scoring feedback, collaboration features, and statistical aggregation are an attractive alternative to prediction markets for distilling the wisdom of crowds.This paper was accepted by Uri Gneezy, behavioral economics.},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/I4SLHN59/Atanasov et al. - 2016 - Distilling the Wisdom of Crowds Prediction Market.pdf;/Users/nikos/github-synced/zotero-nikos/storage/KGDWE6LP/mnsc.2015.html}
}

@techreport{bernicebrownDELPHIPROCESSMETHODOLOGY1968,
  title = {{{DELPHI PROCESS}}: {{A METHODOLOGY USED FOR THE ELICITATION OF OPINIONS OF EXPERTS}}},
  shorttitle = {{{DELPHI PROCESS}}},
  author = {{Bernice Brown}},
  year = {1968},
  month = feb,
  urldate = {2023-02-04},
  abstract = {The Delphi process is representative of an important class of techniques dealing with decision making situations. It involves one of the methodological aspects of modern practice in operations research, namely the reliance on judgment of experts.},
  chapter = {Technical Reports},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/QLCMMTNL/AD0675981.pdf}
}

@article{biggerstaffResultsCentersDisease2016,
  title = {Results from the Centers for Disease Control and Prevention's Predict the 2013{\textendash}2014 {{Influenza Season Challenge}}},
  author = {Biggerstaff, Matthew and Alper, David and Dredze, Mark and Fox, Spencer and Fung, Isaac Chun-Hai and Hickmann, Kyle S. and Lewis, Bryan and Rosenfeld, Roni and Shaman, Jeffrey and Tsou, Ming-Hsiang and Velardi, Paola and Vespignani, Alessandro and Finelli, Lyn and {for the Influenza Forecasting Contest Working Group}},
  year = {2016},
  month = jul,
  journal = {BMC Infectious Diseases},
  volume = {16},
  number = {1},
  pages = {357},
  issn = {1471-2334},
  doi = {10.1186/s12879-016-1669-x},
  urldate = {2021-10-13},
  abstract = {Early insights into the timing of the start, peak, and intensity of the influenza season could be useful in planning influenza prevention and control activities. To encourage development and innovation in influenza forecasting, the Centers for Disease Control and Prevention (CDC) organized a challenge to predict the 2013{\textendash}14 Unites States influenza season.},
  keywords = {Forecasting,Influenza,Modeling,Prediction},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/C554EIY4/Biggerstaff et al. - 2016 - Results from the centers for disease control and p.pdf;/Users/nikos/github-synced/zotero-nikos/storage/UFQ992UF/s12879-016-1669-x.html}
}

@article{bosseComparingHumanModelbased2022,
  title = {Comparing Human and Model-Based Forecasts of {{COVID-19}} in {{Germany}} and {{Poland}}},
  author = {Bosse, Nikos I. and Abbott, Sam and Bracher, Johannes and Hain, Habakuk and Quilty, Billy J. and Jit, Mark and Group, Centre for the Mathematical Modelling of Infectious Diseases COVID-19 Working and van Leeuwen, Edwin and Cori, Anne and Funk, Sebastian},
  year = {2022},
  month = sep,
  journal = {PLOS Computational Biology},
  volume = {18},
  number = {9},
  pages = {e1010405},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1010405},
  urldate = {2023-02-06},
  abstract = {Forecasts based on epidemiological modelling have played an important role in shaping public policy throughout the COVID-19 pandemic. This modelling combines knowledge about infectious disease dynamics with the subjective opinion of the researcher who develops and refines the model and often also adjusts model outputs. Developing a forecast model is difficult, resource- and time-consuming. It is therefore worth asking what modelling is able to add beyond the subjective opinion of the researcher alone. To investigate this, we analysed different real-time forecasts of cases of and deaths from COVID-19 in Germany and Poland over a 1-4 week horizon submitted to the German and Polish Forecast Hub. We compared crowd forecasts elicited from researchers and volunteers, against a) forecasts from two semi-mechanistic models based on common epidemiological assumptions and b) the ensemble of all other models submitted to the Forecast Hub. We found crowd forecasts, despite being overconfident, to outperform all other methods across all forecast horizons when forecasting cases (weighted interval score relative to the Hub ensemble 2 weeks ahead: 0.89). Forecasts based on computational models performed comparably better when predicting deaths (rel. WIS 1.26), suggesting that epidemiological modelling and human judgement can complement each other in important ways.},
  langid = {english},
  keywords = {Convolution,COVID 19,Data visualization,Epidemiology,Forecasting,Germany,Poland,Virus testing},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/QZ74W5WX/Bosse et al. - 2022 - Comparing human and model-based forecasts of COVID.pdf}
}

@article{bosseEvaluatingForecastsScoringutils2022,
  title = {Evaluating {{Forecasts}} with Scoringutils in {{R}}},
  author = {Bosse, Nikos I. and Gruson, Hugo and Cori, Anne and {van Leeuwen}, Edwin and Funk, Sebastian and Abbott, Sam},
  year = {2022},
  month = may,
  journal = {arXiv},
  eprint = {2205.07090},
  primaryclass = {stat},
  doi = {10.48550/arXiv.2205.07090},
  urldate = {2023-01-15},
  abstract = {Evaluating forecasts is essential in order to understand and improve forecasting and make forecasts useful to decision-makers. Much theoretical work has been done on the development of proper scoring rules and other scoring metrics that can help evaluate forecasts. In practice, however, conducting a forecast evaluation and comparison of different forecasters remains challenging. In this paper we introduce scoringutils, an R package that aims to greatly facilitate this process. It is especially geared towards comparing multiple forecasters, regardless of how forecasts were created, and visualising results. The package is able to handle missing forecasts and is the first R package to offer extensive support for forecasts represented through predictive quantiles, a format used by several collaborative ensemble forecasting efforts. The paper gives a short introduction to forecast evaluation, discusses the metrics implemented in scoringutils and gives guidance on when they are appropriate to use, and illustrates the application of the package using example data of forecasts for COVID-19 cases and deaths submitted to the European Forecast Hub between May and September 2021},
  archiveprefix = {arxiv},
  keywords = {Statistics - Applications,Statistics - Computation,Statistics - Methodology},
  annotation = {Preprint, {\textbackslash}url\{https://arxiv.org/abs/2205.07090\}},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/QW8YLYQZ/Bosse et al. - 2022 - Evaluating Forecasts with scoringutils in R.pdf;/Users/nikos/github-synced/zotero-nikos/storage/DNAA9E73/2205.html}
}

@article{bosseTransformationForecastsEvaluating2023,
  title = {Transformation of Forecasts for Evaluating Predictive Performance in an Epidemiological Context},
  author = {Bosse, Nikos I. and Abbott, Sam and Cori, Anne and {van Leeuwen}, Edwin and Bracher, Johannes and Funk, Sebastian},
  year = {2023},
  month = jan,
  doi = {10.1101/2023.01.23.23284722},
  urldate = {2023-02-06},
  abstract = {Abstract           Forecast evaluation plays an essential role in the development cycle of predictive epidemic models and can inform their use for public health decision-making. Common scores to evaluate epidemiological forecasts are the Continuous Ranked Probability Score (CRPS) and the Weighted Interval Score (WIS), which are both measures of the absolute distance between the forecast distribution and the observation. They are commonly applied directly to predicted and observed incidence counts, but it can be questioned whether this yields the most meaningful results given the exponential nature of epidemic processes and the several orders of magnitude that observed values can span over space and time. In this paper, we argue that log transforming counts before applying scores such as the CRPS or WIS can effectively mitigate these difficulties and yield epidemiologically meaningful and easily interpretable results. We motivate the procedure threefold using the CRPS on log-transformed counts as an example: Firstly, it can be interpreted as a probabilistic version of a relative error. Secondly, it reflects how well models predicted the time-varying epidemic growth rate. And lastly, using arguments on variance-stabilizing transformations, it can be shown that under the assumption of a quadratic mean-variance relationship, the logarithmic transformation leads to expected CRPS values which are independent of the order of magnitude of the predicted quantity. Applying the log transformation to data and forecasts from the European COVID-19 Forecast Hub, we find that it changes model rankings regardless of stratification by forecast date, location or target types. Situations in which models missed the beginning of upward swings are more strongly emphasized while failing to predict a downturn following a peak is less severely penalized. We conclude that appropriate transformations, of which the natural logarithm is only one particularly attractive option, should be considered when assessing the performance of different models in the context of infectious disease incidence.},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/C7J3FZBC/Bosse et al. - 2023 - Transformation of forecasts for evaluating predict.pdf}
}

@article{boxAnalysisTransformations1964,
  title = {An {{Analysis}} of {{Transformations}}},
  author = {Box, G. E. P. and Cox, D. R.},
  year = {1964},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {26},
  number = {2},
  eprint = {2984418},
  eprinttype = {jstor},
  pages = {211--252},
  publisher = {{[Royal Statistical Society, Wiley]}},
  issn = {0035-9246},
  urldate = {2023-01-09},
  abstract = {In the analysis of data it is often assumed that observations y\textsubscript{1}, y\textsubscript{2}, ..., y\textsubscript{n} are independently normally distributed with constant variance and with expectations specified by a model linear in a set of parameters {\texttheta}. In this paper we make the less restrictive assumption that such a normal, homoscedastic, linear model is appropriate after some suitable transformation has been applied to the y's. Inferences about the transformation and about the parameters of the linear model are made by computing the likelihood function and the relevant posterior distribution. The contributions of normality, homoscedasticity and additivity to the transformation are separated. The relation of the present methods to earlier procedures for finding transformations is discussed. The methods are illustrated with examples.},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/SNB9W2HD/Box and Cox - 1964 - An Analysis of Transformations.pdf}
}

@article{bracherEvaluatingEpidemicForecasts2021,
  title = {Evaluating Epidemic Forecasts in an Interval Format},
  author = {Bracher, Johannes and Ray, Evan L. and Gneiting, Tilmann and Reich, Nicholas G.},
  year = {2021},
  month = feb,
  journal = {PLoS computational biology},
  volume = {17},
  number = {2},
  pages = {e1008618},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1008618},
  abstract = {For practical reasons, many forecasts of case, hospitalization, and death counts in the context of the current Coronavirus Disease 2019 (COVID-19) pandemic are issued in the form of central predictive intervals at various levels. This is also the case for the forecasts collected in the COVID-19 Forecast Hub (https://covid19forecasthub.org/). Forecast evaluation metrics like the logarithmic score, which has been applied in several infectious disease forecasting challenges, are then not available as they require full predictive distributions. This article provides an overview of how established methods for the evaluation of quantile and interval forecasts can be applied to epidemic forecasts in this format. Specifically, we discuss the computation and interpretation of the weighted interval score, which is a proper score that approximates the continuous ranked probability score. It can be interpreted as a generalization of the absolute error to probabilistic forecasts and allows for a decomposition into a measure of sharpness and penalties for over- and underprediction.},
  langid = {english},
  pmcid = {PMC7880475},
  pmid = {33577550},
  keywords = {Communicable Diseases,COVID-19,Forecasting,Humans,Pandemics,Probability,SARS-CoV-2},
  annotation = {note: DOI 10.1371/journal.pcbi.1008618},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/EX37R6J8/Bracher et al. - 2021 - Evaluating epidemic forecasts in an interval forma.pdf}
}

@article{bracherNationalSubnationalShortterm2022,
  title = {National and Subnational Short-Term Forecasting of {{COVID-19}} in {{Germany}} and {{Poland}} during Early 2021},
  author = {Bracher, Johannes and Wolffram, Daniel and Deuschel, Jannik and G{\"o}rgen, Konstantin and Ketterer, Jakob L. and Ullrich, Alexander and Abbott, Sam and Barbarossa, Maria V. and Bertsimas, Dimitris and Bhatia, Sangeeta and Bodych, Marcin and Bosse, Nikos I. and Burgard, Jan Pablo and Castro, Lauren and Fairchild, Geoffrey and Fiedler, Jochen and Fuhrmann, Jan and Funk, Sebastian and Gambin, Anna and Gogolewski, Krzysztof and Heyder, Stefan and Hotz, Thomas and Kheifetz, Yuri and Kirsten, Holger and Krueger, Tyll and Krymova, Ekaterina and Leith{\"a}user, Neele and Li, Michael L. and Meinke, Jan H. and Miasojedow, B{\l}a{\.z}ej and Michaud, Isaac J. and Mohring, Jan and Nouvellet, Pierre and Nowosielski, Jedrzej M. and Ozanski, Tomasz and Radwan, Maciej and Rakowski, Franciszek and Scholz, Markus and Soni, Saksham and Srivastava, Ajitesh and Gneiting, Tilmann and Schienle, Melanie},
  year = {2022},
  month = oct,
  journal = {Communications Medicine},
  volume = {2},
  number = {1},
  pages = {1--17},
  publisher = {{Nature Publishing Group}},
  issn = {2730-664X},
  doi = {10.1038/s43856-022-00191-8},
  urldate = {2023-02-22},
  abstract = {During the COVID-19 pandemic there has been a strong interest in forecasts of the short-term development of epidemiological indicators to inform decision makers. In this study we evaluate probabilistic real-time predictions of confirmed cases and deaths from COVID-19 in Germany and Poland for the period from January through April 2021.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Epidemiology,Viral infection},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/N3WD8QGU/Bracher et al. - 2022 - National and subnational short-term forecasting of.pdf}
}

@article{bracherPreregisteredShorttermForecasting2021,
  title = {A Pre-Registered Short-Term Forecasting Study of {{COVID-19}} in {{Germany}} and {{Poland}} during the Second Wave},
  author = {Bracher, J. and Wolffram, D. and Deuschel, J. and G{\"o}rgen, K. and Ketterer, J. L. and Ullrich, A. and Abbott, S. and Barbarossa, M. V. and Bertsimas, D. and Bhatia, S. and Bodych, M. and Bosse, N. I. and Burgard, J. P. and Castro, L. and Fairchild, G. and Fuhrmann, J. and Funk, S. and Gogolewski, K. and Gu, Q. and Heyder, S. and Hotz, T. and Kheifetz, Y. and Kirsten, H. and Krueger, T. and Krymova, E. and Li, M. L. and Meinke, J. H. and Michaud, I. J. and Niedzielewski, K. and O{\.z}a{\'n}ski, T. and Rakowski, F. and Scholz, M. and Soni, S. and Srivastava, A. and Zieli{\'n}ski, J. and Zou, D. and Gneiting, T. and Schienle, M.},
  year = {2021},
  month = aug,
  journal = {Nature Communications},
  volume = {12},
  number = {1},
  pages = {5173},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-021-25207-0},
  urldate = {2023-02-22},
  abstract = {Disease modelling has had considerable policy impact during the ongoing COVID-19 pandemic, and it is increasingly acknowledged that combining multiple models can improve the reliability of outputs. Here we report insights from ten weeks of collaborative short-term forecasting of COVID-19 in Germany and Poland (12 October{\textendash}19 December 2020). The study period covers the onset of the second wave in both countries, with tightening non-pharmaceutical interventions (NPIs) and subsequently a decay (Poland) or plateau and renewed increase (Germany) in reported cases. Thirteen independent teams provided probabilistic real-time forecasts of COVID-19 cases and deaths. These were reported for lead times of one to four weeks, with evaluation focused on one- and two-week horizons, which are less affected by changing NPIs. Heterogeneity between forecasts was considerable both in terms of point predictions and forecast spread. Ensemble forecasts showed good relative performance, in particular in terms of coverage, but did not clearly dominate single-model predictions. The study was preregistered and will be followed up in future phases of the pandemic.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Computational models,Epidemiology,SARS-CoV-2,Statistics},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/7YDBDE7G/Bracher et al. - 2021 - A pre-registered short-term forecasting study of C.pdf}
}

@article{brehmerProperizationConstructingProper2020,
  title = {Properization: Constructing Proper Scoring Rules via {{Bayes}} Acts},
  shorttitle = {Properization},
  author = {Brehmer, Jonas R. and Gneiting, Tilmann},
  year = {2020},
  month = jun,
  journal = {Annals of the Institute of Statistical Mathematics},
  volume = {72},
  number = {3},
  pages = {659--673},
  issn = {1572-9052},
  doi = {10.1007/s10463-019-00705-7},
  urldate = {2021-03-26},
  abstract = {Scoring rules serve to quantify predictive performance. A scoring rule is proper if truth telling is an optimal strategy in expectation. Subject to customary regularity conditions, every scoring rule can be made proper, by applying a special case of the Bayes act construction studied by Gr{\"u}nwald and Dawid (Ann Stat 32:1367{\textendash}1433, 2004) and Dawid (Ann Inst Stat Math 59:77{\textendash}93, 2007), to which we refer as properization. We discuss examples from the recent literature and apply the construction to create new types, and reinterpret existing forms, of proper scoring rules and consistent scoring functions. In an abstract setting, we formulate sufficient conditions under which Bayes acts exist and scoring rules can be made proper.},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/5RSYDJY5/Brehmer and Gneiting - 2020 - Properization constructing proper scoring rules v.pdf}
}

@article{brierVerificationForecastsExpressed1950,
  title = {Verification of {{Forecasts Expressed}} in {{Terms}} of {{Probability}}},
  author = {Brier, Glenn W.},
  year = {1950},
  month = jan,
  journal = {Monthly Weather Review},
  volume = {78},
  number = {1},
  pages = {1--3},
  publisher = {{American Meteorological Society}},
  issn = {1520-0493, 0027-0644},
  doi = {10.1175/1520-0493(1950)078<0001:VOFEIT>2.0.CO;2},
  urldate = {2022-01-21},
  abstract = {Abstract No Abstract Available.},
  chapter = {Monthly Weather Review},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/ZCBG3Z38/Brier - 1950 - VERIFICATION OF FORECASTS EXPRESSED IN TERMS OF PR.pdf;/Users/nikos/github-synced/zotero-nikos/storage/I83583N3/1520-0493_1950_078_0001_vofeit_2_0_co_2.html}
}

@article{brooksNonmechanisticForecastsSeasonal2018,
  title = {Nonmechanistic Forecasts of Seasonal Influenza with Iterative One-Week-Ahead Distributions},
  author = {Brooks, Logan C. and Farrow, David C. and Hyun, Sangwon and Tibshirani, Ryan J. and Rosenfeld, Roni},
  editor = {Viboud, Cecile},
  year = {2018},
  month = jun,
  journal = {PLOS Computational Biology},
  volume = {14},
  number = {6},
  pages = {e1006134},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1006134},
  urldate = {2020-03-29},
  abstract = {Accurate and reliable forecasts of seasonal epidemics of infectious disease can assist in the design of countermeasures and increase public awareness and preparedness. This article describes two main contributions we made recently toward this goal: a novel approach to probabilistic modeling of surveillance time series based on ``delta densities'', and an optimization scheme for combining output from multiple forecasting methods into an adaptively weighted ensemble. Delta densities describe the probability distribution of the change between one observation and the next, conditioned on available data; chaining together nonparametric estimates of these distributions yields a model for an entire trajectory. Corresponding distributional forecasts cover more observed events than alternatives that treat the whole season as a unit, and improve upon multiple evaluation metrics when extracting key targets of interest to public health officials. Adaptively weighted ensembles integrate the results of multiple forecasting methods, such as delta density, using weights that can change from situation to situation. We treat selection of optimal weightings across forecasting methods as a separate estimation task, and describe an estimation procedure based on optimizing cross-validation performance. We consider some details of the data generation process, including data revisions and holiday effects, both in the construction of these forecasting methods and when performing retrospective evaluation. The delta density method and an adaptively weighted ensemble of other forecasting methods each improve significantly on the next best ensemble component when applied separately, and achieve even better cross-validated performance when used in conjunction. We submitted real-time forecasts based on these contributions as part of CDC's 2015/2016 FluSight Collaborative Comparison. Among the fourteen submissions that season, this system was ranked by CDC as the most accurate.},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/KWJ8KSUS/Brooks et al. - 2018 - Nonmechanistic forecasts of seasonal influenza wit.pdf}
}

@article{claeskensForecastCombinationPuzzle2016,
  title = {The Forecast Combination Puzzle: {{A}} Simple Theoretical Explanation},
  shorttitle = {The Forecast Combination Puzzle},
  author = {Claeskens, Gerda and Magnus, Jan R. and Vasnev, Andrey L. and Wang, Wendun},
  year = {2016},
  month = jul,
  journal = {International Journal of Forecasting},
  volume = {32},
  number = {3},
  pages = {754--762},
  issn = {01692070},
  doi = {10.1016/j.ijforecast.2015.12.005},
  urldate = {2021-07-02},
  abstract = {This paper offers a theoretical explanation for the stylized fact that forecast combinations with estimated optimal weights often perform poorly in applications. The properties of the forecast combination are typically derived under the assumption that the weights are fixed, while in practice they need to be estimated. If the fact that the weights are random rather than fixed is taken into account during the optimality derivation, then the forecast combination will be biased (even when the original forecasts are unbiased), and its variance will be larger than in the fixed-weight case. In particular, there is no guarantee that the `optimal' forecast combination will be better than the equal-weight case, or even improve on the original forecasts. We provide the underlying theory, some special cases, and a numerical illustration.},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/4TUUCQXG/Claeskens et al. - 2016 - The forecast combination puzzle A simple theoreti.pdf}
}

@article{cramerEvaluationIndividualEnsemble2021,
  title = {Evaluation of Individual and Ensemble Probabilistic Forecasts of {{COVID-19}} Mortality in the {{US}}},
  author = {Cramer, Estee and Ray, Evan L. and Lopez, Velma K. and Bracher, Johannes and Brennen, Andrea and Rivadeneira, Alvaro J. Castro and Gerding, Aaron and Gneiting, Tilmann and House, Katie H. and Huang, Yuxin and Jayawardena, Dasuni and Kanji, Abdul H. and Khandelwal, Ayush and Le, Khoa and M{\"u}hlemann, Anja and Niemi, Jarad and Shah, Apurv and Stark, Ariane and Wang, Yijin and Wattanachit, Nutcha and Zorn, Martha W. and Gu, Youyang and Jain, Sansiddh and Bannur, Nayana and Deva, Ayush and Kulkarni, Mihir and Merugu, Srujana and Raval, Alpan and Shingi, Siddhant and Tiwari, Avtansh and White, Jerome and Woody, Spencer and Dahan, Maytal and Fox, Spencer and Gaither, Kelly and Lachmann, Michael and Meyers, Lauren Ancel and Scott, James G. and Tec, Mauricio and Srivastava, Ajitesh and George, Glover E. and Cegan, Jeffrey C. and Dettwiller, Ian D. and England, William P. and Farthing, Matthew W. and Hunter, Robert H. and Lafferty, Brandon and Linkov, Igor and Mayo, Michael L. and Parno, Matthew D. and Rowland, Michael A. and Trump, Benjamin D. and Corsetti, Sabrina M. and Baer, Thomas M. and Eisenberg, Marisa C. and Falb, Karl and Huang, Yitao and Martin, Emily T. and McCauley, Ella and Myers, Robert L. and Schwarz, Tom and Sheldon, Daniel and Gibson, Graham Casey and Yu, Rose and Gao, Liyao and Ma, Yian and Wu, Dongxia and Yan, Xifeng and Jin, Xiaoyong and Wang, Yu-Xiang and Chen, YangQuan and Guo, Lihong and Zhao, Yanting and Gu, Quanquan and Chen, Jinghui and Wang, Lingxiao and Xu, Pan and Zhang, Weitong and Zou, Difan and Biegel, Hannah and Lega, Joceline and Snyder, Timothy L. and Wilson, Davison D. and McConnell, Steve and Walraven, Robert and Shi, Yunfeng and Ban, Xuegang and Hong, Qi-Jun and Kong, Stanley and Turtle, James A. and {Ben-Nun}, Michal and Riley, Pete and Riley, Steven and Koyluoglu, Ugur and DesRoches, David and Hamory, Bruce and Kyriakides, Christina and Leis, Helen and Milliken, John and Moloney, Michael and Morgan, James and Ozcan, Gokce and Schrader, Chris and Shakhnovich, Elizabeth and Siegel, Daniel and Spatz, Ryan and Stiefeling, Chris and Wilkinson, Barrie and Wong, Alexander and Gao, Zhifeng and Bian, Jiang and Cao, Wei and Ferres, Juan Lavista and Li, Chaozhuo and Liu, Tie-Yan and Xie, Xing and Zhang, Shun and Zheng, Shun and Vespignani, Alessandro and Chinazzi, Matteo and Davis, Jessica T. and Mu, Kunpeng and y Piontti, Ana Pastore and Xiong, Xinyue and Zheng, Andrew and Baek, Jackie and Farias, Vivek and Georgescu, Andreea and Levi, Retsef and Sinha, Deeksha and Wilde, Joshua and Penna, Nicolas D. and Celi, Leo A. and Sundar, Saketh and Cavany, Sean and Espa{\~n}a, Guido and Moore, Sean and Oidtman, Rachel and Perkins, Alex and Osthus, Dave and Castro, Lauren and Fairchild, Geoffrey and Michaud, Isaac and Karlen, Dean and Lee, Elizabeth C. and Dent, Juan and Grantz, Kyra H. and Kaminsky, Joshua and Kaminsky, Kathryn and Keegan, Lindsay T. and Lauer, Stephen A. and Lemaitre, Joseph C. and Lessler, Justin and Meredith, Hannah R. and {Perez-Saez}, Javier and Shah, Sam and Smith, Claire P. and Truelove, Shaun A. and Wills, Josh and Kinsey, Matt and Obrecht, R. F. and Tallaksen, Katharine and Burant, John C. and Wang, Lily and Gao, Lei and Gu, Zhiling and Kim, Myungjin and Li, Xinyi and Wang, Guannan and Wang, Yueying and Yu, Shan and Reiner, Robert C. and Barber, Ryan and Gaikedu, Emmanuela and Hay, Simon and Lim, Steve and Murray, Chris and Pigott, David and Prakash, B. Aditya and Adhikari, Bijaya and Cui, Jiaming and Rodr{\'i}guez, Alexander and Tabassum, Anika and Xie, Jiajia and Keskinocak, Pinar and Asplund, John and Baxter, Arden and Oruc, Buse Eylul and Serban, Nicoleta and Arik, Sercan O. and Dusenberry, Mike and Epshteyn, Arkady and Kanal, Elli and Le, Long T. and Li, Chun-Liang and Pfister, Tomas and Sava, Dario and Sinha, Rajarishi and Tsai, Thomas and Yoder, Nate and Yoon, Jinsung and Zhang, Leyou and Abbott, Sam and Bosse, Nikos I. and Funk, Sebastian and Hellewel, Joel and Meakin, Sophie R. and Munday, James D. and Sherratt, Katherine and Zhou, Mingyuan and Kalantari, Rahi and Yamana, Teresa K. and Pei, Sen and Shaman, Jeffrey and Ayer, Turgay and Adee, Madeline and Chhatwal, Jagpreet and Dalgic, Ozden O. and Ladd, Mary A. and Linas, Benjamin P. and Mueller, Peter and Xiao, Jade and Li, Michael L. and Bertsimas, Dimitris and Lami, Omar Skali and Soni, Saksham and Bouardi, Hamza Tazi and Wang, Yuanjia and Wang, Qinxia and Xie, Shanghong and Zeng, Donglin and Green, Alden and Bien, Jacob and Hu, Addison J. and Jahja, Maria and Narasimhan, Balasubramanian and Rajanala, Samyak and Rumack, Aaron and Simon, Noah and Tibshirani, Ryan and Tibshirani, Rob and Ventura, Valerie and Wasserman, Larry and O'Dea, Eamon B. and Drake, John M. and Pagano, Robert and Walker, Jo W. and Slayton, Rachel B. and Johansson, Michael and Biggerstaff, Matthew and Reich, Nicholas G.},
  year = {2021},
  month = feb,
  journal = {medRxiv},
  pages = {2021.02.03.21250974},
  publisher = {{Cold Spring Harbor Laboratory Press}},
  doi = {10.1101/2021.02.03.21250974},
  urldate = {2021-04-06},
  abstract = {{$<$}h3{$>$}Abstract{$<$}/h3{$>$} {$<$}p{$>$}Short-term probabilistic forecasts of the trajectory of the COVID-19 pandemic in the United States have served as a visible and important communication channel between the scientific modeling community and both the general public and decision-makers. Forecasting models provide specific, quantitative, and evaluable predictions that inform short-term decisions such as healthcare staffing needs, school closures, and allocation of medical supplies. In 2020, the COVID-19 Forecast Hub (https://covid19forecasthub.org/) collected, disseminated, and synthesized hundreds of thousands of specific predictions from more than 50 different academic, industry, and independent research groups. This manuscript systematically evaluates 23 models that regularly submitted forecasts of reported weekly incident COVID-19 mortality counts in the US at the state and national level. One of these models was a multi-model ensemble that combined all available forecasts each week. The performance of individual models showed high variability across time, geospatial units, and forecast horizons. Half of the models evaluated showed better accuracy than a na{\"i}ve baseline model. In combining the forecasts from all teams, the ensemble showed the best overall probabilistic accuracy of any model. Forecast accuracy degraded as models made predictions farther into the future, with probabilistic accuracy at a 20-week horizon more than 5 times worse than when predicting at a 1-week horizon. This project underscores the role that collaboration and active coordination between governmental public health agencies, academic modeling teams, and industry partners can play in developing modern modeling capabilities to support local, state, and federal response to outbreaks.{$<$}/p{$>$}},
  copyright = {{\textcopyright} 2021, Posted by Cold Spring Harbor Laboratory. This article is a US Government work. It is not subject to copyright under 17 USC 105 and is also made available for use under a CC0 license},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/W82X9ZN5/Cramer et al. - 2021 - Evaluation of individual and ensemble probabilisti.pdf;/Users/nikos/github-synced/zotero-nikos/storage/7MC6LGTC/2021.02.03.21250974v1.html}
}

@article{cramerEvaluationIndividualEnsemble2022,
  title = {Evaluation of Individual and Ensemble Probabilistic Forecasts of {{COVID-19}} Mortality in the {{United States}}},
  author = {Cramer, Estee Y. and Ray, Evan L. and Lopez, Velma K. and Bracher, Johannes and Brennen, Andrea and Castro Rivadeneira, Alvaro J. and Gerding, Aaron and Gneiting, Tilmann and House, Katie H. and Huang, Yuxin and Jayawardena, Dasuni and Kanji, Abdul H. and Khandelwal, Ayush and Le, Khoa and M{\"u}hlemann, Anja and Niemi, Jarad and Shah, Apurv and Stark, Ariane and Wang, Yijin and Wattanachit, Nutcha and Zorn, Martha W. and Gu, Youyang and Jain, Sansiddh and Bannur, Nayana and Deva, Ayush and Kulkarni, Mihir and Merugu, Srujana and Raval, Alpan and Shingi, Siddhant and Tiwari, Avtansh and White, Jerome and Abernethy, Neil F. and Woody, Spencer and Dahan, Maytal and Fox, Spencer and Gaither, Kelly and Lachmann, Michael and Meyers, Lauren Ancel and Scott, James G. and Tec, Mauricio and Srivastava, Ajitesh and George, Glover E. and Cegan, Jeffrey C. and Dettwiller, Ian D. and England, William P. and Farthing, Matthew W. and Hunter, Robert H. and Lafferty, Brandon and Linkov, Igor and Mayo, Michael L. and Parno, Matthew D. and Rowland, Michael A. and Trump, Benjamin D. and {Zhang-James}, Yanli and Chen, Samuel and Faraone, Stephen V. and Hess, Jonathan and Morley, Christopher P. and Salekin, Asif and Wang, Dongliang and Corsetti, Sabrina M. and Baer, Thomas M. and Eisenberg, Marisa C. and Falb, Karl and Huang, Yitao and Martin, Emily T. and McCauley, Ella and Myers, Robert L. and Schwarz, Tom and Sheldon, Daniel and Gibson, Graham Casey and Yu, Rose and Gao, Liyao and Ma, Yian and Wu, Dongxia and Yan, Xifeng and Jin, Xiaoyong and Wang, Yu-Xiang and Chen, YangQuan and Guo, Lihong and Zhao, Yanting and Gu, Quanquan and Chen, Jinghui and Wang, Lingxiao and Xu, Pan and Zhang, Weitong and Zou, Difan and Biegel, Hannah and Lega, Joceline and McConnell, Steve and Nagraj, V. P. and Guertin, Stephanie L. and {Hulme-Lowe}, Christopher and Turner, Stephen D. and Shi, Yunfeng and Ban, Xuegang and Walraven, Robert and Hong, Qi-Jun and Kong, Stanley and {van de Walle}, Axel and Turtle, James A. and {Ben-Nun}, Michal and Riley, Steven and Riley, Pete and Koyluoglu, Ugur and DesRoches, David and Forli, Pedro and Hamory, Bruce and Kyriakides, Christina and Leis, Helen and Milliken, John and Moloney, Michael and Morgan, James and Nirgudkar, Ninad and Ozcan, Gokce and Piwonka, Noah and Ravi, Matt and Schrader, Chris and Shakhnovich, Elizabeth and Siegel, Daniel and Spatz, Ryan and Stiefeling, Chris and Wilkinson, Barrie and Wong, Alexander and Cavany, Sean and Espa{\~n}a, Guido and Moore, Sean and Oidtman, Rachel and Perkins, Alex and Kraus, David and Kraus, Andrea and Gao, Zhifeng and Bian, Jiang and Cao, Wei and Lavista Ferres, Juan and Li, Chaozhuo and Liu, Tie-Yan and Xie, Xing and Zhang, Shun and Zheng, Shun and Vespignani, Alessandro and Chinazzi, Matteo and Davis, Jessica T. and Mu, Kunpeng and {Pastore y Piontti}, Ana and Xiong, Xinyue and Zheng, Andrew and Baek, Jackie and Farias, Vivek and Georgescu, Andreea and Levi, Retsef and Sinha, Deeksha and Wilde, Joshua and Perakis, Georgia and Bennouna, Mohammed Amine and {Nze-Ndong}, David and Singhvi, Divya and Spantidakis, Ioannis and Thayaparan, Leann and Tsiourvas, Asterios and Sarker, Arnab and Jadbabaie, Ali and Shah, Devavrat and Della Penna, Nicolas and Celi, Leo A. and Sundar, Saketh and Wolfinger, Russ and Osthus, Dave and Castro, Lauren and Fairchild, Geoffrey and Michaud, Isaac and Karlen, Dean and Kinsey, Matt and Mullany, Luke C. and {Rainwater-Lovett}, Kaitlin and Shin, Lauren and Tallaksen, Katharine and Wilson, Shelby and Lee, Elizabeth C. and Dent, Juan and Grantz, Kyra H. and Hill, Alison L. and Kaminsky, Joshua and Kaminsky, Kathryn and Keegan, Lindsay T. and Lauer, Stephen A. and Lemaitre, Joseph C. and Lessler, Justin and Meredith, Hannah R. and {Perez-Saez}, Javier and Shah, Sam and Smith, Claire P. and Truelove, Shaun A. and Wills, Josh and Marshall, Maximilian and Gardner, Lauren and Nixon, Kristen and Burant, John C. and Wang, Lily and Gao, Lei and Gu, Zhiling and Kim, Myungjin and Li, Xinyi and Wang, Guannan and Wang, Yueying and Yu, Shan and Reiner, Robert C. and Barber, Ryan and Gakidou, Emmanuela and Hay, Simon I. and Lim, Steve and Murray, Chris and Pigott, David and Gurung, Heidi L. and Baccam, Prasith and Stage, Steven A. and Suchoski, Bradley T. and Prakash, B. Aditya and Adhikari, Bijaya and Cui, Jiaming and Rodr{\'i}guez, Alexander and Tabassum, Anika and Xie, Jiajia and Keskinocak, Pinar and Asplund, John and Baxter, Arden and Oruc, Buse Eylul and Serban, Nicoleta and Arik, Sercan O. and Dusenberry, Mike and Epshteyn, Arkady and Kanal, Elli and Le, Long T. and Li, Chun-Liang and Pfister, Tomas and Sava, Dario and Sinha, Rajarishi and Tsai, Thomas and Yoder, Nate and Yoon, Jinsung and Zhang, Leyou and Abbott, Sam and Bosse, Nikos I. and Funk, Sebastian and Hellewell, Joel and Meakin, Sophie R. and Sherratt, Katharine and Zhou, Mingyuan and Kalantari, Rahi and Yamana, Teresa K. and Pei, Sen and Shaman, Jeffrey and Li, Michael L. and Bertsimas, Dimitris and Skali Lami, Omar and Soni, Saksham and Tazi Bouardi, Hamza and Ayer, Turgay and Adee, Madeline and Chhatwal, Jagpreet and Dalgic, Ozden O. and Ladd, Mary A. and Linas, Benjamin P. and Mueller, Peter and Xiao, Jade and Wang, Yuanjia and Wang, Qinxia and Xie, Shanghong and Zeng, Donglin and Green, Alden and Bien, Jacob and Brooks, Logan and Hu, Addison J. and Jahja, Maria and McDonald, Daniel and Narasimhan, Balasubramanian and Politsch, Collin and Rajanala, Samyak and Rumack, Aaron and Simon, Noah and Tibshirani, Ryan J. and Tibshirani, Rob and Ventura, Valerie and Wasserman, Larry and O'Dea, Eamon B. and Drake, John M. and Pagano, Robert and Tran, Quoc T. and Ho, Lam Si Tung and Huynh, Huong and Walker, Jo W. and Slayton, Rachel B. and Johansson, Michael A. and Biggerstaff, Matthew and Reich, Nicholas G.},
  year = {2022},
  month = apr,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {119},
  number = {15},
  pages = {e2113561119},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.2113561119},
  urldate = {2024-02-06},
  abstract = {Short-term probabilistic forecasts of the trajectory of the COVID-19 pandemic in the United States have served as a visible and important communication channel between the scientific modeling community and both the general public and decision-makers. Forecasting models provide specific, quantitative, and evaluable predictions that inform short-term decisions such as healthcare staffing needs, school closures, and allocation of medical supplies. Starting in April 2020, the US COVID-19 Forecast Hub (https://covid19forecasthub.org/) collected, disseminated, and synthesized tens of millions of specific predictions from more than 90 different academic, industry, and independent research groups. A multimodel ensemble forecast that combined predictions from dozens of groups every week provided the most consistently accurate probabilistic forecasts of incident deaths due to COVID-19 at the state and national level from April 2020 through October 2021. The performance of 27 individual models that submitted complete forecasts of COVID-19 deaths consistently throughout this year showed high variability in forecast skill across time, geospatial units, and forecast horizons. Two-thirds of the models evaluated showed better accuracy than a na{\"i}ve baseline model. Forecast accuracy degraded as models made predictions further into the future, with probabilistic error at a 20-wk horizon three to five times larger than when predicting at a 1-wk horizon. This project underscores the role that collaboration and active coordination between governmental public-health agencies, academic modeling teams, and industry partners can play in developing modern modeling capabilities to support local, state, and federal response to outbreaks.},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/KADV8FFW/Cramer et al. - 2022 - Evaluation of individual and ensemble probabilisti.pdf}
}

@article{dalkeyExperimentalApplicationDELPHI1963,
  title = {An {{Experimental Application}} of the {{DELPHI Method}} to the {{Use}} of {{Experts}}},
  author = {Dalkey, Norman and Helmer, Olaf},
  year = {1963},
  month = apr,
  journal = {Management Science},
  volume = {9},
  number = {3},
  pages = {458--467},
  publisher = {{INFORMS}},
  issn = {0025-1909},
  doi = {10.1287/mnsc.9.3.458},
  urldate = {2023-02-04},
  abstract = {This paper gives an account of an experiment in the use of the so-called DELPHI method, which was devised in order to obtain the most reliable opinion consensus of a group of experts by subjecting them to a series of questionnaires in depth interspersed with controlled opinion feedback.}
}

@article{daviesHumanJudgementForecasting2022,
  title = {Human Judgement Forecasting Tournaments: {{A}} Feasibility Study Based on the {{COVID-19}} Pandemic with Public Health Practitioners in {{England}}},
  shorttitle = {Human Judgement Forecasting Tournaments},
  author = {Davies, Nathan and Ferris, Simon},
  year = {2022},
  month = jun,
  journal = {Public Health in Practice},
  volume = {3},
  pages = {100260},
  issn = {2666-5352},
  doi = {10.1016/j.puhip.2022.100260},
  urldate = {2023-02-01},
  abstract = {Objectives To explore the viability of running human judgement forecasting tournaments with public health practitioners, and to gather initial data on forecasting accuracy and participant perceptions of forecasting. Study design Quality improvement study comprising two COVID-19 forecasting tournaments using Brier Skill Score scoring and a follow-up participant questionnaire. Methods Over two forecasting tournaments, public health registrars in the East Midlands, UK, assigned probabilities to future possible binary events relating to COVID-19. Participants also completed a questionnaire on their experiences of forecasting. Results There were 17 participants in the first tournament and nine in the second tournament, with no new participants. In both tournaments, the majority of participants scored a Brier Skill Score above the benchmark of 0. The median Brier Skill Score improved slightly between the two tournaments. Participants reported luck and changing political climates as impacting their performance. Participants reported forecasting in their day job but had received no formal training to do so. Conclusions Forecasting is an important public health skill, and human judgement forecasting tournaments can be run amongst public health practitioners with little time and resource requirements. Further research would help identify whether training, teamwork or other interventions can improve public health forecasting accuracy.},
  langid = {english},
  keywords = {COVID-19,Forecasting,Public health policy,Public health training,Tournaments},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/D99USKGL/Davies and Ferris - 2022 - Human judgement forecasting tournaments A feasibi.pdf;/Users/nikos/github-synced/zotero-nikos/storage/LBVYQNU6/S2666535222000362.html}
}

@article{daviesHumanJudgementForecasting2022a,
  title = {Human Judgement Forecasting Tournaments: {{A}} Feasibility Study Based on the {{COVID-19}} Pandemic with Public Health Practitioners in {{England}}},
  shorttitle = {Human Judgement Forecasting Tournaments},
  author = {Davies, Nathan and Ferris, Simon},
  year = {2022},
  month = jun,
  journal = {Public Health in Practice},
  volume = {3},
  pages = {100260},
  issn = {2666-5352},
  doi = {10.1016/j.puhip.2022.100260},
  urldate = {2023-03-01},
  abstract = {Objectives To explore the viability of running human judgement forecasting tournaments with public health practitioners, and to gather initial data on forecasting accuracy and participant perceptions of forecasting. Study design Quality improvement study comprising two COVID-19 forecasting tournaments using Brier Skill Score scoring and a follow-up participant questionnaire. Methods Over two forecasting tournaments, public health registrars in the East Midlands, UK, assigned probabilities to future possible binary events relating to COVID-19. Participants also completed a questionnaire on their experiences of forecasting. Results There were 17 participants in the first tournament and nine in the second tournament, with no new participants. In both tournaments, the majority of participants scored a Brier Skill Score above the benchmark of 0. The median Brier Skill Score improved slightly between the two tournaments. Participants reported luck and changing political climates as impacting their performance. Participants reported forecasting in their day job but had received no formal training to do so. Conclusions Forecasting is an important public health skill, and human judgement forecasting tournaments can be run amongst public health practitioners with little time and resource requirements. Further research would help identify whether training, teamwork or other interventions can improve public health forecasting accuracy.},
  langid = {english},
  keywords = {COVID-19,Forecasting,Public health policy,Public health training,Tournaments},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/LSQ6QR9Q/Davies and Ferris - 2022 - Human judgement forecasting tournaments A feasibi.pdf;/Users/nikos/github-synced/zotero-nikos/storage/673Q9U2A/S2666535222000362.html}
}

@article{dieboldComparingPredictiveAccuracy2015,
  title = {Comparing {{Predictive Accuracy}}, {{Twenty Years Later}}: {{A Personal Perspective}} on the {{Use}} and {{Abuse}} of {{Diebold}}{\textendash}{{Mariano Tests}}},
  shorttitle = {Comparing {{Predictive Accuracy}}, {{Twenty Years Later}}},
  author = {Diebold, Francis X.},
  year = {2015},
  month = jan,
  journal = {Journal of Business \& Economic Statistics},
  volume = {33},
  number = {1},
  pages = {1--1},
  issn = {0735-0015, 1537-2707},
  doi = {10.1080/07350015.2014.983236},
  urldate = {2023-02-28},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/A4SKDFMF/Diebold - 2015 - Comparing Predictive Accuracy, Twenty Years Later.pdf}
}

@article{dreberUsingPredictionMarkets2015,
  title = {Using Prediction Markets to Estimate the Reproducibility of Scientific Research},
  author = {Dreber, Anna and Pfeiffer, Thomas and Almenberg, Johan and Isaksson, Siri and Wilson, Brad and Chen, Yiling and Nosek, Brian A. and Johannesson, Magnus},
  year = {2015},
  month = dec,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {112},
  number = {50},
  pages = {15343--15347},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.1516179112},
  urldate = {2023-02-22},
  abstract = {Concerns about a lack of reproducibility of statistically significant results have recently been raised in many fields, and it has been argued that this lack comes at substantial economic costs. We here report the results from prediction markets set up to quantify the reproducibility of 44 studies published in prominent psychology journals and replicated in the Reproducibility Project: Psychology. The prediction markets predict the outcomes of the replications well and outperform a survey of market participants' individual forecasts. This shows that prediction markets are a promising tool for assessing the reproducibility of published scientific results. The prediction markets also allow us to estimate probabilities for the hypotheses being true at different testing stages, which provides valuable information regarding the temporal dynamics of scientific discovery. We find that the hypotheses being tested in psychology typically have low prior probabilities of being true (median, 9\%) and that a ``statistically significant'' finding needs to be confirmed in a well-powered replication to have a high probability of being true. We argue that prediction markets could be used to obtain speedy information about reproducibility at low cost and could potentially even be used to determine which studies to replicate to optimally allocate limited resources into replications.},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/VFKRPDUU/Dreber et al. - 2015 - Using prediction markets to estimate the reproduci.pdf}
}

@article{epsteinScoringSystemProbability1969,
  title = {A {{Scoring System}} for {{Probability Forecasts}} of {{Ranked Categories}}},
  author = {Epstein, Edward S.},
  year = {1969},
  month = dec,
  journal = {Journal of Applied Meteorology},
  volume = {8},
  number = {6},
  pages = {985--987},
  publisher = {{American Meteorological Society}},
  issn = {0021-8952},
  doi = {10.1175/1520-0450(1969)008<0985:ASSFPF>2.0.CO;2},
  urldate = {2020-08-13},
  langid = {english},
  keywords = {ranked probability score,RPS},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/XAVX39GC/Epstein - 1969 - A Scoring System for Probability Forecasts of Rank.pdf;/Users/nikos/github-synced/zotero-nikos/storage/CVK2YPKP/A-Scoring-System-for-Probability-Forecasts-of.html}
}

@article{farrowHumanJudgmentApproach2017,
  title = {A Human Judgment Approach to Epidemiological Forecasting},
  author = {Farrow, David C. and Brooks, Logan C. and Hyun, Sangwon and Tibshirani, Ryan J. and Burke, Donald S. and Rosenfeld, Roni},
  year = {2017},
  month = mar,
  journal = {PLOS Computational Biology},
  volume = {13},
  number = {3},
  pages = {e1005248},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1005248},
  urldate = {2021-07-29},
  abstract = {Infectious diseases impose considerable burden on society, despite significant advances in technology and medicine over the past century. Advanced warning can be helpful in mitigating and preparing for an impending or ongoing epidemic. Historically, such a capability has lagged for many reasons, including in particular the uncertainty in the current state of the system and in the understanding of the processes that drive epidemic trajectories. Presently we have access to data, models, and computational resources that enable the development of epidemiological forecasting systems. Indeed, several recent challenges hosted by the U.S. government have fostered an open and collaborative environment for the development of these technologies. The primary focus of these challenges has been to develop statistical and computational methods for epidemiological forecasting, but here we consider a serious alternative based on collective human judgment. We created the web-based ``Epicast'' forecasting system which collects and aggregates epidemic predictions made in real-time by human participants, and with these forecasts we ask two questions: how accurate is human judgment, and how do these forecasts compare to their more computational, data-driven alternatives? To address the former, we assess by a variety of metrics how accurately humans are able to predict influenza and chikungunya trajectories. As for the latter, we show that real-time, combined human predictions of the 2014{\textendash}2015 and 2015{\textendash}2016 U.S. flu seasons are often more accurate than the same predictions made by several statistical systems, especially for short-term targets. We conclude that there is valuable predictive power in collective human judgment, and we discuss the benefits and drawbacks of this approach.},
  langid = {english},
  keywords = {Chikungunya infection,Epidemiological methods and statistics,Epidemiological statistics,Epidemiology,Forecasting,Infectious diseases,Influenza,Statistical methods},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/N6MZGQ7M/Farrow et al. - 2017 - A human judgment approach to epidemiological forec.pdf;/Users/nikos/github-synced/zotero-nikos/storage/VSY8MVUC/article.html}
}

@article{funkShorttermForecastsInform2020,
  title = {Short-Term Forecasts to Inform the Response to the {{Covid-19}} Epidemic in the {{UK}}},
  author = {Funk, Sebastian and Abbott, Sam and Atkins, B. D. and Baguelin, M. and Baillie, J. K. and Birrell, P. and Blake, J. and Bosse, Nikos I. and Burton, J. and Carruthers, J. and Davies, N. G. and Angelis, D. De and Dyson, L. and Edmunds, W. J. and Eggo, R. M. and Ferguson, N. M. and Gaythorpe, K. and Gorsich, E. and {Guyver-Fletcher}, G. and Hellewell, J. and Hill, E. M. and Holmes, A. and House, T. A. and Jewell, C. and Jit, M. and Jombart, T. and Joshi, I. and Keeling, M. J. and Kendall, E. and Knock, E. S. and Kucharski, A. J. and Lythgoe, K. A. and Meakin, S. R. and Munday, J. D. and Openshaw, P. J. M. and Overton, C. E. and Pagani, F. and Pearson, J. and {Perez-Guzman}, P. N. and Pellis, L. and Scarabel, F. and Semple, M. G. and Sherratt, K. and Tang, M. and Tildesley, M. J. and {van Leeuwen}, E. and Whittles, L. K. and Group, CMMID COVID-19 Working and Team, Imperial College COVID-19 Response and Investigators, Isaric4c},
  year = {2020},
  month = nov,
  journal = {medRxiv},
  pages = {2020.11.11.20220962},
  publisher = {{Cold Spring Harbor Laboratory Press}},
  doi = {10.1101/2020.11.11.20220962},
  urldate = {2020-11-28},
  abstract = {{$<$}p{$>$}Background: Short-term forecasts of infectious disease can create situational awareness and inform planning for outbreak response. Here, we report on multi-model forecasts of Covid-19 in the UK that were generated at regular intervals starting at the end of March 2020, in order to monitor expected healthcare utilisation and population impacts in real time. Methods: We evaluated the performance of individual model forecasts generated between 24 March and 14 July 2020, using a variety of metrics including the weighted interval score as well as metrics that assess the calibration, sharpness, bias and absolute error of forecasts separately. We further combined the predictions from individual models to ensemble forecasts using a simple mean as well as a quantile regression average that aimed to maximise performance. We further compared model performance to a null model of no change. Results: In most cases, individual models performed better than the null model, and ensembles models were well calibrated and performed comparatively to the best individual models. The quantile regression average did not noticeably outperform the mean ensemble. Conclusions: Ensembles of multi-model forecasts can inform the policy response to the Covid-19 pandemic by assessing future resource needs and expected population impact of morbidity and mortality.{$<$}/p{$>$}},
  copyright = {{\textcopyright} 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/9RK57885/Funk et al. - 2020 - Short-term forecasts to inform the response to the.pdf;/Users/nikos/github-synced/zotero-nikos/storage/AKDY6PAQ/2020.11.11.20220962v1.full.html}
}

@article{gneitingCalibratedProbabilisticForecasting2005,
  title = {Calibrated {{Probabilistic Forecasting Using Ensemble Model Output Statistics}} and {{Minimum CRPS Estimation}}},
  author = {Gneiting, Tilmann and Raftery, Adrian E. and Westveld, Anton H. and Goldman, Tom},
  year = {2005},
  month = may,
  journal = {Monthly Weather Review},
  volume = {133},
  number = {5},
  pages = {1098--1118},
  publisher = {{American Meteorological Society}},
  issn = {0027-0644},
  doi = {10.1175/MWR2904.1},
  urldate = {2020-08-12},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/DWIEDUKB/Gneiting et al. - 2005 - Calibrated Probabilistic Forecasting Using Ensembl.pdf;/Users/nikos/github-synced/zotero-nikos/storage/UKQZH4WN/Calibrated-Probabilistic-Forecasting-Using.html}
}

@article{gneitingProbabilisticForecastsCalibration2007,
  title = {Probabilistic Forecasts, Calibration and Sharpness},
  author = {Gneiting, Tilmann and Balabdaoui, Fadoua and Raftery, Adrian E.},
  year = {2007},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {69},
  number = {2},
  pages = {243--268},
  issn = {1467-9868},
  doi = {10.1111/j.1467-9868.2007.00587.x},
  urldate = {2020-02-17},
  abstract = {Summary. Probabilistic forecasts of continuous variables take the form of predictive densities or predictive cumulative distribution functions. We propose a diagnostic approach to the evaluation of predictive performance that is based on the paradigm of maximizing the sharpness of the predictive distributions subject to calibration. Calibration refers to the statistical consistency between the distributional forecasts and the observations and is a joint property of the predictions and the events that materialize. Sharpness refers to the concentration of the predictive distributions and is a property of the forecasts only. A simple theoretical framework allows us to distinguish between probabilistic calibration, exceedance calibration and marginal calibration. We propose and study tools for checking calibration and sharpness, among them the probability integral transform histogram, marginal calibration plots, the sharpness diagram and proper scoring rules. The diagnostic approach is illustrated by an assessment and ranking of probabilistic forecasts of wind speed at the Stateline wind energy centre in the US Pacific Northwest. In combination with cross-validation or in the time series context, our proposal provides very general, nonparametric alternatives to the use of information criteria for model diagnostics and model selection.},
  langid = {english},
  keywords = {Cross-validation,Density forecast,Ensemble prediction system,Ex post evaluation,Forecast verification,Model diagnostics,Posterior predictive assessment,Predictive distribution,Prequential principle,Probability integral transform,Proper scoring rule},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/BUWD6CGT/Gneiting et al. - 2007 - Probabilistic forecasts, calibration and sharpness.pdf;/Users/nikos/github-synced/zotero-nikos/storage/EUCMSBKN/j.1467-9868.2007.00587.html}
}

@article{gneitingStrictlyProperScoring2007,
  title = {Strictly {{Proper Scoring Rules}}, {{Prediction}}, and {{Estimation}}},
  author = {Gneiting, Tilmann and Raftery, Adrian E},
  year = {2007},
  month = mar,
  journal = {Journal of the American Statistical Association},
  volume = {102},
  number = {477},
  pages = {359--378},
  issn = {0162-1459, 1537-274X},
  doi = {10.1198/016214506000001437},
  urldate = {2020-03-22},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/P599P5ZY/Gneiting and Raftery - 2007 - Strictly Proper Scoring Rules, Prediction, and Est.pdf}
}

@article{gneitingWeatherForecastingEnsemble2005,
  title = {Weather {{Forecasting}} with {{Ensemble Methods}}},
  author = {Gneiting, Tilmann and Raftery, Adrian E.},
  year = {2005},
  month = oct,
  journal = {Science},
  volume = {310},
  number = {5746},
  pages = {248--249},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1115255},
  urldate = {2021-05-30},
  abstract = {{$<$}p{$>$} Traditional weather forecasting has been built on a foundation of deterministic modeling--start with initial conditions, put them into a supercomputer model, and end up with a prediction about future weather. But as Gneiting and Raftery discuss in their Perspective, a new approach--ensemble forecasting--was introduced in the early 1990s. In this method, up to 100 different computer runs, each with slightly different starting conditions or model assumptions, are combined into a weather forecast. In concert with statistical techniques, ensembles can provide accurate statements about the uncertainty in daily and seasonal forecasting. The challenge now is to improve the modeling, statistical analysis, and visualization technologies for disseminating the ensemble results. {$<$}/p{$>$}},
  chapter = {Perspective},
  copyright = {{\textcopyright} 2005 American Association for the Advancement of Science},
  langid = {english},
  pmid = {16224011},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/VRJMN77J/Gneiting and Raftery - 2005 - Weather Forecasting with Ensemble Methods.pdf;/Users/nikos/github-synced/zotero-nikos/storage/8Q5UA2FU/248.html}
}

@article{goodRationalDecisions1952,
  title = {Rational {{Decisions}}},
  author = {Good, I. J.},
  year = {1952},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {14},
  number = {1},
  eprint = {2984087},
  eprinttype = {jstor},
  pages = {107--114},
  publisher = {{[Royal Statistical Society, Wiley]}},
  issn = {0035-9246},
  urldate = {2020-08-13},
  abstract = {This paper deals first with the relationship between the theory of probability and the theory of rational behaviour. A method is then suggested for encouraging people to make accurate probability estimates, a connection with the theory of information being mentioned. Finally Wald's theory of statistical decision functions is summarised and generalised and its relation to the theory of rational behaviour is discussed.},
  keywords = {Log Score,LogS},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/23458422/2020 - Rational Decisions.pdf}
}

@article{gordonAreReplicationRates2020,
  title = {Are Replication Rates the Same across Academic Fields? {{Community}} Forecasts from the {{DARPA SCORE}} Programme},
  shorttitle = {Are Replication Rates the Same across Academic Fields?},
  author = {Gordon, Michael and Viganola, Domenico and Bishop, Michael and Chen, Yiling and Dreber, Anna and Goldfedder, Brandon and Holzmeister, Felix and Johannesson, Magnus and Liu, Yang and Twardy, Charles and Wang, Juntao and Pfeiffer, Thomas},
  year = {2020},
  month = jul,
  journal = {Royal Society Open Science},
  volume = {7},
  number = {7},
  pages = {200566},
  issn = {2054-5703},
  doi = {10.1098/rsos.200566},
  urldate = {2023-02-22},
  abstract = {The Defense Advanced Research Projects Agency (DARPA) programme `Systematizing Confidence in Open Research and Evidence' (SCORE) aims to generate confidence scores for a large number of research claims from empirical studies in the social and behavioural sciences. The confidence scores will provide a quantitative assessment of how likely a claim will hold up in an independent replication. To create the scores, we follow earlier approaches and use prediction markets and surveys to forecast replication outcomes. Based on an initial set of forecasts for the overall replication rate in SCORE and its dependence on the academic discipline and the time of publication, we show that participants expect replication rates to increase over time. Moreover, they expect replication rates to differ between fields, with the highest replication rate in economics (average survey response 58\%), and the lowest in psychology and in education (average survey response of 42\% for both fields). These results reveal insights into the academic community's views of the replication crisis, including for research fields for which no large-scale replication studies have been undertaken yet.},
  pmcid = {PMC7428244},
  pmid = {32874648},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/WATRDBVL/Gordon et al. - 2020 - Are replication rates the same across academic fie.pdf}
}

@article{heldProbabilisticForecastingInfectious2017,
  title = {Probabilistic Forecasting in Infectious Disease Epidemiology: The 13th {{Armitage}} Lecture},
  shorttitle = {Probabilistic Forecasting in Infectious Disease Epidemiology},
  author = {Held, Leonhard and Meyer, Sebastian and Bracher, Johannes},
  year = {2017},
  journal = {Statistics in Medicine},
  volume = {36},
  number = {22},
  pages = {3443--3460},
  issn = {1097-0258},
  doi = {10.1002/sim.7363},
  urldate = {2019-09-16},
  abstract = {Routine surveillance of notifiable infectious diseases gives rise to daily or weekly counts of reported cases stratified by region and age group. From a public health perspective, forecasts of infectious disease spread are of central importance. We argue that such forecasts need to properly incorporate the attached uncertainty, so they should be probabilistic in nature. However, forecasts also need to take into account temporal dependencies inherent to communicable diseases, spatial dynamics through human travel and social contact patterns between age groups. We describe a multivariate time series model for weekly surveillance counts on norovirus gastroenteritis from the 12 city districts of Berlin, in six age groups, from week 2011/27 to week 2015/26. The following year (2015/27 to 2016/26) is used to assess the quality of the predictions. Probabilistic forecasts of the total number of cases can be derived through Monte Carlo simulation, but first and second moments are also available analytically. Final size forecasts as well as multivariate forecasts of the total number of cases by age group, by district and by week are compared across different models of varying complexity. This leads to a more general discussion of issues regarding modelling, prediction and evaluation of public health surveillance data. Copyright {\textcopyright} 2017 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\textcopyright} 2017 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {age-structured contact matrix,endemic{\textendash}epidemic modelling,multivariate probabilistic forecasting,proper scoring rules,spatio-temporal surveillance data},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/DIJH7TNP/Held et al. - 2017 - Probabilistic forecasting in infectious disease ep.pdf}
}

@article{johanssonOpenChallengeAdvance2019,
  title = {An Open Challenge to Advance Probabilistic Forecasting for Dengue Epidemics},
  author = {Johansson, Michael A. and Apfeldorf, Karyn M. and Dobson, Scott and Devita, Jason and Buczak, Anna L. and Baugher, Benjamin and Moniz, Linda J. and Bagley, Thomas and Babin, Steven M. and Guven, Erhan and Yamana, Teresa K. and Shaman, Jeffrey and Moschou, Terry and Lothian, Nick and Lane, Aaron and Osborne, Grant and Jiang, Gao and Brooks, Logan C. and Farrow, David C. and Hyun, Sangwon and Tibshirani, Ryan J. and Rosenfeld, Roni and Lessler, Justin and Reich, Nicholas G. and Cummings, Derek A. T. and Lauer, Stephen A. and Moore, Sean M. and Clapham, Hannah E. and Lowe, Rachel and Bailey, Trevor C. and {Garc{\'i}a-D{\'i}ez}, Markel and Carvalho, Marilia S{\'a} and Rod{\'o}, Xavier and Sardar, Tridip and Paul, Richard and Ray, Evan L. and Sakrejda, Krzysztof and Brown, Alexandria C. and Meng, Xi and Osoba, Osonde and Vardavas, Raffaele and Manheim, David and Moore, Melinda and Rao, Dhananjai M. and Porco, Travis C. and Ackley, Sarah and Liu, Fengchen and Worden, Lee and Convertino, Matteo and Liu, Yang and Reddy, Abraham and Ortiz, Eloy and Rivero, Jorge and Brito, Humberto and Juarrero, Alicia and Johnson, Leah R. and Gramacy, Robert B. and Cohen, Jeremy M. and Mordecai, Erin A. and Murdock, Courtney C. and Rohr, Jason R. and Ryan, Sadie J. and {Stewart-Ibarra}, Anna M. and Weikel, Daniel P. and Jutla, Antarpreet and Khan, Rakibul and Poultney, Marissa and Colwell, Rita R. and {Rivera-Garc{\'i}a}, Brenda and Barker, Christopher M. and Bell, Jesse E. and Biggerstaff, Matthew and Swerdlow, David and {Mier-y-Teran-Romero}, Luis and Forshey, Brett M. and Trtanj, Juli and Asher, Jason and Clay, Matt and Margolis, Harold S. and Hebbeler, Andrew M. and George, Dylan and Chretien, Jean-Paul},
  year = {2019},
  month = nov,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {116},
  number = {48},
  pages = {24268--24274},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1909865116},
  urldate = {2021-05-30},
  abstract = {A wide range of research has promised new tools for forecasting infectious disease dynamics, but little of that research is currently being applied in practice, because tools do not address key public health needs, do not produce probabilistic forecasts, have not been evaluated on external data, or do not provide sufficient forecast skill to be useful. We developed an open collaborative forecasting challenge to assess probabilistic forecasts for seasonal epidemics of dengue, a major global public health problem. Sixteen teams used a variety of methods and data to generate forecasts for 3 epidemiological targets (peak incidence, the week of the peak, and total incidence) over 8 dengue seasons in Iquitos, Peru and San Juan, Puerto Rico. Forecast skill was highly variable across teams and targets. While numerous forecasts showed high skill for midseason situational awareness, early season skill was low, and skill was generally lowest for high incidence seasons, those for which forecasts would be most valuable. A comparison of modeling approaches revealed that average forecast skill was lower for models including biologically meaningful data and mechanisms and that both multimodel and multiteam ensemble forecasts consistently outperformed individual model forecasts. Leveraging these insights, data, and the forecasting framework will be critical to improve forecast skill and the application of forecasts in real time for epidemic preparedness and response. Moreover, key components of this project{\textemdash}integration with public health needs, a common forecasting framework, shared and standardized data, and open participation{\textemdash}can help advance infectious disease forecasting beyond dengue.},
  chapter = {Biological Sciences},
  copyright = {Copyright {\textcopyright} 2019 the Author(s). Published by PNAS.. https://creativecommons.org/licenses/by/4.0/This open access article is distributed under Creative Commons Attribution License 4.0 (CC BY).},
  langid = {english},
  pmid = {31712420},
  keywords = {dengue,epidemic,forecast,Peru,Puerto Rico},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/EZME2294/Johansson et al. - 2019 - An open challenge to advance probabilistic forecas.pdf;/Users/nikos/github-synced/zotero-nikos/storage/4QECBFPX/24268.html}
}

@article{jordanEvaluatingProbabilisticForecasts2019,
  title = {Evaluating {{Probabilistic Forecasts}} with {{{\textbf{scoringRules}}}}},
  author = {Jordan, Alexander and Kr{\"u}ger, Fabian and Lerch, Sebastian},
  year = {2019},
  journal = {Journal of Statistical Software},
  volume = {90},
  number = {12},
  issn = {1548-7660},
  doi = {10.18637/jss.v090.i12},
  urldate = {2020-02-13},
  abstract = {Probabilistic forecasts in the form of probability distributions over future events have become popular in several fields including meteorology, hydrology, economics, and demography. In typical applications, many alternative statistical models and data sources can be used to produce probabilistic forecasts. Hence, evaluating and selecting among competing methods is an important task. The scoringRules package for R provides functionality for comparative evaluation of probabilistic models based on proper scoring rules, covering a wide range of situations in applied work. This paper discusses implementation and usage details, presents case studies from meteorology and economics, and points to the relevant background literature.},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/DSYW6QUF/Jordan et al. - 2019 - Evaluating Probabilistic Forecasts with bscoring.pdf}
}

@article{katsagounosSuperforecastingRealityCheck2021,
  title = {Superforecasting Reality Check: {{Evidence}} from a Small Pool of Experts and Expedited Identification},
  shorttitle = {Superforecasting Reality Check},
  author = {Katsagounos, Ilias and Thomakos, Dimitrios D. and Litsiou, Konstantia and Nikolopoulos, Konstantinos},
  year = {2021},
  month = feb,
  journal = {European Journal of Operational Research},
  volume = {289},
  number = {1},
  pages = {107--117},
  issn = {0377-2217},
  doi = {10.1016/j.ejor.2020.06.042},
  urldate = {2023-03-01},
  abstract = {Superforecasting has drawn the attention of academics - despite earlier contradictory findings in the literature, arguing that humans can consistently and successfully forecast over long periods. It has also enthused practitioners, due to the major implications for improving forecast-driven decision-making. The evidence in support of the superforecasting hypothesis was provided via a 4-year project led by Tetlock and Mellers, which was based on an exhaustive experiment with more than 5000 experts across the globe, resulting in identifying 260 superforecasters. The result, however, jeopardizes the applicability of the proposition, as exciting as it may be for the academic world; if every company in the world needs to rely on the aforementioned 260 experts, then this will end up an impractical and expensive endeavor. Thus, it would make sense to test the superforecasting hypothesis in real-life conditions: when only a small pool of experts is available, and there is limited time to identify the superforecasters. If under these constrained conditions the hypothesis still holds, then many small and medium-sized organizations could identify fast and consequently utilize their own superforecasters. In this study, we provide supportive empirical evidence from an experiment with an initial (small) pool of 314 experts and an identification phase of (just) 9 months. Furthermore - and corroborating to the superforecasting literature, we also find preliminary evidence that even an additional training of just 20~min, can influence positively the number of superforecasters identified.},
  langid = {english},
  keywords = {Experts,Forecasting,Real-life conditions,Superforecasting,Training},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/DAF3YZRJ/Katsagounos et al. - 2021 - Superforecasting reality check Evidence from a sm.pdf;/Users/nikos/github-synced/zotero-nikos/storage/JFHF6ESX/S0377221720306007.html}
}

@article{macheteContrastingProbabilisticScoring2012,
  title = {Contrasting {{Probabilistic Scoring Rules}}},
  author = {Machete, Reason Lesego},
  year = {2012},
  month = jul,
  journal = {arXiv:1112.4530 [math, stat]},
  eprint = {1112.4530},
  primaryclass = {math, stat},
  urldate = {2020-03-21},
  abstract = {There are several scoring rules that one can choose from in order to score probabilistic forecasting models or estimate model parameters. Whilst it is generally agreed that proper scoring rules are preferable, there is no clear criterion for preferring one proper scoring rule above another. This manuscript compares and contrasts some commonly used proper scoring rules and provides guidance on scoring rule selection. In particular, it is shown that the logarithmic scoring rule prefers erring with more uncertainty, the spherical scoring rule prefers erring with lower uncertainty, whereas the other scoring rules are indifferent to either option.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {{62B10, 62C05, 62G05, 62G07, 62F99, 62P05, 62P12, 62P20},Mathematics - Statistics Theory},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/8FYPC3Y4/Machete - 2012 - Contrasting Probabilistic Scoring Rules.pdf}
}

@article{mathesonScoringRulesContinuous1976,
  title = {Scoring {{Rules}} for {{Continuous Probability Distributions}}},
  author = {Matheson, James E. and Winkler, Robert L.},
  year = {1976},
  month = jun,
  journal = {Management Science},
  volume = {22},
  number = {10},
  pages = {1087--1096},
  publisher = {{INFORMS}},
  issn = {0025-1909},
  doi = {10.1287/mnsc.22.10.1087},
  urldate = {2020-08-13},
  abstract = {Personal, or subjective, probabilities are used as inputs to many inferential and decision-making models, and various procedures have been developed for the elicitation of such probabilities. Included among these elicitation procedures are scoring rules, which involve the computation of a score based on the assessor's stated probabilities and on the event that actually occurs. The development of scoring rules has, in general, been restricted to the elicitation of discrete probability distributions. In this paper, families of scoring rules for the elicitation of continuous probability distributions are developed and discussed.},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/SVJ7YPP7/Matheson and Winkler - 1976 - Scoring Rules for Continuous Probability Distribut.pdf;/Users/nikos/github-synced/zotero-nikos/storage/H5CNZS4U/mnsc.22.10.html}
}

@article{mcandrewAggregatingHumanJudgment2022,
  title = {Aggregating Human Judgment Probabilistic Predictions of the Safety, Efficacy, and Timing of a {{COVID-19}} Vaccine},
  author = {McAndrew, Thomas and Cambeiro, Juan and Besiroglu, Tamay},
  year = {2022},
  month = apr,
  journal = {Vaccine},
  volume = {40},
  number = {15},
  pages = {2331--2341},
  issn = {0264-410X},
  doi = {10.1016/j.vaccine.2022.02.054},
  urldate = {2023-02-01},
  abstract = {Safe, efficacious vaccines were developed to reduce the transmission of SARS-CoV-2 during the COVID-19 pandemic. But in the middle of 2020, vaccine effectiveness, safety, and the timeline for when a vaccine would be approved and distributed to the public was uncertain. To support public health decision making, we solicited trained forecasters and experts in vaccinology and infectious disease to provide monthly probabilistic predictions from July to September of 2020 of the efficacy, safety, timing, and delivery of a COVID-19 vaccine. We found, that despite sparse historical data, a linear pool{\textemdash}a combination of human judgment probabilistic predictions{\textemdash}can quantify the uncertainty in clinical significance and timing of a potential vaccine. The linear pool underestimated how fast a therapy would show a survival benefit and the high efficacy of approved COVID-19 vaccines. However, the linear pool did make an accurate prediction for when a vaccine would be approved by the FDA. Compared to individual forecasters, the linear pool was consistently above the median of the most accurate forecasts. A linear pool is a fast and versatile method to build probabilistic predictions of a developing vaccine that is robust to poor individual predictions. Though experts and trained forecasters did underestimate the speed of development and the high efficacy of a SARS-CoV-2 vaccine, linear pool predictions can improve situational awareness for public health officials and for the public make clearer the risks, rewards, and timing of a vaccine.},
  langid = {english},
  keywords = {COVID-19,Forecasting,Human judgement,Vaccine},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/7QDAJ9C4/McAndrew et al. - 2022 - Aggregating human judgment probabilistic predictio.pdf;/Users/nikos/github-synced/zotero-nikos/storage/ZK26BETC/S0264410X22002006.html}
}

@article{mcandrewChimericForecastingCombining2022,
  title = {Chimeric Forecasting: Combining Probabilistic Predictions from Computational Models and Human Judgment},
  shorttitle = {Chimeric Forecasting},
  author = {McAndrew, Thomas and Codi, Allison and Cambeiro, Juan and Besiroglu, Tamay and Braun, David and Chen, Eva and De C{\`e}saris, Luis Enrique Urtubey and Luk, Damon},
  year = {2022},
  month = nov,
  journal = {BMC infectious diseases},
  volume = {22},
  number = {1},
  pages = {833},
  issn = {1471-2334},
  doi = {10.1186/s12879-022-07794-5},
  urldate = {2023-02-01},
  abstract = {Forecasts of the trajectory of an infectious agent can help guide public health decision making. A traditional approach to forecasting fits a computational model to structured data and generates a predictive distribution. However, human judgment has access to the same data as computational models plus experience, intuition, and subjective data. We propose a chimeric ensemble-a combination of computational and human judgment forecasts-as a novel approach to predicting the trajectory of an infectious agent. Each month from January, 2021 to June, 2021 we asked two generalist crowds, using the same criteria as the COVID-19 Forecast Hub, to submit a predictive distribution over incident cases and deaths at the US national level either two or three weeks into the future and combined these human judgment forecasts with forecasts from computational models submitted to the COVID-19 Forecasthub into a chimeric ensemble. We find a chimeric ensemble compared to an ensemble including only computational models improves predictions of incident cases and shows similar performance for predictions of incident deaths. A chimeric ensemble is a flexible, supportive public health tool and shows promising results for predictions of the spread of an infectious agent.},
  copyright = {cc by},
  langid = {english},
  pmcid = {PMC9648897},
  pmid = {36357829},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/MECUBIMY/12879_2022_7794_MOESM1_ESM.pdf;/Users/nikos/github-synced/zotero-nikos/storage/ULVZRCFS/McAndrew et al. - 2022 - Chimeric forecasting combining probabilistic pred.pdf}
}

@article{mcandrewEarlyHumanJudgment2022,
  title = {Early Human Judgment Forecasts of Human Monkeypox, {{May}} 2022},
  author = {McAndrew, Thomas and Majumder, Maimuna S. and Lover, Andrew A. and Venkatramanan, Srini and Bocchini, Paolo and Besiroglu, Tamay and Codi, Allison and Braun, David and Dempsey, Gaia and Abbott, Sam and Chevalier, Sylvain and Bosse, Nikos I. and Cambeiro, Juan},
  year = {2022},
  month = aug,
  journal = {The Lancet Digital Health},
  volume = {4},
  number = {8},
  pages = {e569-e571},
  publisher = {{Elsevier}},
  issn = {2589-7500},
  doi = {10.1016/S2589-7500(22)00127-3},
  urldate = {2023-02-01},
  langid = {english},
  pmid = {35811294},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/I8DWB4VK/McAndrew et al. - 2022 - Early human judgment forecasts of human monkeypox,.pdf}
}

@article{mcandrewExpertJudgmentModel2022,
  title = {An Expert Judgment Model to Predict Early Stages of the {{COVID-19}} Pandemic in the {{United States}}},
  author = {McAndrew, Thomas and Reich, Nicholas G.},
  year = {2022},
  month = sep,
  journal = {PLOS Computational Biology},
  volume = {18},
  number = {9},
  pages = {e1010485},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1010485},
  urldate = {2023-02-01},
  abstract = {From February to May 2020, experts in the modeling of infectious disease provided quantitative predictions and estimates of trends in the emerging COVID-19 pandemic in a series of 13 surveys. Data on existing transmission patterns were sparse when the pandemic began, but experts synthesized information available to them to provide quantitative, judgment-based assessments of the current and future state of the pandemic. We aggregated expert predictions into a single ``linear pool'' by taking an equally weighted average of their probabilistic statements. At a time when few computational models made public estimates or predictions about the pandemic, expert judgment provided (a) falsifiable predictions of short- and long-term pandemic outcomes related to reported COVID-19 cases, hospitalizations, and deaths, (b) estimates of latent viral transmission, and (c) counterfactual assessments of pandemic trajectories under different scenarios. The linear pool approach of aggregating expert predictions provided more consistently accurate predictions than any individual expert, although the predictive accuracy of a linear pool rarely provided the most accurate prediction. This work highlights the importance that an expert linear pool could play in flexibly assessing a wide array of risks early in future emerging outbreaks, especially in settings where available data cannot yet support data-driven computational modeling.},
  langid = {english},
  keywords = {Body weight,COVID 19,Forecasting,Pandemics,Probability distribution,Public and occupational health,SARS CoV 2,Surveys},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/MX23NF3S/McAndrew and Reich - 2022 - An expert judgment model to predict early stages o.pdf}
}

@article{mcandrewExpertJudgmentModel2022a,
  title = {An Expert Judgment Model to Predict Early Stages of the {{COVID-19}} Pandemic in the {{United States}}},
  author = {McAndrew, Thomas and Reich, Nicholas G.},
  year = {2022},
  month = sep,
  journal = {PLOS Computational Biology},
  volume = {18},
  number = {9},
  pages = {e1010485},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1010485},
  urldate = {2024-02-10},
  abstract = {From February to May 2020, experts in the modeling of infectious disease provided quantitative predictions and estimates of trends in the emerging COVID-19 pandemic in a series of 13 surveys. Data on existing transmission patterns were sparse when the pandemic began, but experts synthesized information available to them to provide quantitative, judgment-based assessments of the current and future state of the pandemic. We aggregated expert predictions into a single ``linear pool'' by taking an equally weighted average of their probabilistic statements. At a time when few computational models made public estimates or predictions about the pandemic, expert judgment provided (a) falsifiable predictions of short- and long-term pandemic outcomes related to reported COVID-19 cases, hospitalizations, and deaths, (b) estimates of latent viral transmission, and (c) counterfactual assessments of pandemic trajectories under different scenarios. The linear pool approach of aggregating expert predictions provided more consistently accurate predictions than any individual expert, although the predictive accuracy of a linear pool rarely provided the most accurate prediction. This work highlights the importance that an expert linear pool could play in flexibly assessing a wide array of risks early in future emerging outbreaks, especially in settings where available data cannot yet support data-driven computational modeling.},
  langid = {english},
  keywords = {Body weight,COVID 19,Forecasting,Pandemics,Probability distribution,Public and occupational health,SARS CoV 2,Surveys},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/4MU38IQN/McAndrew and Reich - 2022 - An expert judgment model to predict early stages o.pdf}
}

@article{mcgowanCollaborativeEffortsForecast2019,
  title = {Collaborative Efforts to Forecast Seasonal Influenza in the {{United States}}, 2015{\textendash}2016},
  author = {McGowan, Craig J. and Biggerstaff, Matthew and Johansson, Michael and Apfeldorf, Karyn M. and {Ben-Nun}, Michal and Brooks, Logan and Convertino, Matteo and Erraguntla, Madhav and Farrow, David C. and Freeze, John and Ghosh, Saurav and Hyun, Sangwon and Kandula, Sasikiran and Lega, Joceline and Liu, Yang and Michaud, Nicholas and Morita, Haruka and Niemi, Jarad and Ramakrishnan, Naren and Ray, Evan L. and Reich, Nicholas G. and Riley, Pete and Shaman, Jeffrey and Tibshirani, Ryan and Vespignani, Alessandro and Zhang, Qian and Reed, Carrie},
  year = {2019},
  month = jan,
  journal = {Scientific Reports},
  volume = {9},
  number = {1},
  pages = {683},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-018-36361-9},
  urldate = {2021-05-30},
  abstract = {Since 2013, the Centers for Disease Control and Prevention (CDC) has hosted an annual influenza season forecasting challenge. The 2015{\textendash}2016 challenge consisted of weekly probabilistic forecasts of multiple targets, including fourteen models submitted by eleven teams. Forecast skill was evaluated using a modified logarithmic score. We averaged submitted forecasts into a mean ensemble model and compared them against predictions based on historical trends. Forecast skill was highest for seasonal peak intensity and short-term forecasts, while forecast skill for timing of season onset and peak week was generally low. Higher forecast skill was associated with team participation in previous influenza forecasting challenges and utilization of ensemble forecasting techniques. The mean ensemble consistently performed well and outperformed historical trend predictions. CDC and contributing teams will continue to advance influenza forecasting and work to improve the accuracy and reliability of forecasts to facilitate increased incorporation into public health response efforts.},
  copyright = {2019 The Author(s)},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/GHDYHNAE/McGowan et al. - 2019 - Collaborative efforts to forecast seasonal influen.pdf;/Users/nikos/github-synced/zotero-nikos/storage/9ES4FDUU/s41598-018-36361-9.html}
}

@article{murphyNoteRankedProbability1971,
  title = {A {{Note}} on the {{Ranked Probability Score}}},
  author = {Murphy, Allan H.},
  year = {1971},
  month = feb,
  journal = {Journal of Applied Meteorology},
  volume = {10},
  number = {1},
  pages = {155--156},
  publisher = {{American Meteorological Society}},
  issn = {0021-8952},
  doi = {10.1175/1520-0450(1971)010<0155:ANOTRP>2.0.CO;2},
  urldate = {2020-08-13},
  langid = {english},
  keywords = {ranked probability score,RPS},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/K7IF4UNP/Murphy - 1971 - A Note on the Ranked Probability Score.pdf;/Users/nikos/github-synced/zotero-nikos/storage/PRWJQBMK/A-Note-on-the-Ranked-Probability-Score.html}
}

@article{pagePrescribingAustraliansLiving2015,
  title = {Prescribing for {{Australians}} Living with Dementia: Study Protocol Using the {{Delphi}} Technique},
  shorttitle = {Prescribing for {{Australians}} Living with Dementia},
  author = {Page, Amy and Potter, Kathleen and Clifford, Rhonda and McLachlan, Andrew and {Etherton-Beer}, Christopher},
  year = {2015},
  month = aug,
  journal = {BMJ Open},
  volume = {5},
  number = {8},
  pages = {e008048},
  issn = {2044-6055, 2044-6055},
  doi = {10.1136/bmjopen-2015-008048},
  urldate = {2023-02-07},
  abstract = {Introduction: Prescribing is complicated for people living with dementia, and careful consideration should be given to continuing and initiating all medicines. This study aims to elicit opinion and gain consensus on appropriate medicine use for people living with dementia in Australia to create a consensus-based list of explicit prescribing criteria. Methods and analysis: A Delphi technique will be used to develop explicit criteria of medication use in adults aged 65 years and above. An interdisciplinary panel of Australian experts in geriatric therapeutics will be convened that will consist of a minimum of 10 participants. To develop the consensus-based criteria, this study will use an iterative, anonymous, multistaged approach with controlled feedback. Round 1 questionnaire will be administered, and subsequently qualitatively analysed. The round 1 results will be fed back to the panel members, and a round 2 questionnaire developed using questions on a five-point Likert scale. This process will repeat until consensus is developed, or diminishing returns are noted. Ethics and dissemination: All participants will be provided with a participant information sheet, and sign a written consent form. Ethical approval has been granted from the University of Western Australia's Human Research Ethics Committee (HREC) (reference: RA/4/1/7172). We expect that data from this study will result in a paper published in a peerreviewed clinical journal and will also present the results at conferences.},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/G524RNFH/Page et al. - 2015 - Prescribing for Australians living with dementia .pdf}
}

@article{recchiaHowWellDid2021,
  title = {How Well Did Experts and Laypeople Forecast the Size of the {{COVID-19}} Pandemic?},
  author = {Recchia, Gabriel and Freeman, Alexandra L. J. and Spiegelhalter, David},
  year = {2021},
  month = may,
  journal = {PLOS ONE},
  volume = {16},
  number = {5},
  pages = {e0250935},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0250935},
  urldate = {2021-06-02},
  abstract = {Throughout the COVID-19 pandemic, social and traditional media have disseminated predictions from experts and nonexperts about its expected magnitude. How accurate were the predictions of `experts'{\textemdash}individuals holding occupations or roles in subject-relevant fields, such as epidemiologists and statisticians{\textemdash}compared with those of the public? We conducted a survey in April 2020 of 140 UK experts and 2,086 UK laypersons; all were asked to make four quantitative predictions about the impact of COVID-19 by 31 Dec 2020. In addition to soliciting point estimates, we asked participants for lower and higher bounds of a range that they felt had a 75\% chance of containing the true answer. Experts exhibited greater accuracy and calibration than laypersons, even when restricting the comparison to a subset of laypersons who scored in the top quartile on a numeracy test. Even so, experts substantially underestimated the ultimate extent of the pandemic, and the mean number of predictions for which the expert intervals contained the actual outcome was only 1.8 (out of 4), suggesting that experts should consider broadening the range of scenarios they consider plausible. Predictions of the public were even more inaccurate and poorly calibrated, suggesting that an important role remains for expert predictions as long as experts acknowledge their uncertainty.},
  langid = {english},
  keywords = {COVID 19,Epidemiological statistics,Forecasting,Numeracy,Pandemics,Probability distribution,Statistical distributions,Virus testing},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/X2FMBK56/Recchia et al. - 2021 - How well did experts and laypeople forecast the si.pdf;/Users/nikos/github-synced/zotero-nikos/storage/TT7K727A/article.html}
}

@article{reichCollaborativeMultiyearMultimodel2019,
  title = {A Collaborative Multiyear, Multimodel Assessment of Seasonal Influenza Forecasting in the {{United States}}},
  author = {Reich, Nicholas G. and Brooks, Logan C. and Fox, Spencer J. and Kandula, Sasikiran and McGowan, Craig J. and Moore, Evan and Osthus, Dave and Ray, Evan L. and Tushar, Abhinav and Yamana, Teresa K. and Biggerstaff, Matthew and Johansson, Michael A. and Rosenfeld, Roni and Shaman, Jeffrey},
  year = {2019},
  month = feb,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {116},
  number = {8},
  pages = {3146--3154},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1812594116},
  urldate = {2021-10-13},
  abstract = {Influenza infects an estimated 9{\textendash}35 million individuals each year in the United States and is a contributing cause for between 12,000 and 56,000 deaths annually. Seasonal outbreaks of influenza are common in temperate regions of the world, with highest incidence typically occurring in colder and drier months of the year. Real-time forecasts of influenza transmission can inform public health response to outbreaks. We present the results of a multiinstitution collaborative effort to standardize the collection and evaluation of forecasting models for influenza in the United States for the 2010/2011 through 2016/2017 influenza seasons. For these seven seasons, we assembled weekly real-time forecasts of seven targets of public health interest from 22 different models. We compared forecast accuracy of each model relative to a historical baseline seasonal average. Across all regions of the United States, over half of the models showed consistently better performance than the historical baseline when forecasting incidence of influenza-like illness 1 wk, 2 wk, and 3 wk ahead of available data and when forecasting the timing and magnitude of the seasonal peak. In some regions, delays in data reporting were strongly and negatively associated with forecast accuracy. More timely reporting and an improved overall accessibility to novel and traditional data sources are needed to improve forecasting accuracy and its integration with real-time public health decision making.},
  chapter = {PNAS Plus},
  copyright = {Copyright {\textcopyright} 2019 the Author(s). Published by PNAS.. https://creativecommons.org/licenses/by-nc-nd/4.0/This open access article is distributed under Creative Commons Attribution-NonCommercial-NoDerivatives License 4.0 (CC BY-NC-ND).},
  langid = {english},
  pmid = {30647115},
  keywords = {forecasting,infectious disease,influenza,public health,statistics},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/XEKLR37W/Reich et al. - 2019 - A collaborative multiyear, multimodel assessment o.pdf;/Users/nikos/github-synced/zotero-nikos/storage/QID3PLW4/3146.html}
}

@article{sherrattPredictivePerformanceMultimodel2022,
  title = {Predictive Performance of Multi-Model Ensemble Forecasts of {{COVID-19}} across  {{European}} Nation},
  author = {Sherratt, K. and Gruson, H. and Grah, R. and Johnson, H. and Niehus, R. and Prasse, B. and Sandman, F. and Deuschel, J. and Wolffram, D. and Abbott, S. and Ullrich, A. and Gibson, G. and Ray, {\relax EL}. and Reich, {\relax NG}. and Sheldon, D. and Wang, Y. and Wattanachit, N. and Wang, L. and Trnka, J. and Obozinski, G. and Sun, T. and Thanou, D. and Pottier, L. and Krymova, E. and Barbarossa, {\relax MV}. and Leith{\"a}user, N. and Mohring, J. and Schneider, J. and Wlazlo, J. and Fuhrmann, J. and Lange, B. and Rodiah, I. and Baccam, P. and Gurung, H. and Stage, S. and Suchoski, B. and Budzinski, J. and Walraven, R. and Villanueva, I. and Tucek, V. and {\v S}m{\'i}d, M. and Zaj{\'i}cek, M. and P{\'e}rez, {\'A}lvarez C. and Reina, B. and Bosse, {\relax NI}. and Meakin, S. and Di Loro, Alaimo and Maruotti, A. and Eclerov{\'a}, V. and Kraus, A. and Kraus, D. and Pribylova, L. and Dimitris, B. and Li, {\relax ML}. and Saksham, S. and Dehning, J. and Mohr, S. and Priesemann, V. and Redlarski, G. and Bejar, B. and Ardenghi, G. and Parolini, N. and Ziarelli, G. and Bock, W. and Heyder, S. and Hotz, T. and E., Singh D. and {Guzman-Merino}, M. and Aznarte, {\relax JL}. and Mori{\~n}a, D. and Alonso, S. and {\'A}lvarez, E. and L{\'o}pez, D. and Prats, C. and Burgard, {\relax JP}. and Rodloff, A. and Zimmermann, T. and Kuhlmann, A. and Zibert, J. and Pennoni, F. and Divino, F. and Catal{\`a}, M. and Lovison, G. and Giudici, P. and Tarantino, B. and Bartolucci, F. and Jona, Lasinio G. and Mingione, M. and Farcomeni, A. and Srivastava, A. and {Montero-Manso}, P. and Adiga, A. and Hurt, B. and Lewis, B. and Marathe, M. and Porebski, P. and Venkatramanan, S. and Bartczuk, R. and Dreger, F. and Gambin, A. and Gogolewski, K. and {Gruziel-Slomka}, M. and Krupa, B. and Moszynski, A. and Niedzielewski, K. and Nowosielski, J. and Radwan, M. and Rakowski, F. and Semeniuk, M. and Szczurek, E. and Zielinski, J. and Kisielewski, J. and Pabjan, B. and Holger, K. and Kheifetz, Y. and Scholz, M. and Bodych, M. and Filinski, M. and Idzikowski, R. and Krueger, T. and Ozanski, T. and Bracher, J. and Funk, S.},
  year = {2022},
  journal = {Europe PMC},
  doi = {10.1101/2022.06.16.22276024},
  urldate = {2023-02-07},
  abstract = {Background  Short-term forecasts of infectious disease burden can contribute to  situational awareness and aid capacity planning. Based on best practice in  other fields and recent insights in infectious disease epidemiology, one can  maximise the predictive performance of such forecasts if multiple models are  combined into an ensemble. Here we report on the performance of ensembles in  predicting COVID-19 cases and deaths across Europe between 08 March 2021 and  07 March 2022. Methods  We used open-source tools to develop a public European COVID-19 Forecast  Hub. We invited groups globally to contribute weekly forecasts for COVID-19  cases and deaths reported from a standardised source over the next one to  four weeks. Teams submitted forecasts from March 2021 using standardised  quantiles of the predictive distribution. Each week we created an ensemble  forecast, where each predictive quantile was calculated as the  equally-weighted average (initially the mean and then from 26th July the  median) of all individual models' predictive quantiles. We measured the  performance of each model using the relative Weighted Interval Score (WIS),  comparing models' forecast accuracy relative to all other models. We  retrospectively explored alternative methods for ensemble forecasts,  including weighted averages based on models' past predictive  performance. Results  Over 52 weeks we collected and combined up to 28 forecast models for 32  countries. We found a weekly ensemble had a consistently strong performance  across countries over time. Across all horizons and locations, the ensemble  performed better on relative WIS than 84\% of participating models' forecasts  of incident cases (with a total N=862), and 92\% of participating models'  forecasts of deaths (N=746). Across a one to four week time horizon,  ensemble performance declined with longer forecast periods when forecasting  cases, but remained stable over four weeks for incident death forecasts. In  every forecast across 32 countries, the ensemble outperformed most  contributing models when forecasting either cases or deaths, frequently  outperforming all of its individual component models. Among several choices  of ensemble methods we found that the most influential and best choice was  to use a median average of models instead of using the mean, regardless of  methods of weighting component forecast models. Conclusions  Our results support the use of combining forecasts from individual  models into an ensemble in order to improve predictive performance across  epidemiological targets and populations during infectious disease epidemics.  Our findings further suggest that median ensemble methods yield better  predictive performance more than ones based on means. Our findings also  highlight that forecast consumers should place more weight on incident death  forecasts than incident case forecasts at forecast horizons greater than two  weeks. Code and data availability  All data and code are publicly available on Github:  covid19-forecast-hub-europe/euro-hub-ensemble.},
  copyright = {cc by},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/HK3RSINT/Sherratt et al. - 2022 - Predictive performance of multi-model ensemble for.pdf}
}

@article{tetlockForecastingTournamentsTools2014,
  title = {Forecasting {{Tournaments}}: {{Tools}} for {{Increasing Transparency}} and {{Improving}} the {{Quality}} of {{Debate}}},
  shorttitle = {Forecasting {{Tournaments}}},
  author = {Tetlock, Philip E. and Mellers, Barbara A. and Rohrbaugh, Nick and Chen, Eva},
  year = {2014},
  month = aug,
  journal = {Current Directions in Psychological Science},
  volume = {23},
  number = {4},
  pages = {290--295},
  publisher = {{SAGE Publications Inc}},
  issn = {0963-7214},
  doi = {10.1177/0963721414534257},
  urldate = {2021-05-30},
  abstract = {Forecasting tournaments are level-playing-field competitions that reveal which individuals, teams, or algorithms generate more accurate probability estimates on which topics. This article describes a massive geopolitical tournament that tested clashing views on the feasibility of improving judgmental accuracy and on the best methods of doing so. The tournament's winner, the Good Judgment Project, outperformed the simple average of the crowd by (a) designing new forms of cognitive-debiasing training, (b) incentivizing rigorous thinking in teams and prediction markets, (c) skimming top talent into elite collaborative teams of ``super forecasters,'' and (d) fine-tuning aggregation algorithms for distilling greater wisdom from crowds. Tournaments have the potential to open closed minds and increase assertion-to-evidence ratios in polarized scientific and policy debates.},
  langid = {english},
  keywords = {accuracy,forecasting,probability,tournaments}
}

@misc{venkatramananUtilityHumanJudgment2022a,
  title = {Utility of Human Judgment Ensembles during Times of Pandemic Uncertainty: {{A}} Case Study during the {{COVID-19 Omicron BA}}.1 Wave in the {{USA}}},
  shorttitle = {Utility of Human Judgment Ensembles during Times of Pandemic Uncertainty},
  author = {Venkatramanan, Srinivasan and Cambeiro, Juan and Liptay, Tom and Lewis, Bryan and Orr, Mark and Dempsey, Gaia and Telionis, Alex and Crow, Justin and Barrett, Chris and Marathe, Madhav},
  year = {2022},
  month = oct,
  pages = {2022.10.12.22280997},
  publisher = {{medRxiv}},
  doi = {10.1101/2022.10.12.22280997},
  urldate = {2023-04-15},
  abstract = {Responding to a rapidly evolving pandemic like COVID-19 is challenging, and involves anticipating novel variants, vaccine uptake, and behavioral adaptations. Human judgment systems can complement computational models by providing valuable real-time forecasts. We report findings from a study conducted on Metaculus, a community forecasting platform, in partnership with the Virginia Department of Health, involving six rounds of forecasting during the Omicron BA.1 wave in the United States from November 2021 to March 2022. We received 8355 probabilistic predictions from 129 unique users across 60 questions pertaining to cases, hospitalizations, vaccine uptake, and peak/trough activity. We observed that the case forecasts performed on par with national multi-model ensembles and the vaccine uptake forecasts were more robust and accurate compared to baseline models. We also identified qualitative shifts in Omicron BA.1 wave prognosis during the surge phase, demonstrating rapid adaptation of such systems. Finally, we found that community estimates of variant characteristics such as growth rate and timing of dominance were in line with the scientific consensus. The observed accuracy, timeliness, and scope of such systems demonstrates the value of incorporating them into pandemic policymaking workflows.},
  archiveprefix = {medRxiv},
  copyright = {{\textcopyright} 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/AGMIG33K/Venkatramanan et al. - 2022 - Utility of human judgment ensembles during times o.pdf}
}

@article{wallingaHowGenerationIntervals2006,
  title = {How Generation Intervals Shape the Relationship between Growth Rates and Reproductive Numbers},
  author = {Wallinga, J and Lipsitch, M},
  year = {2006},
  month = nov,
  journal = {Proceedings of the Royal Society B: Biological Sciences},
  volume = {274},
  number = {1609},
  pages = {599--604},
  publisher = {{Royal Society}},
  doi = {10.1098/rspb.2006.3754},
  urldate = {2023-02-22},
  abstract = {Mathematical models of transmission have become invaluable management tools in planning for the control of emerging infectious diseases. A key variable in such models is the reproductive number R. For new emerging infectious diseases, the value of the reproductive number can only be inferred indirectly from the observed exponential epidemic growth rate r. Such inference is ambiguous as several different equations exist that relate the reproductive number to the growth rate, and it is unclear which of these equations might apply to a new infection. Here, we show that these different equations differ only with respect to their assumed shape of the generation interval distribution. Therefore, the shape of the generation interval distribution determines which equation is appropriate for inferring the reproductive number from the observed growth rate. We show that by assuming all generation intervals to be equal to the mean, we obtain an upper bound to the range of possible values that the reproductive number may attain for a given growth rate. Furthermore, we show that by taking the generation interval distribution equal to the observed distribution, it is possible to obtain an empirical estimate of the reproductive number.},
  keywords = {basic reproduction ratio,epidemiology,influenza,Lotka{\textendash}Euler equation,serial interval},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/ZY9TWRMB/Wallinga and Lipsitch - 2006 - How generation intervals shape the relationship be.pdf}
}

@article{winklerScoringRulesEvaluation1996,
  title = {Scoring Rules and the Evaluation of Probabilities},
  author = {Winkler, R. L. and Mu{\~n}oz, Javier and Cervera, Jos{\'e} L. and Bernardo, Jos{\'e} M. and Blattenberger, Gail and Kadane, Joseph B. and Lindley, Dennis V. and Murphy, Allan H. and Oliver, Robert M. and {R{\'i}os-Insua}, David},
  year = {1996},
  month = jun,
  journal = {Test},
  volume = {5},
  number = {1},
  pages = {1--60},
  issn = {1863-8260},
  doi = {10.1007/BF02562681},
  urldate = {2021-03-04},
  abstract = {In Bayesian inference and decision analysis, inferences and predictions are inherently probabilistic in nature. Scoring rules, which involve the computation of a score based on probability forecasts and what actually occurs, can be used to evaluate probabilities and to provide appropriate incentives for ``good'' probabilities. This paper review scoring rules and some related measures for evaluating probabilities, including decompositions of scoring rules and attributes of ``goodness'' of probabilites, comparability of scores, and the design of scoring rules for specific inferential and decision-making problems},
  langid = {english},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/VHTQR6BK/Winkler et al. - 1996 - Scoring rules and the evaluation of probabilities.pdf}
}

@article{yamanaSuperensembleForecastsDengue2016,
  title = {Superensemble Forecasts of Dengue Outbreaks},
  author = {Yamana, Teresa K. and Kandula, Sasikiran and Shaman, Jeffrey},
  year = {2016},
  month = oct,
  journal = {Journal of The Royal Society Interface},
  volume = {13},
  number = {123},
  pages = {20160410},
  publisher = {{Royal Society}},
  doi = {10.1098/rsif.2016.0410},
  urldate = {2021-05-30},
  abstract = {In recent years, a number of systems capable of predicting future infectious disease incidence have been developed. As more of these systems are operationalized, it is important that the forecasts generated by these different approaches be formally reconciled so that individual forecast error and bias are reduced. Here we present a first example of such multi-system, or superensemble, forecast. We develop three distinct systems for predicting dengue, which are applied retrospectively to forecast outbreak characteristics in San Juan, Puerto Rico. We then use Bayesian averaging methods to combine the predictions from these systems and create superensemble forecasts. We demonstrate that on average, the superensemble approach produces more accurate forecasts than those made from any of the individual forecasting systems.},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/E5APZHIN/Yamana et al. - 2016 - Superensemble forecasts of dengue outbreaks.pdf;/Users/nikos/github-synced/zotero-nikos/storage/2RH9MFEI/rsif.2016.html}
}

@article{zellnerSurveyHumanJudgement2021,
  title = {A Survey of Human Judgement and Quantitative Forecasting Methods},
  author = {Zellner, Maximilian and Abbas, Ali E. and Budescu, David V. and Galstyan, Aram},
  year = {2021},
  month = feb,
  journal = {Royal Society Open Science},
  volume = {8},
  number = {2},
  pages = {201187},
  publisher = {{Royal Society}},
  doi = {10.1098/rsos.201187},
  urldate = {2023-02-01},
  abstract = {This paper's top-level goal is to provide an overview of research conducted in the many academic domains concerned with forecasting. By providing a summary encompassing these domains, this survey connects them, establishing a common ground for future discussions. To this end, we survey literature on human judgement and quantitative forecasting as well as hybrid methods that involve both humans and algorithmic approaches. The survey starts with key search terms that identified more than 280 publications in the fields of computer science, operations research, risk analysis, decision science, psychology and forecasting. Results show an almost 10-fold increase in the application-focused forecasting literature between the 1990s and the current decade, with a clear rise of quantitative, data-driven forecasting models. Comparative studies of quantitative methods and human judgement show that (1) neither method is universally superior, and (2) the better method varies as a function of factors such as availability, quality, extent and format of data, suggesting that (3) the two approaches can complement each other to yield more accurate and resilient models. We also identify four research thrusts in the human/machine-forecasting literature: (i) the choice of the appropriate quantitative model, (ii) the nature of the interaction between quantitative models and human judgement, (iii) the training and incentivization of human forecasters, and (iv) the combination of multiple forecasts (both algorithmic and human) into one. This review surveys current research in all four areas and argues that future research in the field of human/machine forecasting needs to consider all of them when investigating predictive performance. We also address some of the ethical dilemmas that might arise due to the combination of quantitative models with human judgement.},
  keywords = {forecast combination,forecasting,human judgment,quantitative forecasting methods},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/WLF44NUH/Zellner et al. - 2021 - A survey of human judgement and quantitative forec.pdf}
}

@article{zielEnergyDistanceEnsemble2021,
  title = {The Energy Distance for Ensemble and Scenario Reduction},
  author = {Ziel, Florian},
  year = {2021},
  month = jun,
  journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {379},
  number = {2202},
  pages = {20190431},
  publisher = {{Royal Society}},
  doi = {10.1098/rsta.2019.0431},
  urldate = {2023-12-12},
  abstract = {Scenario reduction techniques are widely applied for solving sophisticated dynamic and stochastic programs, especially in energy and power systems, but are also used in probabilistic forecasting, clustering and estimating generative adversarial networks. We propose a new method for ensemble and scenario reduction based on the energy distance which is a special case of the maximum mean discrepancy. We discuss the choice of energy distance in detail, especially in comparison to the popular Wasserstein distance which is dominating the scenario reduction literature. The energy distance is a metric between probability measures that allows for powerful tests for equality of arbitrary multivariate distributions or independence. Thanks to the latter, it is a suitable candidate for ensemble and scenario reduction problems. The theoretical properties and considered examples indicate clearly that the reduced scenario sets tend to exhibit better statistical properties for the energy distance than a corresponding reduction with respect to the Wasserstein distance. We show applications to a Bernoulli random walk and two real data-based examples for electricity demand profiles and day-ahead electricity prices. This article is part of the theme issue `The mathematics of energy systems'.},
  keywords = {electricity load,energy score,Kontorovic distance,maximum mean discrepancy,scenario reduction,stochastic programming,Wasserstein metric},
  file = {/Users/nikos/github-synced/zotero-nikos/storage/ZIY7UT87/Ziel - 2021 - The energy distance for ensemble and scenario redu.pdf}
}

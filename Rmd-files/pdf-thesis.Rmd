--- 
title: |
  $~~~~~$
  
  $~~~~~$
  
  **PhD Thesis**
  
  $~~~~~$
  
  $~~~~~$
  
  Nikos Ioannis Bosse
  
  $~~~~~$
  
  Department of Infectious Disease Epidemiology
  
  Faculty of Epidemiology and Population Health
  
  Centre for Mathematical Modelling of Infectious Diseases
  
  $~~~~~$
  
  London School of Hygiene & Tropical Medicine
  
  $~~~~~$
  
  $~~~~~$
  
  July 2021
  
  $~~~~~$
site: bookdown::bookdown_site
output: 
  bookdown::pdf_book:
    fig_caption: yes
    toc_depth: 3
    latex_engine: pdflatex
    # latex_engine: xelatex
    toc: False
    includes:
      in_header: preamble.tex
    citation_package: natbib
    keep_tex: yes
documentclass: book
classoption: openany
output_dir: "../docs"
bibliography: [bib/phdthesis.bib, bib/packages.bib, bib/GermanyPoland.bib] # bib/scoringutils.bib, 
biblio-style: apalike
link-citations: yes
header-includes:
  - \usepackage{amssymb}
  - \usepackage{tabu}
  # stuff to make sure that the word "Chapter doesn't appear randomly"
  - \usepackage{titlesec} 
  - \titleformat{\chapter}{\normalfont\LARGE\bfseries}{\thechapter}{1em}{}
  - \titlespacing*{\chapter}{0pt}{3.5ex plus 1ex minus .2ex}{2.3ex plus .2ex}
  # include pdf pages
  - \usepackage{pdfpages}
after_body-includes:
  - \addcontentsline{toc}{chapter}{Bibliography}
github-repo: nikosbosse/phd_thesis
cover-image: logo_university.png
description: "Some description"
---



<!--chapter:end:index.Rmd-->

# Abstract {-}

In the Abstract, this thesis is really good. 
one page. 

<!--chapter:end:000-Abstract.Rmd-->

# Ackowldedgements {-}

I would like to thank my supervisors. 



\tableofcontents

<!--chapter:end:001-Acknowledgements.Rmd-->


# Thesis structure {-}

Placeholder



<!--chapter:end:002-Structure.Rmd-->


# Introduction {#intro}

Placeholder


## Motivation
## Aims and objectives
## Thesis outline

<!--chapter:end:01-introduction.Rmd-->


# Backgrond {#background}

Placeholder


## Forecasting and Modelling
## Forecast evaluation
## Human Judgement forecasting

<!--chapter:end:02-Background.Rmd-->

# Evaluating forecasts using `scoringutils` in `R`
\label{sec:scoringutils}

The following chapter establishes the foundation for the rest of this thesis, both in terms of the tools and the theoretical background needed to evaluate the quality of a forecast. It covers the motivation behind forecast information, the forecasting paradigm \citep{gneitingProbabilisticForecastsCalibration2007} underlying the evaluation approaches used in this thesis, as well as a range of different metrics that can be used to assess the quality of a forecast. It discusses in detail how different metris differ and when they are appropriate to use. All metrics and evaluation approaches presented in the next chapter are implemented in the \textsf{R} package `scoringutils` which facilitates all forecast evaluations conducted in this thesis. The package was developed to help address an acute need to understand the quality of the forecasts that were produced to inform the COVID response of public health institutions in the UK and abroad in 2020. It was continuously developed and refined to provide the tools needed not only for the analyses in the following chapters of this PhD. It also supports and faciliates the evaluations conducted by the US and European Forecast Hubs \citep{cramerEvaluationIndividualEnsemble2021, sherrattPredictivePerformanceMultimodel2022a}, which both make use of `scoringutils`. 

The scoring rules presented in this chapter have been used in many different contexts and are not specific for forecasts of infectious diseases. Later, in Chapter \ref{sec:LogTransformation}, some of the scoring metrics presented here will be revisited and adapted such that they take particular characteristics of infectious disease forecasts better into account. 

\clearpage

<!-- \includepdf[pages=-]{papers/Research Paper Cover Sheet-18 Paper 2.docx.pdf} -->

<!-- \includepdf[pages=-]{papers/Paper-1-scoringutils.pdf} -->

<!--chapter:end:03-Evaluating-forecasts-in-R.Rmd-->

# Comparing human and model-based forecasts of COVID-19 in Germany and Poland
\label{sec:GermanyPoland}

This chapter investigates what human judgement can contribute to infectious disease forecasting, applying the foundational concepts that were covered in Chapter \ref{sec:scoringutils}. The work in this chapter was motivated by the need to produce timely and useful forecasts of COVID-19. In October 2020 the German and Polish COVID0-19 Forecast Hub \citep{bracherPreregisteredShorttermForecasting2021} launched, eliciting forecasts to help inform public health decision making in Germany and Poland. Previous submissions from our working group to the US COVID-19 Forecast Hub \citep{cramerEvaluationIndividualEnsemble2021} had proved cumbersome and it was not clear what the added benefit of mathematical modelling over human judgement alone was. We therefore developed an open source application, `crowdforecastr` \citep{crowdforecastr}, which would allow humans to submit direct forecasts of cases and deaths in Germany and Poland. These forecasts were collected every week, aggregated and submitted to the German and Polish Forecast Hub. In order to investigate a better understanding of what mathematical modelling may add to human judgement alone, we submitted these forecasts alongside two mathematical models with minimal tuning. In an additional analyses, we further explore how adding different forecasts affects the quality of an ensemble of forecasts. This analysis was motivated by the desire to better understand what kinds of forecasts may contribute to public health decision making, and whether even imperfect forecast models can contribute usefully. 

The study was limited both by resource constraints as well as the time that was available to develop the `crowdforecastr` platform, obtain ethics approval, conduct outreach, and the need to submit the first forecasts within 6 weeks of starting this PhD. The study should therefore be treated like a case study that explores a variety of questions related to the interplay of human judgement and mathematical modelling, rather than providing definitive conclusions. 




\clearpage


\includepdf[pages=-]{papers/Research Paper Cover Sheet-18 Paper 2.docx.pdf}

<!-- \includepdf[pages=-]{papers/Paper-2-Forecasts-Germany-Poland.pdf} -->
<!-- \includepdf[pages=-]{papers/Paper-2-SI.pdf} -->

<!--chapter:end:04-Forecasts-in-Germany-Poland.Rmd-->

# Transformation of forecasts for evaluating predictive performance in an epidemiological context
\label{sec:LogTransformation}

Chapter \ref{sec:GermanyPoland} revealed a few shortcomings of the weighted interval score at it is commonly applied in epidemiology. We saw that average scores where dominated by outliers, and overall scores scaled with incidences. This made it difficult to compare forecasts across time and location, and in particular across forecast targets. More importantly, there is a tension when evaluating forecasts on the absolute distance between forecast and observation, while the underlying process we try to model is exponential in nature. It might therefore be more appropriate to evaluate forecasts of infectious disease based on how well they capture the exponential growth rate of a disease process. This could provide a more meaningful signal about which forecasters to trust in the future, as it more closely represents the actual modelling task one has to solve in order to create an accurate representation of the spread of the infectious disease. 

Different scores emphasise different kinds of errors differently. For example, in Chapter \ref{sec:GermanyPoland}, the WIS penalised forecasts strongly that overshot after missing a peak. It did, however, penalise models only very lightly if they were too late to predict an increase in numbers while incidences were still low. Arguably, this kind of early warning is something that policy makers would care about, but which is neglected in current evaluations. Forecast evaluations play an important role not only as a signal to modellers who aim to improve their models. They also help decision makers select which models should inform their policies in the future. If the score does not accurately reflect what policy makers care about in a good forecast, then policy makers may not pick the best model to guide their decisions. 

Chapter \@ref(sec:LogTransformation) explores ways in which the forecast evaluation can be aligned more closely with what forecast consumers actually care about. One possible solution is to transform forecasts and observations before applying the WIS. We propose and analyse the natural logarithm as a transformation that is particularly attractive in an epidemiological context, but there are many more possible transformations. In particular, the idea of transforming forecasts opens up the possibility of creating composite scores, in which a score is constructed as a linear combination of scores obtained after various different transformation. This could, in the future, enable policy makers to create their own custom scores which exactly reflect what they care about. This, however, is not discussed in detail in Chapter \@ref(sec:LogTransformation), which mostly focuses on the natural logarithm as a suitable transformation. 

\clearpage


<!-- \includepdf[pages=-]{papers/Research Paper Cover Sheet-18 Paper 2.docx.pdf} -->

<!-- \includepdf[pages=-]{Paper-3-LogTransformation.pdf} -->






<!--chapter:end:05-Transformation-forecasts.Rmd-->

# Human Judgement Forecasting of COVID-19 in the UK
\label{sec:UKforecastingChallenge}

The crowd forecasts submitted to the German and Polish Forecast Hub described in Chapter \ref{sec:GermanyPoland} formed part of an acute COVID-19 response effort and therefore exhibited a few shortcomings that the study presented in this chapter aims to address. In particular, the study in Germany and Poland suffered from a low number of participants, both in terms of the crowd forecast as well as the number of model-based predictions submitted to the Forecast Hub. This makes it difficult to generalise findings. Forecasts were also only evaluated on the natural scale with all the shortcomings discussed in Chapter \ref{sec:LogTransformation}. 

This chapter describes a follow-up study conducted in the UK. In order to increase and diversify participation, we organised the study in the form of a public forecasting tournament, the "UK Crowd Forecasting Challenge". This allowed us to analyse whether findings from initial study would hold in a different setting with a larger pool of participants. Forecasts were analysed both on the natural and on the log scale, providing a more complete picture of predictive performance of human judgement forecasts. 

This chapter also extends the work in Chapter \ref{sec:GermanyPoland} by exploring a novel way to combine human judgement and mathematical modelling as proposed in the original work. Instead of asking forecasters to predict case and death incidences directly, we elicited forecasts the effective reproduction number $R_t$ which then got mapped to cases and deaths using an epidemiological model. The motivation behind this idea was twofold. One the one hand this might be a possibility to improve forecasts by providing a means to harness the respective strengths of human judgement and mathematical modelling. On the other hand, combining human judgement and mathematical modelling might be a way to make human judgement forecasting more scalable by reducing the cognitive load and the number of forecasts an individual needs to provide. 


\clearpage

<!--chapter:end:06-UK-Crowd-Forecasting.Rmd-->

# Discussion 
\label{sec:discussion}

Summary of key results

Strengths and limitations

Contributions of this research relative to previous knowledge

Implications and future research opportunities





## Summary and contributions to existing work

The work presented in this thesis made several contributions related to forecast evaluation and human judgement forecasting in epidemiology. 

### Forecast evaluation {-}

The first major contribution is towards improving the evaluation of infectious disease forecasts, both from a practical and a theoretical perspective. 

#### scoringutils {-}
The `scoringutils` package and the review of scoring practices presented in Chapter \ref{sec:scoringutils} help make forecast evaluation and the necessary tools more accessible to researchers and decision makers. The package implements a suite of proper scoring rules and other scoring metrics in a flexible framework and offers extensive documentation to guide users through the process. The paper reviews the theoretical foundation of forecast evaluation and provides justification for the choice of a particular score over another. Most of the scores implemented in `scoringutils` had been implemented previously. Notable existing packages are `scoringRules` CITATION, `topmodels`, `tscount` and `Metrics`, among others. `scoringutils` improves usability over existing software and is especially geared towards comparing forecasts from different models. All forecasts can be scored and summarised in a convenient `data.frame` format and the package offers functionality to compare performance visually across different dimensions and account for missing forecasts. `scoringutils` has since been used in published and unpublished work supporting major public health organisations such the US Centers for Disease Control and Prevention, the European Centre for Disease Prevention and Control, the UK Health Security Agency, and Médecins Sans Frontièrs. 

#### Evaluating forecasts in an epidemiological context {-}

In addition to improving the tools available for forecast evaluations, this thesis made a theoretical contribution to our understanding of forecast evaluations in the context of epidemiology. We argued that transforming forecasts and observations prior to applying the continuous ranked probability score (CRPS) or the weighted interval score (WIS) makes it possible to obtain more meaningful results than with the untransformed CRPS and WIS.






The WIS is a quantile-based approximation of the CRPS which in turn is a generalisation of the absolute error to probabilistic forecasts and measures the absolute distance between the forecast and the observation. This may not be the most appropriate measure of performance given the exponential nature of infectious disease processes. Applying a transformation to the forecasts and observations before evaluation 

The natural logarithm is particularly attractive in an epidemiological context, as the resulting score represents a measure of how well the forecasts predicted the exponential growth rate. 





### Human judgement {-}

The second major contribution is in improving our understanding of the role of human judgement in infectious disease forecasting and the potential and limitations of human judgement forecasts. 

In order to be able to compare probabilistic forecasts from human forecasters against computational models, we developed a new open source platform and R package, `crowdforecastr`, which allows the elicitation of probabilistic time series forecasts from individual forecasters. When we submitted our forecasts to the German and Polish Forecast Hub in 2020 and 2021, our study was the first one in an epidemiological context to compare full predictive distributions from individual users against model-based predictions. A previous study by \cite{farrowHumanJudgmentApproach2017} had elicited point forecasts from individual participants and combined them to a probabilistic forecasts based on the mean and variation in participants' predictions. 

In the first study presented in Chapter \ref{sec:GermanyPoland}, we compared a small crowd of human forecasters against two minimally-tuned epidemiological models and an ensemble of model-based predictions submitted to the German and Polish Forecast Hub. We found predictions for cases from human forecasters to be slightly better in terms of the weighted interval score than forecasts of the Hub ensemble, but worse when predicting deaths. Our minimally-tuned model forecasts performed worse than both the crowd forecasts and the Hub ensemble for cases as well as deaths, especially for longer horizons. This suggests that human judgement is beneficial in guiding model-based predictions when conditions change over time. We also found that even forecasts that are worse than an pre-existing ensemble can help to improve ensemble forecasts when including it in that ensemble. 

In the second study in the UK presented in Chapter \ref{uk-challenge}, we repeated the basic set-up of the first study with a larger number of participants in a different country. Forecasts were elicited as part of a public forecasting challenge over the course of 13 weeks and submitted to the European COVID-19 Forecast Hub. In addition, we tested a novel forecasting approach in which users submitted a forecast of the effective reproduction number $R_t$, which would then get mapped to reported cases and deaths using the R package `EpiNow2` CITAITON. Following the conclusions presented in Chapter \ref{sec:LogTransformation} we evaluated forecasts using both untransformed and log-transformed predictions and observations. We found performance of human and model-based forecasts to be overall comparable. The Hub ensemble performed slightly better than human forecasts on the natural scale, and slightly worse when evaluated on the log scale. Our novel $R_t$ approach performed comparable to other forecasts when predicting cases (which forecasters could observe directly), and noticeably worse for death predictions (which forecasters couldn't see), suggesting that the underlying model did not accurately capture the relation between cases and deaths. 



- note that this was outbreak response


is the development of tools geared towards the elicitation of human forecasts of infectious diseases. 



The following aims were set out in Chapter \ref{sec:introduction}: 
<!-- - Establish appropriate tools to evaluate predictions in `R` and review best practices in forecast evaluation -->
<!-- - Develop tools to elicit human forecasts of infectious diseases, specifically COVID-19 -->
<!-- - Analyse the role of human judgement in forecasting COVID-19 in Germany and Poland. Compare human judgement forecasts against model-based predictions and analyse the added benefit of human input over mathematical models -->
- Improve current evaluation methods so that they are betters suited for evaluating forecasts in an epidemiological context
- Examine the potential to use public crowd forecasting tournaments to predict COVID-19 in the UK and explore possibilities to combine human judgement and epidemiological modelling




- established tools to evaluate forecasts and reviewed current scoring methods
- improved current evaluation methods in an epidemiological context
- created an open source forecast elicitation platform and used it to inform COVID response
- investigated the role of human judgement forecasting
- 

- Issue with human forecasting: 
  - trust in public health decision making
  
- 
  
## Limitations (and strengths)

- R package could need more metrics and is still under active development
- Established evidence that humans can be good, but still evidence needed regarding the role of human judgement in modelling
  - it would be nice to have a study where you compare a) human forecasts, b) human forecasters who adjust model outputs and c) just the model outputs. 
  - would also need better tools for that. 
- Forecast evaluations are difficult and there is lots of researcher degrees of freedom. 
- we still don't really know how well human forecasting works. Most of my research was more like a case study. 
  
## Implications
  
- unclear whether humans are better than at deaths
- humans can in principle keep up with models. 
- human judgement forecasting is hard and it's really tough to recruit enough participants.
- Implications for the Forecast Hub evaluations. Two proper scoring rules commonly used in Epidemiology (among others, by the COVID-19 Forecast Hubs in the US, Europe, and Germany / Poland) are the 



## Future work

- combining humans and models
  - Rt forecasting
  - using humans to predict parameters
    - Metaculus question we authored
    - Monkeypox work that Tom made - things are complicated
  - Heaving humans come up with model weights
  - having humans adapt models

- Related: Develop ways to scale human forecasting

- Unclear what score corresponds to what usefulness: e.g. if we show decision makers different forecasts, which score will choose the model that different decision makers describe as the most useful one. 
- Also lots of potential development for new and composite scores. 

- Modelling of scores - e.g. by standardising the scores. 

- how do details of human forecasters



<!--chapter:end:08-Discussion.Rmd-->

# (APPENDIX) Appendix {-}

# Appendix

Some Appendix here. 

<!--chapter:end:09-Appendix.Rmd-->

\addcontentsline{toc}{chapter}{Bibliography}

<!--chapter:end:99-References.Rmd-->


# Evaluating forecasts using `scoringutils` in `R`
\label{sec:scoringutils}

The following chapter establishes the foundation for the rest of this thesis, both in terms of the tools and the theoretical background needed to evaluate the quality of a forecast. It covers the motivation behind forecast information, the forecasting paradigm \citep{gneitingProbabilisticForecastsCalibration2007} underlying the evaluation approaches used in this thesis, as well as a range of different metrics that can be used to assess the quality of a forecast. It discusses in detail how different metris differ and when they are appropriate to use. All metrics and evaluation approaches presented in the next chapter are implemented in the \textsf{R} package `scoringutils` which facilitates all forecast evaluations conducted in this thesis. The package was developed to help address an acute need to understand the quality of the forecasts that were produced to inform the COVID response of public health institutions in the UK and abroad in 2020. It was continuously developed and refined to provide the tools needed not only for the analyses in the following chapters of this PhD. It also supports and faciliates the evaluations conducted by the US and European Forecast Hubs \citep{cramerEvaluationIndividualEnsemble2021, sherrattPredictivePerformanceMultimodel2022a}, which both make use of `scoringutils`. 

The scoring rules presented in this chapter have been used in many different contexts and are not specific for forecasts of infectious diseases. Later, in Chapter \ref{sec:LogTransformation}, some of the scoring metrics presented here will be revisited and adapted such that they take particular characteristics of infectious disease forecasts better into account. 

\clearpage

<!-- \includepdf[pages=-]{papers/Research Paper Cover Sheet-18 Paper 2.docx.pdf} -->

<!-- \includepdf[pages=-]{papers/Paper-1-scoringutils.pdf} -->
